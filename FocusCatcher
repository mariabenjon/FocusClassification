import os, sys, email, re, pdb
from nltk.corpus import stopwords
import glob
from glob import glob
from os.path import join
import time
import os
from sklearn.preprocessing import StandardScaler
import re
import datetime
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib import transforms
import matplotlib.ticker as tkr
from tensorflow.keras.utils import img_to_array, array_to_img
from keras.preprocessing import image
import shutil
import read_roi
from skimage import io, feature, filters, morphology, draw
from skimage.measure import regionprops
import skimage.measure as measure
from sklearn.manifold import TSNE
from skimage.measure import shannon_entropy
from skimage.measure import blur_effect
from skimage.measure import euler_number
from skimage.measure import subdivide_polygon
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multioutput import MultiOutputClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import roc_auc_score
from sklearn import preprocessing
from sklearn.datasets import load_digits
from umap import UMAP
import umap.plot
import pacmap
import plotly.express as px
from sklearn.utils import class_weight
from sklearn.ensemble import VotingClassifier
from skmultilearn.ensemble import MajorityVotingClassifier
from skmultilearn.ensemble import LabelSpacePartitioningClassifier
from skmultilearn.problem_transform import BinaryRelevance
from skmultilearn.cluster.networkx import NetworkXLabelGraphClusterer
from skmultilearn.cluster import LabelCooccurrenceGraphBuilder
import time
from sklearn import model_selection
from sklearn.model_selection import cross_validate
import statistics
from sklearn.utils import class_weight

# check version number
import imblearn
from sklearn import datasets
import sklearn.neighbors
from scipy import stats
import itertools
from sklearn.metrics import roc_curve
from itertools import cycle
from cycler import cycler
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \
    confusion_matrix
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import matthews_corrcoef
from sklearn.model_selection import KFold
from sklearn.metrics import log_loss
import math
from math import log, pi, sqrt
from yellowbrick.classifier import ClassPredictionError
import numpy as np
import pandas as pd
import cv2
import os
import mahotas as mh
# from __future__ import division                 #to avoid integer devision problem

import seaborn as sns
from itertools import repeat
# for machine learning focus detection
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import scale
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris



def focus_classifier(unclassified_foci_props):
    foci_info_file = 'Z:/rothenberglab/archive/Maria/2020_FociData' + '/foci_info.csv'
    df_foci_full = pd.read_csv(foci_info_file)
    foci_classes = df_foci_full['status'].unique()
    df_foci = df_foci_full.copy()
    # change other focus classes that aren't 1 (true focus) to 0 (false focus)
    df_foci.loc[(df_foci.status > 1), ('status')] = 0

    # split data into train and test set
    X_raw = np.array(
        list([df_foci['area'].to_numpy(), df_foci['perimeter'].to_numpy(), df_foci['major_axis_length'].to_numpy(),
              df_foci['minor_axis_length'].to_numpy()]))
    Y_raw = np.array(list(df_foci['status']))
    X_notscaled, test_X, Y_notscaled, test_y = train_test_split(X_raw.transpose(), Y_raw, test_size=0.3, random_state=0)

    # scale data to speed up svm
    scaling = MinMaxScaler(feature_range=(-1, 1)).fit(X_notscaled)  # MinMaxScaler(feature_range=(-1, 1)).fit(X)
    X = scaling.transform(X_notscaled)  # scaling.transform(X_raw.transpose())
    test_X = scaling.transform(test_X)  # scaling.transform(test_X)
    Y = Y_notscaled

    # scale input data
    unclassified_foci_props = unclassified_foci_props.transpose()
    scaling_ = MinMaxScaler(feature_range=(-1, 1)).fit(unclassified_foci_props)
    unclassified_foci_props_sc = scaling_.transform(unclassified_foci_props)

    # initialize SVM
    clf = svm.SVC(kernel='poly', gamma=10, C=0.05)
    # fit the model with the training data
    clf.fit(X, Y)
    # check accuracy of model
    accuracy = clf.score(test_X, test_y)

    # predict class of focus for detected foci
    classified_foci = clf.predict(unclassified_foci_props_sc)

    return classified_foci


def draw_on_img(file, file_root, draw_img, obj_bbox, obj_coords, center_coordinates, box, l, w, draw_color,
                draw_bbox=True, draw_contours=True, draw_label=True, label=''):
    # draw tracking window (bbox) and object contours on intensity stack in white color
    # if draw_label == True then also label will be drawn at top corner of bbox
    # imgplot = plt.imshow(draw_img) # img_rgb_1
    # plt.show()
    # plt.close()

    (min_row, min_col, max_row, max_col) = obj_bbox
    if (draw_bbox):
        cv2.rectangle(draw_img, (min_col, min_row), (max_col, max_row), draw_color, 1)
    if (draw_label == True):
        test_img = draw_img
        cv2.putText(test_img, label, (max_col, max_row), cv2.FONT_HERSHEY_SIMPLEX, 0.7, draw_color, 1, cv2.LINE_AA) # before draw_color: 0.5
        # cv2.putText(draw_img, label, (max_col, max_row), cv2.FONT_HERSHEY_SIMPLEX, 0.23, draw_color, 1, cv2.LINE_AA)
        # im = Image.new(mode='RGB', size=(draw_img.shape[0], draw_img.shape[1]))
        # draw = ImageDraw.Draw(draw_img)
        # font = ImageFont.truetype("arial.ttf", 18)
        # draw.text((max_col, max_row), label, (255, 255, 255), font=font)

    # imgplot = plt.imshow(draw_img) # img_rgb_1
    # plt.show()
    # plt.close()

    if (draw_contours):
        coords = obj_coords
        obj_mask = np.zeros(shape=draw_img.shape, dtype='uint8')  # shape=draw_img.shape[:-1]
        for i in range(len(coords)):
            obj_mask[coords[i][0], coords[i][1]] = 255
        contours, hierarchy = cv2.findContours(obj_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
        cv2.drawContours(draw_img, contours, -1, draw_color, 1)
        # Radius of circle
        radius = 1
        # Blue color in BGR
        color = draw_color
        # color = (255, 0, 0)
        # Line thickness of 2 px
        thickness = 1

        # Using cv2.circle() method
        # Draw a circle with blue line borders of thickness of 2 px
        # cv2.circle(draw_img, center_coordinates, radius, color, thickness)


# folder format is DATE_DRUG_TIME
def get_dir_list(base_dir):
    final_dir_dict = {}
    # get all directories that match expected format
    dir_list = glob(base_dir + '/*')
    for dir_ in dir_list:
        if (os.path.isdir(dir_)):
            key = os.path.split(dir_)[1]
            mo = re.match(r'^([0-9]+)_([^_]+)_([^_]+)$', key)
            if (not mo): continue  # skipping, directory name is not correct format

            date = mo.group(1)
            drug = mo.group(2)
            treatment_time = mo.group(3)

            final_dir_dict[key] = []

            cur_dir = dir_ + '/Output'
            experiment_dirs = glob(cur_dir + '/*')
            for exp_dir in experiment_dirs:
                if (os.path.isdir(exp_dir)):
                    mo = re.match(r'^[0-9]+$', os.path.split(exp_dir)[1])
                    if (not mo): continue

                    exp_num = os.path.split(exp_dir)[1]
                    spool_dirs = glob(exp_dir + '/spool_*')
                    for spool_dir in spool_dirs:
                        if (os.path.isdir(spool_dir)):
                            spool = os.path.split(spool_dir)[1]
                            mo = re.match(r'^spool_[0-9]+$', spool)
                            if (not mo): continue
                            # print(spool_dir)
                            final_dir_dict[key].append([date, drug, treatment_time, exp_num, spool, spool_dir])
    return final_dir_dict

def pre_process_movies_optimizations(summary_dir, dir_list):
    #dir_list = dir_list[9:11] # for debugging
    for dir_ in dir_list:
        # Read IDR data
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        spool_list_file = spool_list[0]
        test = dir_[5] + '/' + spool_list_file
        df_file = io.imread(dir_[5] + '/' + spool_list_file)

        # import images
        def last_4chars(x):
            print(x[-8:])
            return (x[-8:])

        # Compile images; left = red chnl, right, green chnl
        # Define directories
        r_raw, c_raw, chan_raw = df_file.shape
        file = spool_list_file  # if only using first frame
        # Initialize image
        t_ = 0
        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip')
        # check number of rois
        roi_is_zip = True
        rgb_movie = df_file  # np.empty(shape=((len(spool_list)), r_raw, c_raw, 3), dtype='uint16') * 0  #
        for roi_file in roi_list:
            print('Processing ', roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            num_frames = len(rgb_movie)
            # roi_points_test = roi_points[7:13]# for debugging
            for roi_count, bbox_points in enumerate(roi_points):
                # roi_count = 7 # for debugging
                print('roi count: ' + str(roi_count))
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1))):
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1))
                n_rows = bbox_points[2][0] - bbox_points[0][0] + 1
                n_cols = bbox_points[2][1] - bbox_points[0][1] + 1
                file_root = os.path.split(file)[1][:-4]
                # for frame_i in range(num_frames):
                # 0.0 append frames and extract defined rois
                cropped_img = rgb_movie[bbox_points[0][0]:bbox_points[2][0] + 1,
                              bbox_points[0][1]:bbox_points[2][1] + 1]

                # rescale image
                '''
                rescale_factor = 4
                cropped_img = cv2.resize(cropped_img, (0, 0), fx=rescale_factor, fy=rescale_factor)
                n_rows_rescale, n_cols_rescale = cropped_img.shape
                '''

                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_01_raw_roi' + str(roi_count + 1) + '_beforeResize.tif', cropped_img)


                #---
                # find the pix number stats
                df_foci = pd.read_csv('H:/2021_FociData/foci_info_20220128.csv')
                df_foci_idr = pd.read_csv('H:/2021_FociData/foci_info_idr.csv')
                df_foci_df_R =  pd.read_csv( summary_dir + '/foci_info_df_R.csv')
                df_foci_df_G =  pd.read_csv( summary_dir + '/foci_info_df_G.csv')

                df_foci_idr_resize_0 = pd.read_csv( summary_dir + '/foci_info_df_R.csv')
                df_foci_idr_resize_1 = pd.read_csv( summary_dir + '/foci_info_df_G.csv')
                df_foci_idr_resize_2 = pd.read_csv('H:/2021_FociData/foci_info_20220128.csv')
                df_foci_idr_resize_3 = pd.read_csv('H:/2021_FociData/foci_info_20220128.csv')

                pix_no = df_foci['tot_pix_no']
                pix_no_idr = df_foci_idr['tot_pix_no']
                pix_no_idr_resize_0 = df_foci_idr_resize_0['tot_pix_no']
                pix_no_idr_resize_1 = df_foci_idr_resize_1['tot_pix_no']
                pix_no_idr_resize_2 = df_foci_idr_resize_2['tot_pix_no']
                pix_no_idr_resize_3 = df_foci_idr_resize_3['tot_pix_no']

                med_pix_no = pix_no.median()
                med_pix_no_idr = pix_no_idr.median()
                med_pix_no_idr_resize_0 = pix_no_idr_resize_0.median()
                med_pix_no_idr_resize_1 = pix_no_idr_resize_1.median()
                med_pix_no_idr_resize_2 = pix_no_idr_resize_2.median()
                med_pix_no_idr_resize_3 = pix_no_idr_resize_3.median()

                mean_pix_no = pix_no.mean()
                mean_pix_no_idr = pix_no_idr.mean()
                mean_pix_no_idr_resize_0 = pix_no_idr_resize_0.mean()
                mean_pix_no_idr_resize_1 = pix_no_idr_resize_1.mean()
                mean_pix_no_idr_resize_2 = pix_no_idr_resize_2.mean()
                mean_pix_no_idr_resize_3 = pix_no_idr_resize_3.mean()

                '''
                majaxlen = df_foci['major_axis_length']
                majaxlen_idr = df_foci_idr['major_axis_length']
                minaxlen = df_foci['minor_axis_length']
                minaxlen_idr = df_foci_idr['minor_axis_length']

                bbox_ = df_foci['bbox']
                bbox_idr = df_foci_idr['bbox']
                bbox_idr_resize_0 = df_foci_idr_resize_0['bbox']
                bbox_idr_resize_1 = df_foci_idr_resize_1['bbox']
                bbox_idr_resize_2 = df_foci_idr_resize_2['bbox']
                bbox_idr_resize_3 = df_foci_idr_resize_3['bbox']

                n_cols = []
                n_rows = []
                n_cols_idr = []
                n_rows_idr = []
                n_cols_idr_resize_0 = []
                n_rows_idr_resize_0 = []
                n_cols_idr_resize_1 = []
                n_rows_idr_resize_1 = []
                n_cols_idr_resize_2 = []
                n_rows_idr_resize_2 = []
                n_cols_idr_resize_3 = []
                n_rows_idr_resize_3 = []
                for idx, bbox_points in enumerate(bbox_):
                    bbox_points_ = bbox_points.split('(')
                    bbox_points_ = bbox_points_[1].split(')')
                    bbox_points_ = bbox_points_[0].split(',')
                    bbox_points_ = [int(numeric_string) for numeric_string in bbox_points_]
                    n_rows.append(bbox_points_[2] - bbox_points_[0] + 1)
                    n_cols.append(bbox_points_[3] - bbox_points_[1] + 1)

                for idx_idr, bbox_points_idr in enumerate(bbox_idr):
                    bbox_points_idr = bbox_points_idr.split('(')
                    bbox_points_idr = bbox_points_idr[1].split(')')
                    bbox_points_idr = bbox_points_idr[0].split(',')
                    bbox_points_idr = [int(numeric_string_idr) for numeric_string_idr in bbox_points_idr]
                    n_rows_idr.append(bbox_points_idr[2] - bbox_points_idr[0] + 1)
                    n_cols_idr.append(bbox_points_idr[3] - bbox_points_idr[1] + 1)

                for idx_idr_resize_0, bbox_points_idr_resize_0 in enumerate(bbox_idr_resize_0):
                    bbox_points_idr_resize_0 = bbox_points_idr_resize_0.split('(')
                    bbox_points_idr_resize_0 = bbox_points_idr_resize_0[1].split(')')
                    bbox_points_idr_resize_0 = bbox_points_idr_resize_0[0].split(',')
                    bbox_points_idr_resize_0 = [int(numeric_string_idr_resize_0) for numeric_string_idr_resize_0 in bbox_points_idr_resize_0]
                    n_rows_idr_resize_0.append(bbox_points_idr_resize_0[2] - bbox_points_idr_resize_0[0] + 1)
                    n_cols_idr_resize_0.append(bbox_points_idr_resize_0[3] - bbox_points_idr_resize_0[1] + 1)

                for idx_idr_resize_1, bbox_points_idr_resize_1 in enumerate(bbox_idr_resize_1):
                    bbox_points_idr_resize_1 = bbox_points_idr_resize_1.split('(')
                    bbox_points_idr_resize_1 = bbox_points_idr_resize_1[1].split(')')
                    bbox_points_idr_resize_1 = bbox_points_idr_resize_1[0].split(',')
                    bbox_points_idr_resize_1 = [int(numeric_string_idr_resize_1) for numeric_string_idr_resize_1 in bbox_points_idr_resize_1]
                    n_rows_idr_resize_1.append(bbox_points_idr_resize_1[2] - bbox_points_idr_resize_1[0] + 1)
                    n_cols_idr_resize_1.append(bbox_points_idr_resize_1[3] - bbox_points_idr_resize_1[1] + 1)

                for idx_idr_resize_2, bbox_points_idr_resize_2 in enumerate(bbox_idr_resize_2):
                    bbox_points_idr_resize_2 = bbox_points_idr_resize_2.split('(')
                    bbox_points_idr_resize_2 = bbox_points_idr_resize_2[1].split(')')
                    bbox_points_idr_resize_2 = bbox_points_idr_resize_2[0].split(',')
                    bbox_points_idr_resize_2 = [int(numeric_string_idr_resize_2) for numeric_string_idr_resize_2 in bbox_points_idr_resize_2]
                    n_rows_idr_resize_2.append(bbox_points_idr_resize_2[2] - bbox_points_idr_resize_2[0] + 1)
                    n_cols_idr_resize_2.append(bbox_points_idr_resize_2[3] - bbox_points_idr_resize_2[1] + 1)

                for idx_idr_resize_3, bbox_points_idr_resize_3 in enumerate(bbox_idr_resize_3):
                    bbox_points_idr_resize_3 = bbox_points_idr_resize_3.split('(')
                    bbox_points_idr_resize_3 = bbox_points_idr_resize_3[1].split(')')
                    bbox_points_idr_resize_3 = bbox_points_idr_resize_3[0].split(',')
                    bbox_points_idr_resize_3 = [int(numeric_string_idr_resize_3) for numeric_string_idr_resize_3 in bbox_points_idr_resize_3]
                    n_rows_idr_resize_3.append(bbox_points_idr_resize_3[2] - bbox_points_idr_resize_3[0] + 1)
                    n_cols_idr_resize_3.append(bbox_points_idr_resize_3[3] - bbox_points_idr_resize_3[1] + 1)


                med_n_rows = statistics.median(n_rows)
                med_n_cols = statistics.median(n_cols)
                med_n_rows_idr = statistics.median(n_rows_idr)
                med_n_cols_idr = statistics.median(n_cols_idr)
                med_n_rows_idr_resize_0 = statistics.median(n_rows_idr_resize_0)
                med_n_cols_idr_resize_0 = statistics.median(n_cols_idr_resize_0)
                med_n_rows_idr_resize_1 = statistics.median(n_rows_idr_resize_1)
                med_n_cols_idr_resize_1 = statistics.median(n_cols_idr_resize_1)
                med_n_rows_idr_resize_2 = statistics.median(n_rows_idr_resize_2)
                med_n_cols_idr_resize_2 = statistics.median(n_cols_idr_resize_2)
                med_n_rows_idr_resize_3 = statistics.median(n_rows_idr_resize_3)
                med_n_cols_idr_resize_3 = statistics.median(n_cols_idr_resize_3)

                ratio = med_n_cols/med_n_rows_idr_resize_0
                stop = 1
                '''

def pre_process_movies(summary_dir, dir_list):
    #dir_list = dir_list[5:8] # for debugging
    for dir_ in dir_list:
        # Read BP1-2 data
        # dir_ = dir_list[25] # for debugging
        left_chnl_dir = dir_[5] + '/lefts'
        right_chnl_dir = dir_[5] + '/rights'
        left_chnl_dir_files = os.listdir(left_chnl_dir)
        right_chnl_dir_files = os.listdir(right_chnl_dir)

        # import images
        def last_4chars(x):
            print(x[-8:])
            return (x[-8:])

        # Compile images; left = red chnl, right, green chnl
        # Define directories
        spool_list = sorted(left_chnl_dir_files)
        single_left_file = io.imread(left_chnl_dir + '/' + spool_list[1])
        r_raw, c_raw = single_left_file.shape
        spool_list = spool_list[0:3]
        # file = spool_list # if only using first frame
        # Initialize image
        rgb_movie = np.empty(shape=((len(spool_list)), r_raw, c_raw, 3), dtype='uint16') * 0  #
        t_ = 0

        # Import images from each channel
        # Use if more than 1st image is used
        for file in spool_list:
            if (file == 'Thumbs.db'):
                continue
            rgb_movie[t_, :, :, 0] = io.imread(left_chnl_dir + '/' + file)
            rgb_movie[t_, :, :, 1] = io.imread(right_chnl_dir + '/' + file)
            t_ = t_ + 1
        io.imsave(dir_[5] + '/' + 'rgb_movie.tif', rgb_movie)

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/*.roi')
        if (len(roi_list) == 1):
            roi_is_zip = False
        elif (len(roi_list) < 1):
            # more than one ROI
            roi_list = glob(dir_[5] + '/*.zip')
            roi_is_zip = True
        else:
            print("Missing ROI file for movie file")
            continue

        for roi_file in roi_list:
            print('Processing ', roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            num_frames = len(rgb_movie)
            for roi_count, bbox_points in enumerate(roi_points):
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1))):
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1))
                n_rows = bbox_points[2][0] - bbox_points[0][0] + 1
                n_cols = bbox_points[2][1] - bbox_points[0][1] + 1
                cropped_rgb_movie = np.empty(shape=(num_frames, n_rows, n_cols, 3), dtype='uint16')
                cropped_g_movie = np.empty(shape=(num_frames, n_rows, n_cols, 1), dtype='uint16')
                raw_cropped_g_movie = np.empty(shape=(num_frames, n_rows, n_cols, 1), dtype='uint16')
                file_root = os.path.split(file)[1][:-4]
                for frame_i in range(num_frames):
                    # 0.0 append frames and extract defined rois
                    cropped_rgb_movie[frame_i] = rgb_movie[frame_i][bbox_points[0][0]:bbox_points[2][0] + 1,
                                                 bbox_points[0][1]:bbox_points[2][1] + 1, :]
                    cropped_g_movie[frame_i, :, :, :] = cropped_rgb_movie[frame_i, :, :, 1].reshape((n_rows, n_cols, 1))

                    # 0.1 extract defined roi
                    raw_img = cropped_g_movie[frame_i, :, :, :].reshape(n_rows, n_cols).copy()
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_preproc_01_raw_single_frame_roi' + str(roi_count + 1) + '_.tif', raw_img)

                    # 0.2 histogram equalization
                    clahe = cv2.createCLAHE(clipLimit=10, tileGridSize=(10, 10))
                    img = clahe.apply(raw_img)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_preproc_02_clahe_roi' + str(roi_count + 1) + '_.tif', img)

                    # 0.3 remove hot pixels
                    medblurred_img = cv2.medianBlur(img, 3)
                    dif_img = img - medblurred_img
                    img = img - dif_img
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_preproc_03_hotpixfilt_roi' + str(roi_count + 1) + '_.tif', img)

                    # 0.4 gaussian blur
                    img = cv2.GaussianBlur(img, (7, 7), 0)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_preproc_04_gaussblur_roi' + str(roi_count + 1) + '_.tif', img)

                    # 0.5 background subtraction via k means clustering
                    max_pix = np.max(img)
                    Z = img.reshape((n_rows * n_cols), 1)
                    # convert to np.float32
                    Z = np.float32(Z)
                    # define criteria, number of clusters(K) and apply kmeans()
                    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
                    K = 4  # 3  # for #1: 4
                    ret, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
                    sorted_center = center[:, 0].sort()
                    _, img = cv2.threshold(img, int(center[2]), max_pix, cv2.THRESH_TOZERO)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_preproc_05_bskmeans_roi' + str(roi_count + 1) + '_.tif', img)

                    # append processed single roi for this frame
                    cropped_g_movie[frame_i, :, :, :] = np.asarray(img).reshape(n_rows, n_cols, 1)
                    raw_cropped_g_movie[frame_i, :, :, :] = np.asarray(raw_img).reshape(n_rows, n_cols, 1)

                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_roi' + str(roi_count + 1) + '_.tif', cropped_g_movie)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_raw_roi' + str(roi_count + 1) + '_.tif', raw_cropped_g_movie)

                # 1.0 mean projection
                max_img = np.mean(cropped_g_movie, axis=0).reshape(n_rows, n_cols)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_proc_10_maxproj_roi' + str(roi_count + 1) + '_.tif', max_img.astype('uint16'))
                raw_max_img = np.mean(raw_cropped_g_movie, axis=0).reshape(n_rows, n_cols)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_proc_10_rawmaxproj_roi' + str(roi_count + 1) + '_.tif', raw_max_img.astype('uint16'))

                # 1.1 OTSU thresholding
                th = filters.threshold_otsu(max_img)
                img_mask = max_img > th
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_proc_11_OTSU_roi' + str(roi_count + 1) + '_.tif', img_mask)

                # 1.2 connected component labeling
                l_, n_ = mh.label(img_mask.reshape(n_rows, n_cols), np.ones((3, 3), bool))  # binary_closed_hztl_k
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_proc_12_ccl_roi' + str(roi_count + 1) + '_.tif', l_)

                # 1.3 measure region properties
                rs_k = regionprops(l_)
                im_props = regionprops(l_, intensity_image=max_img.reshape(n_rows, n_cols))
                results = []

                for blob in im_props:
                    properties = []
                    if ((blob.area < 25) or (blob.area > 1000)):
                        continue

                    #blob = im_props[30]
                    properties.append(blob.label)
                    properties.append(blob.centroid[0])
                    properties.append(blob.centroid[1])
                    properties.append(blob.orientation)
                    properties.append(blob.area)
                    properties.append(blob.perimeter)
                    properties.append(blob.major_axis_length)
                    properties.append(blob.minor_axis_length)
                    properties.append(blob.eccentricity)
                    properties.append(blob.coords)
                    properties.append(blob.bbox)

                    i = blob.label
                    test = blob.coords
                    x, y, w, h = cv2.boundingRect(test)
                    single_focus_img = max_img[x:x + w, y:y + h]
                    single_focus_img_raw = raw_max_img[x:x + w, y:y + h]
                    rows, cols = single_focus_img_raw.shape
                    (h, w) = single_focus_img_raw.shape

                    # ------------------------------------
                    # blur detection in IQM
                    abs_f = np.abs(f)
                    mag_max = np.max(abs_f)
                    thresh = mag_max/1000
                    thresh_f = (abs_f < thresh).sum()
                    imagequalmeas = thresh_f/(rows*cols)
                    # ------------------------------------
                    # save images
                    # grayscale
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(i) + '_0_grayscale' + str(roi_count + 1) + '.tif',
                              single_focus_img_raw.astype('uint8'))

                    # gaussian blur
                    gaussblur_img = cv2.GaussianBlur(single_focus_img_raw.astype('uint8'), (7, 7), 0)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(i) + '_gausblur_roi' + str(roi_count + 1) + '.tif', gaussblur_img)
                    otsu_th = filters.threshold_otsu(gaussblur_img)
                    _, thresh_gauss_img = cv2.threshold(gaussblur_img, otsu_th, 255, cv2.THRESH_TOZERO)
                    single_focus_img_gray = single_focus_img / 4
                    single_focus_img_gray_8U = single_focus_img_gray.astype('uint8')
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[4] + str(
                        roi_count + 1) + 'full_max.tif', max_img.astype('uint8'))
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(i) + '_roi' + str(roi_count + 1) + '.tif', single_focus_img_gray_8U)

                    proj_img_x = np.mean(single_focus_img, axis=0).reshape(w, )
                    proj_img_y = np.mean(single_focus_img, axis=1).reshape(h, )

                    l_x = len(proj_img_x)
                    l_y = len(proj_img_y)
                    if (l_x != l_y):
                        if (l_y > l_x):
                            diff = l_y - l_x
                            proj_img_x = np.pad(proj_img_x, [(0, diff)])

                        if (l_x > l_y):
                            diff = l_x - l_y
                            proj_img_y = np.pad(proj_img_y, [(0, diff)])

                    proj_img_x = proj_img_x
                    proj_img_y = proj_img_y
                    labels_x = np.arange(0, len(proj_img_x), 1)

                    fig = plt.figure()
                    width = 1
                    ax_x = plt.subplot(3, 1, 1)
                    ax_x.bar(labels_x, proj_img_x, width, color='g', edgecolor="k")
                    ax_x.set_xlabel("")
                    ax_x.set_title(' x ')

                    labels_y = np.arange(0, len(proj_img_y), 1)
                    ax_y = plt.subplot(3, 1, 2)
                    ax_y.bar(labels_y, proj_img_y, width, color='g', edgecolor="k")
                    ax_y.set_xlabel("")
                    ax_y.set_title(' y ')

                    ax_xy = plt.subplot(3, 1, 3)
                    rects1 = ax_xy.bar(labels_x, proj_img_x, width, color='royalblue', edgecolor="k")
                    rects2 = ax_xy.bar(labels_y, proj_img_y, width, color='seagreen', edgecolor="k")
                    ax_xy.set_xlabel("")
                    ax_xy.legend((rects1[0], rects2[0]), ('x', 'y'))
                    plt.close(fig)

                    proj_img_x_ = proj_img_x.ravel().astype('float32')
                    proj_img_y_ = proj_img_y.ravel().astype('float32')

                    compare_val_correl_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CORREL)
                    compare_val_chisq_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CHISQR)
                    compare_val_intrsct_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_INTERSECT)
                    compare_val_bc_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_BHATTACHARYYA)
                    compare_val_chisqalt_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CHISQR_ALT)
                    compare_val_hellinger_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_HELLINGER)
                    compare_val_kl_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_KL_DIV)

                    properties.append(imagequalmeas)
                    properties.append(compare_val_correl_xy)
                    properties.append(compare_val_chisq_xy)
                    properties.append(compare_val_intrsct_xy)
                    properties.append(compare_val_bc_xy)
                    properties.append(compare_val_chisqalt_xy)
                    properties.append(compare_val_hellinger_xy)
                    properties.append(compare_val_kl_xy)
                    results.append(properties)

                df_foci_full = pd.DataFrame(results,
                                            columns=['focus_label', 'centroid-0', 'centroid-1', 'orientation', 'area',
                                                     'perimeter', 'major_axis_length',
                                                     'minor_axis_length', 'eccentricity', 'coords', 'bbox', 'img_qual_meas',
                                                     'var_laplc_raw_otsu', 'var_laplc_raw', 'var_laplc',
                                                     'compare_val_correl',
                                                     'compare_val_chisq', 'compare_val_intrsct', 'compare_val_bc',
                                                     'compare_val_chisqalt', 'compare_val_hellngr', 'compare_val_kl'])

                # save labeled and contoured foci
                a = img.copy()
                max_val = int(np.max(a))
                raw_max_val = int(np.max(raw_max_img))
                center_coordinates = 0
                box = 0
                # draw on all detected and processed foci, contours only
                for focus_full_i, focus_full_row in df_foci_full.iterrows():
                    draw_on_img(file, file_root, a, focus_full_row['bbox'], focus_full_row['coords'],
                                center_coordinates, box, 1, 1,
                                (max_val, max_val, max_val), draw_bbox=False,
                                draw_contours=True, draw_label=True,
                                label=str(focus_full_row['focus_label']))  # str(focus_full_i)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_procfocusDetectionCheck_Full.tif', a)

                # draw on all detected and processed foci, contours only
                for focus_full_i, focus_full_row in df_foci_full.iterrows():
                    draw_on_img(file, file_root, raw_max_img, focus_full_row['bbox'], focus_full_row['coords'],
                                center_coordinates, box, 1, 1,
                                (raw_max_val, raw_max_val, raw_max_val), draw_bbox=False,
                                draw_contours=True, draw_label=True,
                                label=str(focus_full_row['focus_label']))  # str(focus_full_i)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_rawfocusDetectionCheck_Full.tif', raw_max_img.astype('uint16'))
                stop = 1
    print("Done.")
    stop = 1


def pre_process_movies_idr(summary_dir, dir_list):
    #dir_list = dir_list[14:16] # for debugging
    #dir_list = [dir_list[i] for i in [2,3,4,5,6,7,8,9,10,12,13,14,15,16,17,18,19,20,21,23]]
    for dir_ in dir_list:
        # Read IDR data
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        spool_list_file = spool_list[0]
        test = dir_[5] + '/' + spool_list_file
        idr_file = io.imread(dir_[5] + '/' + spool_list_file)

        # import images
        def last_4chars(x):
            print(x[-8:])
            return (x[-8:])

        # Compile images; left = red chnl, right, green chnl
        # Define directories
        r_raw, c_raw = idr_file.shape
        file = spool_list_file # if only using first frame
        # Initialize image
        #rgb_movie = np.empty(shape=((len(spool_list)), r_raw, c_raw, 3), dtype='uint16') * 0  #
        t_ = 0

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip')

        # check number of rois
        roi_is_zip = True
        rgb_movie = idr_file

        for roi_file in roi_list:
            print('Processing ', roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            num_frames = len(rgb_movie)
            #roi_points = roi_points[35:37]# for debugging
            for roi_count, bbox_points in enumerate(roi_points):
                #roi_count = 7 # for debugging
                print('roi count: ' + str(roi_count))
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1))):
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1))
                if (os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci')): # delete individual foci directory
                    shutil.rmtree(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci')
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci')): # create individual foci directory
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci')
                if (os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci_Outlines')): # delete individual foci directory with outlines
                    shutil.rmtree(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci_Outlines')
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci_Outlines')): # create individual foci directory with outlines
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci_Outlines')

                n_rows = bbox_points[2][0] - bbox_points[0][0] + 1
                n_cols = bbox_points[2][1] - bbox_points[0][1] + 1
                file_root = os.path.split(file)[1][:-4]

                #for frame_i in range(num_frames):
                # 0.0 append frames and extract defined rois
                cropped_img = rgb_movie[bbox_points[0][0]:bbox_points[2][0] + 1, bbox_points[0][1]:bbox_points[2][1] + 1]

                # rescale image
                '''
                rescale_factor = 4
                cropped_img = cv2.resize(cropped_img, (0, 0), fx=rescale_factor, fy=rescale_factor)
                n_rows_rescale, n_cols_rescale = cropped_img.shape
                '''

                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_01_raw_roi' + str(roi_count + 1) + '_beforeResize.tif', cropped_img)

                # resize
                ratio = 5.02  #idr: 10.2 , df: 1.47
                width = int(cropped_img.shape[1] * ratio)
                height = int(cropped_img.shape[0] * ratio)
                dim = (width, height)
                cropped_img = cv2.resize(cropped_img, dim, interpolation = cv2.INTER_LINEAR)
                n_rows_rescale, n_cols_rescale = cropped_img.shape
                len_img_test_new = cropped_img.size

                # 0.1 extract defined roi
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_01_raw_roi' + str(roi_count + 1) + '_afterResize.tif', cropped_img)

                img = cropped_img

                # 0.2 histogram equalization
                clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(1, 1))
                img = clahe.apply(cropped_img)
                img_clahe = img.copy()
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_02_clahe_roi' + str(roi_count + 1) + '.tif', img)

                # 0.3 remove hot pixels
                medblurred_img = cv2.medianBlur(img, 3)
                dif_img = img - medblurred_img
                img = img - dif_img
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_03_hotpixfilt_roi' + str(roi_count + 1) + '.tif', img)

                # 0.4 gaussian blur
                img = cv2.GaussianBlur(img, (1, 1), 0)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_04_gaussblur_roi' + str(roi_count + 1) + '.tif', img)

                # 0.5 background subtraction via k means clustering
                max_pix = np.max(img)
                Z = img.reshape((n_rows_rescale * n_cols_rescale), 1)
                # convert to np.float32
                Z = np.float32(Z)
                # define criteria, number of clusters(K) and apply kmeans()
                criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
                K = 4  # 3  # for #1: 4
                ret, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
                sorted_center = center[:, 0].sort()
                _, img = cv2.threshold(img, int(center[3]), max_pix, cv2.THRESH_TOZERO)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_05_bskmeans_roi' + str(roi_count + 1) + '.tif', img)

                # 1.0 OTSU thresholding
                th = filters.threshold_otsu(img)
                img_mask = img > th
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_proc_11_OTSU_roi' + str(roi_count + 1) + '.tif', img_mask)

                # 1.1 connected component labeling
                l_, n_ = mh.label(img.reshape(n_rows_rescale, n_cols_rescale), np.ones((3, 3), bool))  # binary_closed_hztl_k
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_proc_12_ccl_roi' + str(roi_count + 1) + '.tif', l_)

                # 1.2 measure region properties
                rs_k = regionprops(l_)
                im_props = regionprops(l_, intensity_image=img.reshape(n_rows_rescale, n_cols_rescale))
                results = []
                for blob in im_props:
                    properties = []
                    i_focus = blob.label
                    focus_coords = blob.coords
                    x, y, w, h = cv2.boundingRect(focus_coords)
                    single_focus_img = img[x:x + w, y:y + h]
                    single_focus_img_raw = cropped_img[x:x + w, y:y + h]
                    rows, cols = single_focus_img_raw.shape
                    focus_tot_pix = rows * cols
                    if ((focus_tot_pix < 3) or (focus_tot_pix > 1700)): #
                        continue

                    # blob = im_props[30] # for debugging
                    print('blob label: ' + str(blob.label))
                    properties.append(blob.label)
                    properties.append(blob.centroid[0])
                    properties.append(blob.centroid[1])
                    properties.append(blob.orientation)
                    properties.append(blob.area)
                    properties.append(blob.perimeter)
                    properties.append(blob.major_axis_length)
                    properties.append(blob.minor_axis_length)
                    properties.append(blob.eccentricity)
                    properties.append(blob.coords)
                    properties.append(blob.bbox)

                    focus_coords = blob.coords

                    i = blob.label
                    test = blob.coords
                    x, y, w, h = cv2.boundingRect(test)
                    single_focus_img = img[x:x + w, y:y + h]
                    single_focus_img_raw = cropped_img[x:x + w, y:y + h]
                    rows, cols = single_focus_img_raw.shape
                    (h, w) = single_focus_img_raw.shape

                    proj_img_x = np.mean(single_focus_img, axis=0).reshape(w, )
                    proj_img_y = np.mean(single_focus_img, axis=1).reshape(h, )

                    l_x = len(proj_img_x)
                    l_y = len(proj_img_y)
                    if (l_x != l_y):
                        if (l_y > l_x):
                            diff = l_y - l_x
                            proj_img_x = np.pad(proj_img_x, [(0, diff)])

                        if (l_x > l_y):
                            diff = l_x - l_y
                            proj_img_y = np.pad(proj_img_y, [(0, diff)])

                    proj_img_x = proj_img_x
                    proj_img_y = proj_img_y
                    labels_x = np.arange(0, len(proj_img_x), 1)

                    fig = plt.figure()
                    width = 1
                    ax_x = plt.subplot(3, 1, 1)
                    ax_x.bar(labels_x, proj_img_x, width, color='g', edgecolor="k")
                    ax_x.set_xlabel("")
                    ax_x.set_title(' x ')

                    labels_y = np.arange(0, len(proj_img_y), 1)
                    ax_y = plt.subplot(3, 1, 2)
                    ax_y.bar(labels_y, proj_img_y, width, color='g', edgecolor="k")
                    ax_y.set_xlabel("")
                    ax_y.set_title(' y ')

                    ax_xy = plt.subplot(3, 1, 3)
                    rects1 = ax_xy.bar(labels_x, proj_img_x, width, color='royalblue', edgecolor="k")
                    rects2 = ax_xy.bar(labels_y, proj_img_y, width, color='seagreen', edgecolor="k")
                    ax_xy.set_xlabel("")
                    ax_xy.legend((rects1[0], rects2[0]), ('x', 'y'))
                    plt.close(fig)

                    proj_img_x_ = proj_img_x.ravel().astype('float32')
                    proj_img_y_ = proj_img_y.ravel().astype('float32')

                    compare_val_correl_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CORREL)
                    compare_val_chisq_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CHISQR)
                    compare_val_intrsct_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_INTERSECT)
                    compare_val_bc_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_BHATTACHARYYA)
                    compare_val_chisqalt_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CHISQR_ALT)
                    compare_val_hellinger_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_HELLINGER)
                    compare_val_kl_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_KL_DIV)

                    # pixel number
                    len_single_focus = single_focus_img.size

                    properties.append(compare_val_correl_xy)
                    properties.append(compare_val_chisq_xy)
                    properties.append(compare_val_intrsct_xy)
                    properties.append(compare_val_bc_xy)
                    properties.append(compare_val_chisqalt_xy)
                    properties.append(compare_val_hellinger_xy)
                    properties.append(compare_val_kl_xy)
                    properties.append(len_single_focus)
                    results.append(properties)

                    # save images of individual foci, with and without outlines
                    # extract focus bounding box
                    focus_bbox = blob.bbox
                    img_r1, img_c1 = img_clahe.shape
                    # define bounding box
                    pixel_border = 3
                    r0 = focus_bbox[0] - pixel_border
                    r1 = focus_bbox[2] + pixel_border
                    c0 = focus_bbox[1] - pixel_border
                    c1 = focus_bbox[3] + pixel_border
                    # check if bounding box goes beyond image limits
                    if (r0 < 0):
                        r0 = 0
                    if (r1 > img_r1):
                        r1 = img_r1
                    if (c0 < 0):
                        c0 = 0
                    if (c1 > img_c1):
                        c1 = img_c1
                    # focus with no outlines
                    img_focus_clahe = img_clahe.copy()[r0:r1, c0:c1]
                    # focus with outlines
                    # draw outlines on full nucleus ROI
                    focus_max_val = int(np.max(img_clahe) / 2)
                    obj_mask = np.zeros(shape=img_clahe.shape, dtype='uint8')  # shape=draw_img.shape[:-1]
                    for i in range(len(focus_coords)):
                        obj_mask[focus_coords[i][0], focus_coords[i][1]] = 200
                    contours, hierarchy = cv2.findContours(obj_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                    cv2.drawContours(img_clahe, contours, -1, (focus_max_val, focus_max_val, focus_max_val), 1)
                    # crop out focus of interest
                    img_focus_clahe_outlines = img_clahe[r0:r1, c0:c1]

                    # save focus in focus folder
                    # max proj
                    # no outlines
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci/' + dir_[0] + '_' +
                              dir_[1] + '_' + dir_[2] + '_' + dir_[3] + dir_[4] + '_roi' + str(roi_count + 1) +
                              '_focusNo' + str(i_focus) + '_' + '.tif', img_focus_clahe)
                    # outlines
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci_Outlines/' + dir_[0] + '_' +
                              dir_[1] + '_' + dir_[2] + '_' + dir_[3] + dir_[4] + '_roi' + str(roi_count + 1) +
                              '_focusNo' + str(i_focus) + '_' + '.tif',
                              img_focus_clahe_outlines)

                df_foci_full = pd.DataFrame(results,
                                            columns=['focus_label', 'centroid-0', 'centroid-1', 'orientation', 'area',
                                                     'perimeter', 'major_axis_length',
                                                     'minor_axis_length', 'eccentricity', 'coords', 'bbox',
                                                     'compare_val_correl',
                                                     'compare_val_chisq', 'compare_val_intrsct', 'compare_val_bc',
                                                     'compare_val_chisqalt', 'compare_val_hellngr', 'compare_val_kl','tot_pix_no'])

                # save properties
                df_foci_full.to_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + 'focus_info_full.csv')

                # save labeled and contoured foci
                a = img.copy()
                max_val = int(np.max(a))
                raw_max_val = int(np.max(cropped_img))
                center_coordinates = 0
                box = 0
                # draw on all detected and processed foci, contours only
                for focus_full_i, focus_full_row in df_foci_full.iterrows():
                    draw_on_img(file, file_root, a, focus_full_row['bbox'], focus_full_row['coords'],
                                center_coordinates, box, 1, 1,
                                (max_val, max_val, max_val), draw_bbox=False,
                                draw_contours=True, draw_label=True,
                                label=str(focus_full_row['focus_label']))  # str(focus_full_i)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_procfocusDetectionCheck_Full.tif', a)

                # draw on all detected and processed foci, contours only
                for focus_full_i, focus_full_row in df_foci_full.iterrows():
                    draw_on_img(file, file_root, cropped_img, focus_full_row['bbox'], focus_full_row['coords'],
                                center_coordinates, box, 1, 1,
                                (raw_max_val, raw_max_val, raw_max_val), draw_bbox=False,
                                draw_contours=True, draw_label=True,
                                label=str(focus_full_row['focus_label']))  # str(focus_full_i)
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_rawfocusDetectionCheck_Full.tif', cropped_img.astype('uint16'))

                stop = 1
    print("Done.")
    stop = 1


def pre_process_movies_df(summary_dir, dir_list):
    #dir_list = dir_list[13:19] # for debugging
    for dir_ in dir_list:
        # Read IDR data
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        if ('.DS_Store' in spool_list):
            spool_list.remove('.DS_Store')
        spool_list_file = spool_list[0]
        df_file = io.imread(dir_[5] + '/' + spool_list_file)

        # import images
        def last_4chars(x):
            print(x[-8:])
            return (x[-8:])

        # Define directories
        file = spool_list_file  # if only using first frame
        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip')
        # check number of rois
        roi_is_zip = True
        rgb_movie = df_file  # np.empty(shape=((len(spool_list)), r_raw, c_raw, 3), dtype='uint16') * 0  #
        for roi_file in roi_list:
            print('Processing ', roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            num_frames = len(rgb_movie)
            for roi_count, bbox_points in enumerate(roi_points): # for debugging roi_points[1:2]
                #roi_count = 1 # for debugging
                print('roi count: ' + str(roi_count))
                if (os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1))): # delete previous roi directory
                    shutil.rmtree(roi_file[:-4] + '_' + str(roi_count + 1))
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1))): # create roi directory
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1))
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci')): # create individual foci directory
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci')
                if (not os.path.isdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci_Outlines')): # create individual foci directory with outlines
                    os.mkdir(roi_file[:-4] + '_' + str(roi_count + 1)+ '/DetectedFoci_Outlines')

                n_rows = bbox_points[2][0] - bbox_points[0][0] + 1
                n_cols = bbox_points[2][1] - bbox_points[0][1] + 1
                file_root = os.path.split(file)[1][:-4]
                # for frame_i in range(num_frames):
                # 0.0 append frames and extract defined rois
                cropped_img = rgb_movie[bbox_points[0][0]:bbox_points[2][0] + 1,
                              bbox_points[0][1]:bbox_points[2][1] + 1]

                # rescale image
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_01_raw_roi' + str(roi_count + 1) + '_beforeResize.tif', cropped_img)

                # resize
                ratio = 2.7  # idr: 10.2 , df: 1.47
                width = int(cropped_img.shape[1] * ratio)
                height = int(cropped_img.shape[0] * ratio)
                dim = (width, height)
                cropped_img = cv2.resize(cropped_img, dim, interpolation = cv2.INTER_LINEAR)
                n_rows_rescale, n_cols_rescale, _ = cropped_img.shape
                n_rows, n_cols, _ = cropped_img.shape

                # 0.1 extract defined roi
                io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    4] + '_preproc_01_raw_roi' + str(roi_count + 1) + '_afterResize.tif', cropped_img)

                channel_names = ['R','G']
                for i_chanl in [1]: # [0, 1] # to loop through both channels
                    #i_chanl = 1 # for debugging
                    img = cropped_img[:,:,i_chanl]
                    print('image channel: ' + channel_names[i_chanl])
                    # 0.2 histogram equalization
                    clahe = cv2.createCLAHE(clipLimit=41, tileGridSize=(7, 7)) #11, (7,7)
                    img = clahe.apply(img)
                    img_clahe = img.copy()
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_preproc_02_clahe_roi' + str(roi_count + 1) + '_' + channel_names[i_chanl] +'.tif', img)

                    # 0.3 remove hot pixels
                    medblurred_img = cv2.medianBlur(img, 3) #3
                    dif_img = img - medblurred_img
                    img = img - dif_img
                    #io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    #    4] + '_preproc_03_hotpixfilt_roi' + str(roi_count + 1) +'_' + channel_names[i_chanl] + '.tif', img)

                    # 0.4 gaussian blur
                    img = cv2.GaussianBlur(img, (5, 5), 0) #(1,1)
                    #io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    #    4] + '_preproc_04_gaussblur_roi' + str(roi_count + 1) + '_' + channel_names[i_chanl] +'.tif', img)

                    # 0.5 background subtraction via k means clustering
                    max_pix = np.max(img)
                    Z = img.reshape((n_rows_rescale * n_cols_rescale), 1) #*******************RESHAPE
                    # convert to np.float32
                    Z = np.float32(Z)
                    # define criteria, number of clusters(K) and apply kmeans()
                    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
                    K = 3  # 3  # for #1: 4
                    ret, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
                    sorted_center = center[:, 0].sort()
                    _, img = cv2.threshold(img, int(center[1]), max_pix, cv2.THRESH_TOZERO)
                    #io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    #    4] + '_preproc_05_bskmeans_roi' + str(roi_count + 1) +'_' + channel_names[i_chanl] + '.tif', img)

                    # 1.0 OTSU thresholding
                    th = filters.threshold_otsu(img)
                    img_mask = img > th
                    #io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    #    4] + '_proc_11_OTSU_roi' + str(roi_count + 1) +'_' + channel_names[i_chanl] + '.tif', img_mask)

                    # 1.1 connected component labeling
                    l_, n_ = mh.label(img.reshape(n_rows_rescale, n_cols_rescale), np.ones((3, 3), bool))  # binary_closed_hztl_k #*******************RESCALE
                    #io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    #    4] + '_proc_12_ccl_roi' + str(roi_count + 1) +'_' + channel_names[i_chanl] + '.tif', l_)

                    # 1.2 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=img.reshape(n_rows_rescale, n_cols_rescale))#*******************RESCALE
                    results = []

                    if (len(im_props)>1000):
                        continue

                    for blob in im_props:
                        properties = []
                        i_focus = blob.label
                        focus_coords = blob.coords
                        x, y, w, h = cv2.boundingRect(focus_coords)
                        single_focus_img = img[x:x + w, y:y + h]
                        single_focus_img_raw = cropped_img[x:x + w, y:y + h, i_chanl]
                        rows, cols = single_focus_img_raw.shape
                        focus_tot_pix = rows*cols
                        (h, w) = single_focus_img_raw.shape
                        if ((focus_tot_pix < 30) or (focus_tot_pix > 1700)): #or (blob.area > 1000)
                            continue
                        #blob = im_props[10] # for debugging
                        print('blob label: ' + str(blob.label))
                        properties.append(blob.label)
                        properties.append(blob.centroid[0])
                        properties.append(blob.centroid[1])
                        properties.append(blob.orientation)
                        properties.append(blob.area)
                        properties.append(blob.perimeter)
                        properties.append(blob.major_axis_length)
                        properties.append(blob.minor_axis_length)
                        properties.append(blob.eccentricity)
                        properties.append(blob.coords)
                        properties.append(blob.bbox)

                        proj_img_x = np.mean(single_focus_img, axis=0).reshape(w, )
                        proj_img_y = np.mean(single_focus_img, axis=1).reshape(h, )

                        l_x = len(proj_img_x)
                        l_y = len(proj_img_y)
                        if (l_x != l_y):
                            if (l_y > l_x):
                                diff = l_y - l_x
                                proj_img_x = np.pad(proj_img_x, [(0, diff)])

                            if (l_x > l_y):
                                diff = l_x - l_y
                                proj_img_y = np.pad(proj_img_y, [(0, diff)])

                        proj_img_x = proj_img_x
                        proj_img_y = proj_img_y
                        labels_x = np.arange(0, len(proj_img_x), 1)

                        fig = plt.figure()
                        width = 1
                        ax_x = plt.subplot(3, 1, 1)
                        ax_x.bar(labels_x, proj_img_x, width, color='g', edgecolor="k")
                        ax_x.set_xlabel("")
                        ax_x.set_title(' x ')

                        labels_y = np.arange(0, len(proj_img_y), 1)
                        ax_y = plt.subplot(3, 1, 2)
                        ax_y.bar(labels_y, proj_img_y, width, color='g', edgecolor="k")
                        ax_y.set_xlabel("")
                        ax_y.set_title(' y ')

                        ax_xy = plt.subplot(3, 1, 3)
                        rects1 = ax_xy.bar(labels_x, proj_img_x, width, color='royalblue', edgecolor="k")
                        rects2 = ax_xy.bar(labels_y, proj_img_y, width, color='seagreen', edgecolor="k")
                        ax_xy.set_xlabel("")
                        ax_xy.legend((rects1[0], rects2[0]), ('x', 'y'))
                        plt.close(fig)

                        proj_img_x_ = proj_img_x.ravel().astype('float32')
                        proj_img_y_ = proj_img_y.ravel().astype('float32')

                        compare_val_correl_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CORREL)
                        compare_val_chisq_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CHISQR)
                        compare_val_intrsct_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_INTERSECT)
                        compare_val_bc_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_BHATTACHARYYA)
                        compare_val_chisqalt_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_CHISQR_ALT)
                        compare_val_hellinger_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_HELLINGER)
                        compare_val_kl_xy = cv2.compareHist(proj_img_x_, proj_img_y_, cv2.HISTCMP_KL_DIV)

                        # pixel number
                        len_single_focus = single_focus_img.size

                        properties.append(compare_val_correl_xy)
                        properties.append(compare_val_chisq_xy)
                        properties.append(compare_val_intrsct_xy)
                        properties.append(compare_val_bc_xy)
                        properties.append(compare_val_chisqalt_xy)
                        properties.append(compare_val_hellinger_xy)
                        properties.append(compare_val_kl_xy)
                        properties.append(len_single_focus)
                        results.append(properties)

                        # save images of individual foci, with and without outlines
                        # extract focus bounding box
                        focus_bbox = blob.bbox
                        img_r1, img_c1 = img_clahe.shape
                        # define bounding box
                        pixel_border = 3
                        r0 = focus_bbox[0] - pixel_border
                        r1 = focus_bbox[2] + pixel_border
                        c0 = focus_bbox[1] - pixel_border
                        c1 = focus_bbox[3] + pixel_border
                        # check if bounding box goes beyond image limits
                        if (r0 < 0):
                            r0 = 0
                        if (r1 > img_r1):
                            r1 = img_r1
                        if (c0 < 0):
                            c0 = 0
                        if (c1 > img_c1):
                            c1 = img_c1
                        # focus with no outlines
                        img_focus_clahe = img_clahe.copy()[r0:r1, c0:c1]
                        # focus with outlines
                        # draw outlines on full nucleus ROI
                        focus_max_val = int(np.max(img_clahe)/2)
                        obj_mask = np.zeros(shape= img_clahe.shape, dtype='uint8')  # shape=draw_img.shape[:-1]
                        for i in range(len(focus_coords)):
                            obj_mask[focus_coords[i][0], focus_coords[i][1]] = 200
                        contours, hierarchy = cv2.findContours(obj_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
                        cv2.drawContours(img_clahe, contours, -1, (focus_max_val, focus_max_val, focus_max_val), 1)
                        # crop out focus of interest
                        img_focus_clahe_outlines = img_clahe[r0:r1, c0:c1]

                        # save focus in focus folder
                        # max proj
                        # no outlines
                        io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci/' + dir_[0] + '_' +
                                  dir_[1] + '_'+ dir_[2] + '_' + dir_[3] + dir_[4] + '_roi' + str(roi_count + 1) +
                                  '_focusNo' + str(i_focus) + '_' + channel_names[i_chanl] + '.tif', img_focus_clahe)
                        # outlines
                        io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/DetectedFoci_Outlines/' + dir_[0] + '_' +
                                  dir_[1] + '_'+ dir_[2] + '_' + dir_[3] + dir_[4] + '_roi' + str(roi_count + 1) +
                                  '_focusNo' + str(i_focus) + '_' + channel_names[i_chanl] + '.tif', img_focus_clahe_outlines)

                    df_foci_full = pd.DataFrame(results,
                                                columns=['focus_label', 'centroid-0', 'centroid-1', 'orientation', 'area',
                                                         'perimeter', 'major_axis_length',
                                                         'minor_axis_length', 'eccentricity', 'coords', 'bbox',
                                                         'compare_val_correl',
                                                         'compare_val_chisq', 'compare_val_intrsct', 'compare_val_bc',
                                                         'compare_val_chisqalt', 'compare_val_hellngr', 'compare_val_kl',
                                                         'tot_pix_no'])

                    # save properties
                    df_foci_full.to_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + 'focus_info_full_' + channel_names[i_chanl] + '.csv')

                    # save labeled and contoured foci
                    a = img.copy()
                    max_val = int(np.max(a))
                    img_1 = np.float32(cropped_img[:,:, i_chanl])
                    raw_max_val = int(np.max(img_1))
                    center_coordinates = 0
                    box = 0
                    # draw on all detected and processed foci, contours only
                    for focus_full_i, focus_full_row in df_foci_full.iterrows():
                        draw_on_img(file, file_root, a, focus_full_row['bbox'], focus_full_row['coords'],
                                    center_coordinates, box, 1, 1,
                                    (max_val, max_val, max_val), draw_bbox=False,
                                    draw_contours=True, draw_label=True,
                                    label=str(focus_full_row['focus_label']))  # str(focus_full_i)
                    #io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                    #    4] + '_procfocusDetectionCheck_Full_roi'+ str(roi_count + 1) +'_' + channel_names[i_chanl] + '.tif', a)

                    # draw on all detected and processed foci, contours only
                    for focus_full_i, focus_full_row in df_foci_full.iterrows():
                        #print(focus_full_i) # for debugging
                        draw_on_img(file, file_root, img_1, focus_full_row['bbox'], focus_full_row['coords'],
                                    center_coordinates, box, 1, 1, (raw_max_val, raw_max_val, raw_max_val), draw_bbox=False,
                                    draw_contours=True, draw_label=True, label=str(focus_full_row['focus_label']))  # str(focus_full_i)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_rawfocusDetectionCheck_Full_roi' + str(roi_count + 1) +'_' + channel_names[i_chanl] + '.tif', img_1.astype('uint16'))

                    stop = 1

    print("Done.")
    stop = 1

def measure_foci(summary_dir, dir_list):
    # load csv files for all three datasets
    base_dirs_full = ['H:/2021_FociData', 'C:']
    df_foci = pd.read_csv(base_dirs_full[0] + '/foci_info.csv')#pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci_idr = pd.read_csv(base_dirs_full[0] + '/foci_info_idr.csv')
    df_foci_df = pd.read_csv(base_dirs_full[0] + '/foci_info_df.csv')
    df_foci_df = df_foci_df[df_foci_df.tot_pix_no < 1000]
    # join column with total pixel number (focus roi) from each dataset
    datasets_full = ['train','test_idr','test_df']
    totpixno_flat_rescaled = df_foci['tot_pix_no'].tolist() + df_foci_idr['tot_pix_no'].tolist() + df_foci_df['tot_pix_no'].tolist()
    train_label = [datasets_full[0]] * len(df_foci['tot_pix_no'])
    test_idr_label = [datasets_full[1]] * len(df_foci_idr['tot_pix_no'])
    test_df_label = [datasets_full[2]] * len(df_foci_df['tot_pix_no'])
    # calculate medians
    median_train = df_foci['tot_pix_no'].median()
    median_test_idr = df_foci_idr['tot_pix_no'].median()
    median_test_df = df_foci_df['tot_pix_no'].median()
    # boxplot of total pixel number (focus roi) for all three datasets
    labels_flat_rescaled = train_label + test_idr_label + test_df_label
    sns.boxplot(y=labels_flat_rescaled, x=totpixno_flat_rescaled);
    plt.show()
    # histogram of total pixel number for all three datasets
    plt.close()
    bins_totpixno = np.linspace(min(totpixno_flat_rescaled), max(totpixno_flat_rescaled), 20)
    plt.hist(df_foci['tot_pix_no'], bins_totpixno, density=True, label=datasets_full[0], alpha=0.5)
    plt.hist(df_foci_idr['tot_pix_no'], bins_totpixno, density=True, label=datasets_full[1], alpha=0.5)
    plt.hist(df_foci_df['tot_pix_no'], bins_totpixno, density=True, label=datasets_full[2], alpha=0.5)
    plt.legend(loc='upper right')
    plt.title('Total Pixel Number, No Rescale')
    plt.show()
    stop = 1

def measure_rois_foci(summary_dir, dir_list):

    #******************************************************************************
    # START : LOAD PROPERTIES
    base_dirs_full = ['H:/2021_FociData', 'C:']
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    #df_foci = pd.read_csv(base_dirs_full[0] + '/foci_info.csv')#pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci = df_foci.loc[ (df_foci['status'] == 1) | (df_foci['status'] == 3) | (df_foci['status'] == 4) | (df_foci['status'] == 5)]
    df_foci_og = df_foci
    #df_foci_idr = pd.read_csv(base_dirs_full[0] + '/foci_info_idr.csv')
    df_foci_df = pd.read_csv(base_dirs_full[0] + '/foci_info_df.csv')
    #df_foci_df = df_foci_df[df_foci_df.tot_pix_no < 1000]
    # TRAINING DATASET
    datasets_full = ['train','test_idr','test_df']
    df_foci_full = [df_foci, df_foci_idr, df_foci_df]
    datasets = []
    exp_folders =[]
    i = 0
    # put all exp_folders in single list
    for i_df in df_foci_full:
        i_exp_folder = i_df['exp_folder'].unique()
        for i_exp in i_exp_folder:
            datasets.append(datasets_full[i])
            exp_folders.append(i_exp)
        i = i + 1
    roi_dims_full = []
    # measure roi dims for datasets
    for i_exp, exp in enumerate(exp_folders):
        # assign appropriate dataset
        if (datasets[i_exp] == datasets_full[0]): # train
            i_df_foci = df_foci_full[0]
            base_dir = base_dirs_full[0]
            df_foci_exp = i_df_foci.loc[i_df_foci['exp_folder'] == exp]
            roi_ids = df_foci_exp['roi_id'].unique()
            exp_split = exp.split('_')  # training
            # take roi measurements for each roi
            for roi_id in roi_ids:
                roi_id_split = roi_id.split('_')
                sample = roi_id_split[0]
                spool = roi_id_split[1] + '_' + roi_id_split[2]
                roi_img = exp_split[0] + '_' + sample + roi_id_split[1] + '_' + roi_id_split[
                    2] + '_rawfocusDetectionCheck_Full.tif'
                img_dir = base_dir + '/' + exp + '/Output/' + sample + '/' + spool + '/' + roi_id + '/' + roi_img
                # import roi
                img = io.imread(img_dir)
                # take roi measurements
                shape = img.shape
                row = shape[0]
                col = shape[1]
                tot_pix_no = row * col
                df_foci_exp_roi = df_foci_exp.loc[df_foci_exp ['roi_id'] == roi_id]
                for i_focus, row_focus in df_foci_exp_roi.iterrows():
                    roi_dims = []
                    focus_tot_pix_no = row_focus['tot_pix_no']
                    # append to local list
                    roi_dims.append(datasets[i_exp])
                    roi_dims.append(exp)
                    roi_dims.append(roi_id)
                    roi_dims.append(row)
                    roi_dims.append(col)
                    roi_dims.append(tot_pix_no)
                    roi_dims.append(focus_tot_pix_no)
                    # append to final list
                    roi_dims_full.append(roi_dims)

        elif(datasets[i_exp] == datasets_full[1]): # test idr
            i_df_foci = df_foci_full[1]
            base_dir = base_dirs_full[0]
            roi_ids = i_df_foci['roi_id'].unique()
            # take roi measurements for each roi
            for roi_id in roi_ids:
                test = i_df_foci[i_df_foci['roi_id'] == roi_id].reset_index()
                # take roi measurements
                row = test['roi_total_rows'][0]
                col = test['roi_total_cols'][0]
                roi_tot_pix_no = test['roi_tot_pix_no'][0]
                for i_focus, row_focus in test.iterrows():
                    roi_dims = []
                    focus_tot_pix_no = row_focus['tot_pix_no']
                    # append to local list
                    roi_dims.append(datasets[i_exp])
                    roi_dims.append(exp)
                    roi_dims.append(roi_id)
                    roi_dims.append(row)
                    roi_dims.append(col)
                    roi_dims.append(roi_tot_pix_no)
                    roi_dims.append(focus_tot_pix_no)
                    # append to final list
                    roi_dims_full.append(roi_dims)

        elif(datasets[i_exp] == datasets_full[2]): # test df
            i_df_foci = df_foci_full[2]
            base_dir = base_dirs_full[1]
            roi_ids = i_df_foci['roi_id'].unique()
            # take roi measurements for each roi
            for roi_id in roi_ids:
                test = i_df_foci[i_df_foci['roi_id'] == roi_id].reset_index()
                # take roi measurements
                row = test['roi_total_rows'][0]
                col = test['roi_total_rows'][0]
                roi_tot_pix_no = test['roi_tot_pix_no'][0]
                for i_focus, row_focus in test.iterrows():
                    roi_dims = []
                    focus_tot_pix_no = row_focus['tot_pix_no']
                    # append to local list
                    roi_dims.append(datasets[i_exp])
                    roi_dims.append(exp)
                    roi_dims.append(roi_id)
                    roi_dims.append(row)
                    roi_dims.append(col)
                    roi_dims.append(roi_tot_pix_no)
                    roi_dims.append(focus_tot_pix_no)
                    # append to final list
                    roi_dims_full.append(roi_dims)

    # convert list to df
    df_roi_dims = pd.DataFrame(roi_dims_full, columns=["dataset", "exp_folder", "roi_id", "roi_r", "roi_c", "roi_tot_pix_no", 'focus_tot_pix_no'])

    # focus statistics
    focus_medians = []
    focus_max = []
    focus_min = []
    focus_max_r = []
    focus_max_c = []
    roi_medians = []
    for i_dataset in datasets_full:
        focus_medians.append(df_roi_dims[df_roi_dims['dataset'] == i_dataset]['focus_tot_pix_no'].median())
        focus_max.append(df_roi_dims[df_roi_dims['dataset'] == i_dataset]['focus_tot_pix_no'].max())
        focus_max_r.append(df_roi_dims[df_roi_dims['dataset'] == i_dataset]['roi_r'].max())
        focus_max_c.append(df_roi_dims[df_roi_dims['dataset'] == i_dataset]['roi_c'].max())
        focus_min.append(df_roi_dims[df_roi_dims['dataset'] == i_dataset]['focus_tot_pix_no'].min())
        roi_medians.append(df_roi_dims[df_roi_dims['dataset'] == i_dataset]['roi_tot_pix_no'].median())

    # calculate ratio
    focus_ratio_idr = focus_medians[0]/focus_medians[1]
    focus_ratio_df = focus_medians[0]/focus_medians[2]

    # plots of total pixel number for all three datasets
    # focus
    sns.boxplot(y= df_roi_dims['dataset'], x= df_roi_dims['focus_tot_pix_no'])
    plt.show()
    plt.close()
    sns.histplot(data = df_roi_dims, x='focus_tot_pix_no', hue = 'dataset')
    plt.show()
    # roi
    plt.close()
    sns.boxplot(y=df_roi_dims['dataset'], x=df_roi_dims['roi_tot_pix_no'])
    plt.show()
    STOP = 1


def measure_rois(summary_dir, dir_list):

    #******************************************************************************
    # START : LOAD PROPERTIES
    base_dirs_full = ['H:/2021_FociData', 'C:']
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    #df_foci = pd.read_csv(base_dirs_full[0] + '/foci_info.csv')#pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci_og = df_foci
    #df_foci_idr = pd.read_csv(base_dirs_full[0] + '/foci_info_idr.csv')
    df_foci_df = pd.read_csv(base_dirs_full[0] + '/foci_info_df.csv')
    #df_foci_df = df_foci_df[df_foci_df.tot_pix_no < 1000]
    # TRAINING DATASET
    datasets_full = ['train','test_idr','test_df']
    df_foci_full = [df_foci, df_foci_idr, df_foci_df]
    datasets = []
    exp_folders =[]
    i = 0
    # put all exp_folders in single list
    for i_df in df_foci_full:
        i_exp_folder = i_df['exp_folder'].unique()
        for i_exp in i_exp_folder:
            datasets.append(datasets_full[i])
            exp_folders.append(i_exp)
        i = i + 1
    roi_dims_full = []
    # measure roi dims for training dataset
    for i_exp, exp in enumerate(exp_folders):
        # assign appropriate dataset
        if (datasets[i_exp] == datasets_full[0]): # train
            i_df_foci = df_foci_full[0]
            base_dir = base_dirs_full[0]
            df_foci_exp = i_df_foci.loc[i_df_foci['exp_folder'] == exp]
            roi_ids = df_foci_exp['roi_id'].unique()
            exp_split = exp.split('_')  # training
            # take roi measurements for each roi
            for roi_id in roi_ids:
                roi_dims = []
                roi_id_split = roi_id.split('_')
                sample = roi_id_split[0]
                spool = roi_id_split[1] + '_' + roi_id_split[2]
                roi_img = exp_split[0] + '_' + sample + roi_id_split[1] + '_' + roi_id_split[
                    2] + '_rawfocusDetectionCheck_Full.tif'
                img_dir = base_dir + '/' + exp + '/Output/' + sample + '/' + spool + '/' + roi_id + '/' + roi_img
                # import roi
                img = io.imread(img_dir)
                # take roi measurements
                shape = img.shape
                row = shape[0]
                col = shape[1]
                tot_pix_no = row * col
                # append to local list
                roi_dims.append(datasets[i_exp])
                roi_dims.append(exp)
                roi_dims.append(roi_id)
                roi_dims.append(row)
                roi_dims.append(col)
                roi_dims.append(tot_pix_no)
                # append to final list
                roi_dims_full.append(roi_dims)

        elif(datasets[i_exp] == datasets_full[1]): # test idr
            i_df_foci = df_foci_full[1]
            base_dir = base_dirs_full[0]
            roi_ids = i_df_foci['roi_id'].unique()
            # take roi measurements for each roi
            for roi_id in roi_ids:
                roi_dims = []
                test = i_df_foci[i_df_foci['roi_id'] == roi_id].reset_index()
                # take roi measurements
                row = test['roi_total_rows'][0]
                col = test['roi_total_cols'][0]
                tot_pix_no = test['roi_tot_pix_no'][0]
                # append to local list
                roi_dims.append(datasets[i_exp])
                roi_dims.append(exp)
                roi_dims.append(roi_id)
                roi_dims.append(row)
                roi_dims.append(col)
                roi_dims.append(tot_pix_no)
                # append to final list
                roi_dims_full.append(roi_dims)

        elif(datasets[i_exp] == datasets_full[2]): # test df
            i_df_foci = df_foci_full[2]
            base_dir = base_dirs_full[1]
            roi_ids = i_df_foci['roi_id'].unique()
            # take roi measurements for each roi
            for roi_id in roi_ids:
                roi_dims = []
                test = i_df_foci[i_df_foci['roi_id'] == roi_id].reset_index()
                # take roi measurements
                row = test['roi_total_rows'][0]
                col = test['roi_total_rows'][0]
                tot_pix_no = test['roi_tot_pix_no'][0]
                # append to local list
                roi_dims.append(datasets[i_exp])
                roi_dims.append(exp)
                roi_dims.append(roi_id)
                roi_dims.append(row)
                roi_dims.append(col)
                roi_dims.append(tot_pix_no)
                # append to final list
                roi_dims_full.append(roi_dims)

    # convert list to df
    df_roi_dims = pd.DataFrame(roi_dims_full, columns=["dataset", "exp_folder", "roi_id", "r", "c", "tot_pix_no"])
    # measure roi statistics for all datasets
    tot_pix_no_full = []
    tot_pix_no_med_full = []
    row_full = []
    row_med_full = []
    col_full = []
    col_med_full = []
    for i_data in datasets_full:
        i_dataset = df_roi_dims.loc[df_roi_dims['dataset'] == i_data]
        tot_pix_no_full.append(i_dataset["tot_pix_no"])
        tot_pix_no_med_full.append(i_dataset["tot_pix_no"].median())
        if (i_data == 'train'):
            tot_pix_no_min = i_dataset["tot_pix_no"].min()
            tot_pix_no_max = i_dataset["tot_pix_no"].max()
            row_min = i_dataset["r"].min()
            row_max = i_dataset["r"].max()
            col_min = i_dataset["c"].min()
            col_max = i_dataset["c"].max()
        row_full.append(i_dataset["r"])
        row_med_full.append(i_dataset["r"].median())
        col_full.append(i_dataset["c"])
        col_med_full.append(i_dataset["c"].median())

    # rescaled histograms
    # total pix no
    # before rescale
    plt.close()
    bins_totpixno = np.linspace(df_roi_dims['tot_pix_no'].min(), df_roi_dims['tot_pix_no'].max(), 20)
    plt.hist(tot_pix_no_full[0], bins_totpixno, density=True, label = datasets_full[0], alpha = 0.5)
    plt.hist(tot_pix_no_full[1], bins_totpixno, density=True, label = datasets_full[1], alpha = 0.5)
    plt.hist(tot_pix_no_full[2], bins_totpixno, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Total Pixel Number, No Rescale')
    plt.show()
    # calculate rescale factor
    rescale_factor_totpixno_idr = tot_pix_no_med_full[0]/tot_pix_no_med_full[1]
    rescale_factor_totpixna_df = tot_pix_no_med_full[0]/tot_pix_no_med_full[2]
    # rescale
    totpixno_idr_rescaled = tot_pix_no_full[1] * rescale_factor_totpixno_idr
    totpixno_df_rescaled = tot_pix_no_full[2] * rescale_factor_totpixno_df
    totpixno_flat_rescaled = tot_pix_no_full[0].tolist() + totpixno_idr_rescaled.tolist() + totpixno_df_rescaled.tolist()
    # plot histograms for all datasets
    plt.close()
    bins_totpixno = np.linspace(min(totpixno_flat_rescaled), max(totpixno_flat_rescaled), 20)
    plt.hist(tot_pix_no_full[0], bins_totpixno, density=True, label = datasets_full[0], alpha = 0.5)
    plt.hist(totpixno_idr_rescaled, bins_totpixno, density=True, label = datasets_full[1], alpha = 0.5)
    plt.hist(totpixno_df_rescaled, bins_totpixno, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Total Pixel Number, Rescaled')
    plt.show()
    # row
    # before rescale
    plt.close()
    bins_row = np.linspace(df_roi_dims['r'].min(), df_roi_dims['r'].max(), 20)
    plt.hist(row_full[0], bins_row, density=True,label = datasets_full[0], alpha = 0.5)
    plt.hist(row_full[1], bins_row, density=True,label = datasets_full[1], alpha = 0.5)
    plt.hist(row_full[2], bins_row, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Row No., No Rescaled')
    plt.show()
    # calculate rescale factor
    rescale_factor_row_idr = row_med_full[0]/row_med_full[1]
    rescale_factor_row_df = row_med_full[0]/row_med_full[2]
    # rescale
    row_idr_rescaled = row_full[1] * rescale_factor_row_idr
    row_df_rescaled = row_full[2] * rescale_factor_row_df
    row_flat_rescaled = row_full[0].tolist() + row_idr_rescaled.tolist() + row_df_rescaled.tolist()
    # plot histograms for all datasets
    plt.close()
    bins_row = np.linspace(min(row_flat_rescaled), max(row_flat_rescaled), 20)
    plt.hist(row_full[0], bins_row, density=True,label = datasets_full[0], alpha = 0.5)
    plt.hist(row_idr_rescaled, bins_row, density=True,label = datasets_full[1], alpha = 0.5)
    plt.hist(row_df_rescaled, bins_row, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Row No., Rescaled')
    plt.show()
    # col
    # before rescale
    plt.close()
    bins_col = np.linspace(df_roi_dims['c'].min(), df_roi_dims['c'].max(), 20)
    plt.hist(col_full[0], bins_col, density=True,label = datasets_full[0], alpha = 0.5)
    plt.hist(col_full[1], bins_col, density=True,label = datasets_full[1], alpha = 0.5)
    plt.hist(col_full[2], bins_col, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Column No., No Rescaled')
    plt.show()
    # calculate rescale factor
    rescale_factor_col_idr = col_med_full[0]/col_med_full[1]
    rescale_factor_col_df = col_med_full[0]/col_med_full[2]
    # rescale
    col_idr_rescaled = col_full[1] * rescale_factor_col_idr
    col_df_rescaled = col_full[2] * rescale_factor_col_df
    col_flat_rescaled = col_full[0].tolist() + col_idr_rescaled.tolist() + col_df_rescaled.tolist()
    # plot histograms for all datasets
    plt.close()
    bins_col = np.linspace(min(col_flat_rescaled), max(col_flat_rescaled), 20)
    plt.hist(col_full[0], bins_col, density=True,label = datasets_full[0], alpha = 0.5)
    plt.hist(col_idr_rescaled, bins_col, density=True,label = datasets_full[1], alpha = 0.5)
    plt.hist(col_df_rescaled, bins_col, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Column No., Rescaled')
    plt.show()

    # final rescale factors
    idr_rescale = (rescale_factor_row_idr + rescale_factor_col_idr)/2 # 10.2
    df_rescale = (rescale_factor_row_df + rescale_factor_col_df)/2 # 1.47

    # row rescale
    row_idr_rescaled = row_full[1] * idr_rescale
    row_df_rescaled = row_full[2] * df_rescale
    row_flat_rescaled = row_full[0].tolist() + row_idr_rescaled.tolist() + row_df_rescaled.tolist()
    # plot histograms for all datasets
    plt.close()
    bins_row = np.linspace(min(row_flat_rescaled), max(row_flat_rescaled), 20)
    plt.hist(row_full[0], bins_row, density=True,label = datasets_full[0], alpha = 0.5)
    plt.hist(row_idr_rescaled, bins_row, density=True,label = datasets_full[1], alpha = 0.5)
    plt.hist(row_df_rescaled, bins_row, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Row No., Rescaled')
    plt.show()
    # col rescale
    col_idr_rescaled = col_full[1] * idr_rescale
    col_df_rescaled = col_full[2] * df_rescale
    col_flat_rescaled = col_full[0].tolist() + col_idr_rescaled.tolist() + col_df_rescaled.tolist()
    # plot histograms for all datasets
    plt.close()
    bins_col = np.linspace(min(col_flat_rescaled), max(col_flat_rescaled), 20)
    plt.hist(col_full[0], bins_col, density=True,label = datasets_full[0], alpha = 0.5)
    plt.hist(col_idr_rescaled, bins_col, density=True,label = datasets_full[1], alpha = 0.5)
    plt.hist(col_df_rescaled, bins_col, density=True, label = datasets_full[2], alpha = 0.5)
    plt.legend(loc='upper right')
    plt.title('Column No., Rescaled')
    plt.show()

    stop = 1

    ratio = num  # idr: 10.2 , df: 1.47
    new_height = int(height * ratio)
    new_width = int(width * ratio)
    dimensions = (new_width, new_height)
    new_image = cv2.resize(image, dimensions, interpolation=cv2.INTER_LINEAR)

    # END : LOAD PROPERTIES
    #******************************************************************************


def measure_props(summary_dir, dir_list):
    all_files = []
    #dir_list= dir_list[12:14] # for debugging
    for dir_ in dir_list:
        #dir_ = dir_list[7] # for debugging
        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.roi')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.roi')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        # skip if there are no rois
        if (len(all_files) < 1):
            continue
        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/*.roi')
        if (len(roi_list) == 1):
            roi_is_zip = False
        elif (len(roi_list) < 1):
            # more than one ROI
            roi_list = glob(dir_[5] + '/*.zip')
            roi_is_zip = True
        else:
            print("Missing ROI file for movie file")
            continue

        for roi_file in roi_list:
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            #roi_points_test = roi_points[1] # for debugging
            for roi_count, bbox_points in enumerate(roi_points):#enumerate(roi_points_test):
                # roi_count = 1 # for debugging
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = sep_file_[1][:-7]

                test = dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv'
                test_ = os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == True
                if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == True):
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv')
                else:
                    continue

                properties_full = []
                #df_foci_test = df_foci.iloc[8:10]
                for index, row in df_foci.iterrows():
                    #print(index), 28
                    #index = 70
                    #row = df_foci.iloc[26]
                    print('Focus label: ' + str(row['focus_label']))
                    print('Index: ' + str(index))
                    properties = []
                    test = roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_proc_10_maxproj_roi' + str(roi_count + 1) + '.tif'

                    # import images
                    # max proj
                    # no outlines
                    img = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_proc_10_maxproj_roi' + str(roi_count + 1) + '.tif') # import max projection image
                    # outlines
                    img_outlines = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_procfocusDetectionCheck_Full.tif')
                    # max proj images of raw data
                    # no outlines
                    img_raw = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_proc_10_rawmaxproj_roi' + str(roi_count + 1) + '.tif') # import max projection image
                    # outlines
                    img_raw_outlines = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_rawfocusDetectionCheck_Full.tif')

                    # extract focus bounding box
                    bbox = row['bbox'].split('(', 2)
                    test1 = bbox[1].split(')', 2)
                    test2 = test1[0].split(',', 4)
                    results = list(map(int, test2))

                    pixel_border = 3
                    r0 = results[0] - pixel_border
                    r1 = results[2] + pixel_border
                    c0 = results[1] - pixel_border
                    c1 = results[3] + pixel_border
                    img_test = img[results[0]:results[2], results[1]:results[3]]
                    img_test = (img_test - np.min(img_test)) / (np.max(img_test) - np.min(img_test)) # scale from 0 to 1
                    img_test = np.multiply(img_test, 255) # scale from 0 to 255
                    img_test = img_test.astype(np.float32) # 'uint32'
                    # pixel no
                    len_img = img_test.size

                    # A) BLUR DETECTION
                    # A.0) blur detection with variance of laplacian
                    # 0 mean proj single foci
                    var_laplacian = cv2.Laplacian(img_test, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_0vl_test_.tif', img_test) # save img
                    laplacian = cv2.Laplacian(img_test, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_0l_test_.tif', laplacian) # save img

                    # 1 gaussian blur 0 of 0
                    img_test_gb0 = cv2.GaussianBlur(img_test, (3, 3), 0)
                    var_laplacian_gb0 = cv2.Laplacian(img_test_gb0, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_1vlgb0_test.tif', img_test_gb0) # save img
                    laplacian_gb0 = cv2.Laplacian(img_test_gb0, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_1l_gb0_test.tif', laplacian_gb0) # save img

                    # 2 gaussian blur 1 of 0
                    img_test_gb1 = cv2.GaussianBlur(img_test, (5, 5), 0)
                    var_laplacian_gb1 = cv2.Laplacian(img_test_gb1, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_2vlgb1_test.tif', img_test_gb1) # save img
                    laplacian_gb1 = cv2.Laplacian(img_test_gb1, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_2l_gb1_test.tif', laplacian_gb1) # save img


                    # 3 gaussian blur 2 of 0
                    img_test_gb2 = cv2.GaussianBlur(img_test, (7, 7), 0)
                    var_laplacian_gb2 = cv2.Laplacian(img_test_gb2, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_3vlgb2_test.tif', img_test_gb2) # save img
                    laplacian_gb2 = cv2.Laplacian(img_test_gb2, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_3l_gb2_test.tif', laplacian_gb2) # save img


                    # 4 gaussian blur 3 of 0
                    img_test_gb3 = cv2.GaussianBlur(img_test, (9, 9), 0)
                    var_laplacian_gb3 = cv2.Laplacian(img_test_gb3, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_4vlgb3_test.tif', img_test_gb3) # save img
                    laplacian_gb3 = cv2.Laplacian(img_test_gb3, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_4l_gb3_test.tif', laplacian_gb3) # save img

                    # A.1) blur detection using measure function
                    blur_2 = measure.blur_effect(img_test)

                    # B) shannon entropy
                    shannon_entropy = measure.shannon_entropy(img_test)

                    # C) stats
                    focus_mean = np.mean(img_test)
                    focus_med = np.median(img_test)
                    focus_sum = np.sum(img_test)
                    focus_std = np.std(img_test)
                    focus_var = np.var(img_test)
                    focus_max = np.max(img_test)
                    focus_min = np.min(img_test)

                    # D) compactness
                    focus_area = row['area']
                    focus_perim = row['perimeter']

                    if (focus_perim < 1):
                        df_foci.drop(index, inplace=True)
                        print('Focus is too small. Focus dropped.')
                        continue

                    # D.0) Polsby-Popper
                    compactness_pp = (4 * pi * focus_area) / (focus_perim * focus_perim)

                    # D.1) Schwartzberg
                    compactness_s = 1 / (focus_perim / (2 * pi * sqrt(focus_area / pi)))

                    # E) other geometric properties
                    # 1.2 connected component labeling
                    # img_mask.reshape(n_rows, n_cols)
                    l_, n_ = mh.label(img_test, np.ones((3, 3), bool))  # binary_closed_hztl_k
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_CCL_mag.tif', l_)

                    # 1.3 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=img_test)
                    results = []

                    # only append focus properties if single object is detected in magnitude spectrum
                    if (len(im_props) == 1):
                        for focus_blob in im_props:
                            # if (focus_blob.area > 1000):
                            #    continue
                            # blob = im_props[30]
                            properties.append(focus_blob.euler_number)
                            properties.append(focus_blob.solidity)
                            stop = 1
                    else:
                        df_foci.drop(index, inplace=True)
                        print('Focus bbox has more or less than 1 object. Focus dropped.')
                        continue

                    # F) FFT Magnitude properties
                    # ft
                    f = np.fft.fft2(img_test)
                    # magnitude: ft
                    magnitude_spectrum_ft = 20 * np.log(np.abs(f))
                    # shift ft
                    fshift = np.fft.fftshift(f)
                    # magnitude: shifted ft
                    magnitude_spectrum_fshift = 20 * np.log(np.abs(fshift))
                    magnitude_spectrum_fshift_32F = magnitude_spectrum_fshift.astype(np.float32)  # 'uint16'

                    # F.A) Blur detection
                    # F.A.0) Blur detection, Variance of Laplacian
                    fft_var_laplacian = cv2.Laplacian(magnitude_spectrum_fshift_32F, cv2.CV_32F).var() # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_5mag_.tif', magnitude_spectrum_fshift_32F) # save img
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_5vlm_.tif', magnitude_spectrum_fshift_32F) # save img

                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_input5mag_.tif', img_test)

                    # F.A.1) blur detection using measure function
                    fft_blur_2 = measure.blur_effect(magnitude_spectrum_fshift_32F)

                    # F.B) shannon entropy
                    fft_shannon_entropy = measure.shannon_entropy(magnitude_spectrum_fshift_32F)

                    # F.C) Stats
                    fft_mean = np.mean(magnitude_spectrum_fshift_32F)
                    fft_med = np.median(magnitude_spectrum_fshift_32F)
                    fft_sum = np.sum(magnitude_spectrum_fshift_32F)
                    fft_std = np.std(magnitude_spectrum_fshift_32F)
                    fft_var = np.var(magnitude_spectrum_fshift_32F)
                    fft_max = np.max(magnitude_spectrum_fshift_32F)
                    fft_min = np.min(magnitude_spectrum_fshift_32F)

                    # F.D) Other Geometric Properties
                    # 1.1 OTSU thresholding
                    magnitude_spectrum_fshift_32F_gb = cv2.GaussianBlur(magnitude_spectrum_fshift_32F, (3, 3), 0)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_gb3_mag_.tif', magnitude_spectrum_fshift_32F_gb)

                    th = filters.threshold_otsu(magnitude_spectrum_fshift_32F_gb)
                    img_mask = magnitude_spectrum_fshift_32F_gb > th
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_OTSUthresh_mag.tif', img_mask)

                    # 1.2 connected component labeling
                    # img_mask.reshape(n_rows, n_cols)
                    l_, n_ = mh.label(img_mask, np.ones((3, 3), bool))  # binary_closed_hztl_k
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_CCL_mag_.tif', l_)

                    # 1.3 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=magnitude_spectrum_fshift_32F_gb)
                    results = []

                    # only append focus properties if single object is detected in magnitude spectrum
                    if (len(im_props) == 1):
                        for blob in im_props:
                            # if (blob.area > 1000):
                            #    continue
                            # blob = im_props[30]
                            properties.append(blob.orientation)
                            properties.append(blob.area)
                            properties.append(blob.perimeter)
                            properties.append(blob.major_axis_length)
                            properties.append(blob.minor_axis_length)
                            properties.append(blob.eccentricity)
                            properties.append(blob.euler_number)
                            properties.append(blob.solidity)

                            fft_area = blob.area
                            fft_perimeter = blob.perimeter
                            # F.E) compactness
                            # F.E.0) Polsby-Popper
                            fft_compactness_pp = (4 * pi * fft_area) / (focus_perim * fft_perimeter)
                            properties.append(fft_compactness_pp)
                            # F.E.1) Schwartzberg
                            fft_compactness_s = 1 / (fft_perimeter / (2 * pi * sqrt(fft_area / pi)))
                            properties.append(fft_compactness_s)
                    else:
                        df_foci.drop(index, inplace=True)
                        print('FFT magnitude has more or less than 1 object. Focus dropped.')
                        continue

                    area = properties[1]
                    if (area > 1000):
                        df_foci.drop(index, inplace=True)
                        print('FFT magnitude object area is too large. Focus dropped.')
                        continue

                    # Append rest of fft properties
                    properties.append(fft_var_laplacian)
                    properties.append(fft_blur_2)
                    properties.append(fft_shannon_entropy)
                    properties.append(fft_mean)
                    properties.append(fft_med)
                    properties.append(fft_sum)
                    properties.append(fft_std)
                    properties.append(fft_var)
                    properties.append(fft_max)
                    properties.append(fft_min)

                    # Append rest of focus properties
                    properties.append(var_laplacian)
                    properties.append(var_laplacian_gb0)
                    properties.append(var_laplacian_gb1)
                    properties.append(var_laplacian_gb2)
                    properties.append(var_laplacian_gb3)
                    properties.append(blur_2)
                    properties.append(shannon_entropy)
                    properties.append(focus_mean)
                    properties.append(focus_med)
                    properties.append(focus_sum)
                    properties.append(focus_std)
                    properties.append(focus_var)
                    properties.append(focus_max)
                    properties.append(focus_min)
                    properties.append(compactness_pp)
                    properties.append(compactness_s)
                    properties.append(len_img)

                    if (np.isnan(properties).any() == True):
                        print("Drop focus, contains nan in metrics.")
                        df_foci.drop(index, inplace=True)
                        continue
                    properties_full.append(properties)

                if (len(df_foci) < 1):
                    print('Focus dataframe empty. Skip this ROI.')
                    continue
                properties_full_ar = np.array(properties_full)

                df_foci['euler_number'] = properties_full_ar[:, 0]
                df_foci['solidity'] = properties_full_ar[:, 1]

                df_foci['fft_orientation'] = properties_full_ar[:, 2]
                df_foci['fft_area'] = properties_full_ar[:, 3]
                df_foci['fft_perimeter'] = properties_full_ar[:, 4]
                df_foci['fft_major_axis_length'] = properties_full_ar[:, 5]
                df_foci['fft_minor_axis_length'] = properties_full_ar[:, 6]
                df_foci['fft_eccentricity'] = properties_full_ar[:, 7]
                df_foci['fft_euler_number'] = properties_full_ar[:, 8]
                df_foci['fft_solidity'] = properties_full_ar[:, 9]
                df_foci['fft_compactness_pp'] = properties_full_ar[:, 10]
                df_foci['fft_compactness_s'] = properties_full_ar[:, 11]
                df_foci['fft_var_laplacian'] = properties_full_ar[:, 12]
                df_foci['fft_blur_2'] = properties_full_ar[:, 13]
                df_foci['fft_shannon_entropy'] = properties_full_ar[:, 14]
                df_foci['fft_mean'] = properties_full_ar[:, 15]
                df_foci['fft_med'] = properties_full_ar[:, 16]
                df_foci['fft_sum'] = properties_full_ar[:, 17]
                df_foci['fft_std'] = properties_full_ar[:, 18]
                df_foci['fft_var'] = properties_full_ar[:, 19]
                df_foci['fft_max'] = properties_full_ar[:, 20]
                df_foci['fft_min'] = properties_full_ar[:, 21]

                df_foci['var_lap'] = properties_full_ar[:, 22]
                df_foci['var_lap_gb3'] = properties_full_ar[:, 23]
                df_foci['var_lap_gb5'] = properties_full_ar[:, 24]
                df_foci['var_lap_gb7'] = properties_full_ar[:, 25]
                df_foci['var_lap_gb9'] = properties_full_ar[:, 26]
                df_foci['blur_2'] = properties_full_ar[:, 27]
                df_foci['shannon_entropy'] = properties_full_ar[:, 28]
                df_foci['focus_mean'] = properties_full_ar[:, 29]
                df_foci['focus_med'] = properties_full_ar[:, 30]
                df_foci['focus_sum'] = properties_full_ar[:, 31]
                df_foci['focus_std'] = properties_full_ar[:, 32]
                df_foci['focus_var'] = properties_full_ar[:, 33]
                df_foci['focus_max'] = properties_full_ar[:, 34]
                df_foci['focus_min'] = properties_full_ar[:, 35]
                df_foci['compactness_pp'] = properties_full_ar[:, 36]
                df_foci['compactness_s'] = properties_full_ar[:, 37]
                df_foci['tot_pix_no'] = properties_full_ar[:, 38]

                df_foci.to_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + 'focus_info_full_props.csv')

                stop = 1
    print('Next experiment...')


def measure_props_idr(summary_dir, dir_list):
    all_files = []
    #dir_list = [dir_list[i] for i in [2,3,4,5,6,7,8,9,10,12,13,14,15,16,17,18,19,20,21,23]]
    #dir_list = dir_list[1:3]  # for debugging
    for dir_ in dir_list:
        #dir_ = dir_list[6] # for debugging
        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.zip')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.zip')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        # skip if there are no rois
        if (len(all_files) < 1):
            continue
        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        spool_list_file = spool_list[0]
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip') #glob(dir_[5] + '/' + '001-23 53bp10011_roi' + '*.zip')
        roi_is_zip = True

        for roi_file in roi_list:
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            #roi_points_test = roi_points[1] # for debugging
            for roi_count, bbox_points in enumerate(roi_points):#enumerate(roi_points_test):
                # roi_count = 1 # for debugging
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = sep_file_[1][:-7]

                df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full.csv') #focus_info_full_status.csv
                '''
                # check for csv file with manually classified foci
                if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == True):
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv')
                else:
                    continue
                '''

                #df_foci = pd.read_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[4] + 'focus_info_full.csv')
                properties_full = []
                #df_foci_test = df_foci.iloc[26]
                for index, row in df_foci.iterrows():
                    focus_label = row['focus_label']
                    #print(index), 28
                    #index = 70
                    #row = df_foci.iloc[26]
                    print('Focus label: ' + str(row['focus_label']))
                    print('Index: ' + str(index))
                    properties = []
                    img = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[4] + '_preproc_04_gaussblur_roi'+ str(roi_count + 1) + '.tif') # import max projection image

                    # extract focus bounding box
                    bbox = row['bbox'].split('(', 2)
                    test1 = bbox[1].split(')', 2)
                    test2 = test1[0].split(',', 4)
                    results = list(map(int, test2))
                    img_test = img[results[0]:results[2], results[1]:results[3]]
                    img_test = (img_test - np.min(img_test)) / (np.max(img_test) - np.min(img_test)) # scale from 0 to 1
                    img_test = np.multiply(img_test, 255) # scale from 0 to 255
                    img_test = img_test.astype(np.float32)
                    # pixel no
                    len_img = img_test.size

                    '''
                    # save focus in focus folder
                    io.imsave(sep_file_[0] + '/' + sep_file_[1] + '/' + 'FocusClasses/' + str(row['status']) + '/' + dir_[0] + '_' + dir_[3] + '_' + dir_[
                        4] + '__' + str(sep_file_[4][:-8]) + '_roi' + str(roi_count + 1) +'_focusNo' + str(row['focus_label']) + '.tif', img_test)
                    '''

                    # A) BLUR DETECTION
                    # A.0) blur detection with variance of laplacian
                    # 0 mean proj single foci
                    var_laplacian = cv2.Laplacian(img_test, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_0vl_test.tif', img_test) # save img
                    laplacian = cv2.Laplacian(img_test, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_0l_test.tif', laplacian) # save img

                    # 1 gaussian blur 0 of 0
                    img_test_gb0 = cv2.GaussianBlur(img_test, (3, 3), 0)
                    var_laplacian_gb0 = cv2.Laplacian(img_test_gb0, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_1vlgb0_test.tif', img_test_gb0) # save img
                    laplacian_gb0 = cv2.Laplacian(img_test_gb0, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_1l_gb0_test.tif', laplacian_gb0) # save img

                    # 2 gaussian blur 1 of 0
                    img_test_gb1 = cv2.GaussianBlur(img_test, (5, 5), 0)
                    var_laplacian_gb1 = cv2.Laplacian(img_test_gb1, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_2vlgb1_test.tif', img_test_gb1) # save img
                    laplacian_gb1 = cv2.Laplacian(img_test_gb1, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_2l_gb1_test.tif', laplacian_gb1) # save img


                    # 3 gaussian blur 2 of 0
                    img_test_gb2 = cv2.GaussianBlur(img_test, (7, 7), 0)
                    var_laplacian_gb2 = cv2.Laplacian(img_test_gb2, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_3vlgb2_test.tif', img_test_gb2) # save img
                    laplacian_gb2 = cv2.Laplacian(img_test_gb2, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_3l_gb2_test.tif', laplacian_gb2) # save img

                    # 4 gaussian blur 3 of 0
                    img_test_gb3 = cv2.GaussianBlur(img_test, (9, 9), 0)
                    var_laplacian_gb3 = cv2.Laplacian(img_test_gb3, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_4vlgb3_test.tif', img_test_gb3) # save img
                    laplacian_gb3 = cv2.Laplacian(img_test_gb3, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_4l_gb3_test.tif', laplacian_gb3) # save img

                    # A.1) blur detection using measure function
                    blur_2 = measure.blur_effect(img_test)

                    # B) shannon entropy
                    shannon_entropy = measure.shannon_entropy(img_test)

                    # C) stats
                    focus_mean = np.mean(img_test)
                    focus_med = np.median(img_test)
                    focus_sum = np.sum(img_test)
                    focus_std = np.std(img_test)
                    focus_var = np.var(img_test)
                    focus_max = np.max(img_test)
                    focus_min = np.min(img_test)

                    # D) compactness
                    focus_area = row['area']
                    focus_perim = row['perimeter']

                    if (focus_perim < 1):
                        df_foci.drop(index, inplace=True)
                        print('Focus is too small. Focus dropped.')
                        continue

                    # D.0) Polsby-Popper
                    compactness_pp = (4 * pi * focus_area) / (focus_perim * focus_perim)

                    # D.1) Schwartzberg
                    compactness_s = 1 / (focus_perim / (2 * pi * sqrt(focus_area / pi)))

                    # E) other geometric properties
                    # 1.2 connected component labeling
                    # img_mask.reshape(n_rows, n_cols)
                    l_, n_ = mh.label(img_test, np.ones((3, 3), bool))  # binary_closed_hztl_k
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_CCL_mag.tif', l_)

                    # 1.3 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=img_test)
                    results = []

                    # only append focus properties if single object is detected in magnitude spectrum
                    if (len(im_props) == 1):
                        for focus_blob in im_props:
                            # if (focus_blob.area > 1000):
                            #    continue
                            # blob = im_props[30]
                            properties.append(focus_blob.euler_number)
                            properties.append(focus_blob.solidity)
                            stop = 1
                    else:
                        df_foci.drop(index, inplace=True)
                        print('Focus bbox has more or less than 1 object. Focus dropped.')
                        continue

                    # F) FFT Magnitude properties
                    # ft
                    f = np.fft.fft2(img_test)
                    # magnitude: ft
                    magnitude_spectrum_ft = 20 * np.log(np.abs(f))
                    # shift ft
                    fshift = np.fft.fftshift(f)
                    # magnitude: shifted ft
                    magnitude_spectrum_fshift = 20 * np.log(np.abs(fshift))
                    magnitude_spectrum_fshift_32F = magnitude_spectrum_fshift.astype(np.float32)  # convert to uint8

                    # F.A) Blur detection
                    # F.A.0) Blur detection, Variance of Laplacian
                    fft_var_laplacian = cv2.Laplacian(magnitude_spectrum_fshift_32F, cv2.CV_32F).var() # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_5mag.tif', magnitude_spectrum_fshift_32F) # save img
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_5vlm.tif', magnitude_spectrum_fshift_32F) # save img

                    # F.A.1) blur detection using measure function
                    fft_blur_2 = measure.blur_effect(magnitude_spectrum_fshift_32F)

                    # F.B) shannon entropy
                    fft_shannon_entropy = measure.shannon_entropy(magnitude_spectrum_fshift_32F)

                    # F.C) Stats
                    fft_mean = np.mean(magnitude_spectrum_fshift_32F)
                    fft_med = np.median(magnitude_spectrum_fshift_32F)
                    fft_sum = np.sum(magnitude_spectrum_fshift_32F)
                    fft_std = np.std(magnitude_spectrum_fshift_32F)
                    fft_var = np.var(magnitude_spectrum_fshift_32F)
                    fft_max = np.max(magnitude_spectrum_fshift_32F)
                    fft_min = np.min(magnitude_spectrum_fshift_32F)

                    # F.D) Other Geometric Properties
                    # 1.1 OTSU thresholding
                    if (np.isnan(magnitude_spectrum_fshift_32F).any() == True):
                            print("Drop focus, contains nan in magnitude spectrum.")
                            df_foci.drop(index, inplace=True)
                            continue
                    magnitude_spectrum_fshift_32F_gb = cv2.GaussianBlur(magnitude_spectrum_fshift_32F, (3, 3), 0)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_gb3_mag.tif', magnitude_spectrum_fshift_32F_gb)

                    th = filters.threshold_otsu(magnitude_spectrum_fshift_32F_gb)
                    img_mask = magnitude_spectrum_fshift_32F_gb > th
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_OTSUthresh_mag.tif', img_mask)

                    # 1.2 connected component labeling
                    # img_mask.reshape(n_rows, n_cols)
                    l_, n_ = mh.label(img_mask, np.ones((3, 3), bool))  # binary_closed_hztl_k
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_CCL_mag.tif', l_)

                    # 1.3 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=magnitude_spectrum_fshift_32F_gb)
                    results = []

                    # only append focus properties if single object is detected in magnitude spectrum
                    if (len(im_props) == 1):
                        for blob in im_props:
                            # if (blob.area > 1000):
                            #    continue
                            # blob = im_props[30]
                            properties.append(blob.orientation)
                            properties.append(blob.area)
                            properties.append(blob.perimeter)
                            properties.append(blob.major_axis_length)
                            properties.append(blob.minor_axis_length)
                            properties.append(blob.eccentricity)
                            properties.append(blob.euler_number)
                            properties.append(blob.solidity)

                            fft_area = blob.area
                            fft_perimeter = blob.perimeter
                            # F.E) compactness
                            # F.E.0) Polsby-Popper
                            fft_compactness_pp = (4 * pi * fft_area) / (focus_perim * fft_perimeter)
                            properties.append(fft_compactness_pp)
                            # F.E.1) Schwartzberg
                            fft_compactness_s = 1 / (fft_perimeter / (2 * pi * sqrt(fft_area / pi)))
                            properties.append(fft_compactness_s)
                    else:
                        df_foci.drop(index, inplace=True)
                        print('FFT magnitude has more or less than 1 object. Focus dropped.')
                        continue

                    area = properties[1]
                    if (area > 10000):
                        df_foci.drop(index, inplace=True)
                        print('FFT magnitude object area is too large. Focus dropped.')
                        continue

                    # Append rest of fft properties
                    properties.append(fft_var_laplacian)
                    properties.append(fft_blur_2)
                    properties.append(fft_shannon_entropy)
                    properties.append(fft_mean)
                    properties.append(fft_med)
                    properties.append(fft_sum)
                    properties.append(fft_std)
                    properties.append(fft_var)
                    properties.append(fft_max)
                    properties.append(fft_min)

                    # Append rest of focus properties
                    properties.append(var_laplacian)
                    properties.append(var_laplacian_gb0)
                    properties.append(var_laplacian_gb1)
                    properties.append(var_laplacian_gb2)
                    properties.append(var_laplacian_gb3)
                    properties.append(blur_2)
                    properties.append(shannon_entropy)
                    properties.append(focus_mean)
                    properties.append(focus_med)
                    properties.append(focus_sum)
                    properties.append(focus_std)
                    properties.append(focus_var)
                    properties.append(focus_max)
                    properties.append(focus_min)
                    properties.append(compactness_pp)
                    properties.append(compactness_s)
                    properties.append(len_img)

                    if (np.isnan(properties).any() == True):
                            print("Drop focus, contains nan in metrics.")
                            df_foci.drop(index, inplace=True)
                            continue
                    properties_full.append(properties)

                if (len(df_foci) < 1):
                    print('Focus dataframe empty. Skip this ROI.')
                    continue
                properties_full_ar = np.array(properties_full)


                df_foci['euler_number'] = properties_full_ar[:, 0]
                df_foci['solidity'] = properties_full_ar[:, 1]

                df_foci['fft_orientation'] = properties_full_ar[:, 2]
                df_foci['fft_area'] = properties_full_ar[:, 3]
                df_foci['fft_perimeter'] = properties_full_ar[:, 4]
                df_foci['fft_major_axis_length'] = properties_full_ar[:, 5]
                df_foci['fft_minor_axis_length'] = properties_full_ar[:, 6]
                df_foci['fft_eccentricity'] = properties_full_ar[:, 7]
                df_foci['fft_euler_number'] = properties_full_ar[:, 8]
                df_foci['fft_solidity'] = properties_full_ar[:, 9]
                df_foci['fft_compactness_pp'] = properties_full_ar[:, 10]
                df_foci['fft_compactness_s'] = properties_full_ar[:, 11]
                df_foci['fft_var_laplacian'] = properties_full_ar[:, 12]
                df_foci['fft_blur_2'] = properties_full_ar[:, 13]
                df_foci['fft_shannon_entropy'] = properties_full_ar[:, 14]
                df_foci['fft_mean'] = properties_full_ar[:, 15]
                df_foci['fft_med'] = properties_full_ar[:, 16]
                df_foci['fft_sum'] = properties_full_ar[:, 17]
                df_foci['fft_std'] = properties_full_ar[:, 18]
                df_foci['fft_var'] = properties_full_ar[:, 19]
                df_foci['fft_max'] = properties_full_ar[:, 20]
                df_foci['fft_min'] = properties_full_ar[:, 21]

                df_foci['var_lap'] = properties_full_ar[:, 22]
                df_foci['var_lap_gb3'] = properties_full_ar[:, 23]
                df_foci['var_lap_gb5'] = properties_full_ar[:, 24]
                df_foci['var_lap_gb7'] = properties_full_ar[:, 25]
                df_foci['var_lap_gb9'] = properties_full_ar[:, 26]
                df_foci['blur_2'] = properties_full_ar[:, 27]
                df_foci['shannon_entropy'] = properties_full_ar[:, 28]
                df_foci['focus_mean'] = properties_full_ar[:, 29]
                df_foci['focus_med'] = properties_full_ar[:, 30]
                df_foci['focus_sum'] = properties_full_ar[:, 31]
                df_foci['focus_std'] = properties_full_ar[:, 32]
                df_foci['focus_var'] = properties_full_ar[:, 33]
                df_foci['focus_max'] = properties_full_ar[:, 34]
                df_foci['focus_min'] = properties_full_ar[:, 35]
                df_foci['compactness_pp'] = properties_full_ar[:, 36]
                df_foci['compactness_s'] = properties_full_ar[:, 37]
                df_foci['tot_pix_no'] = properties_full_ar[:, 38]

                df_foci.to_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + 'focus_info_full_props.csv')

                stop = 1
    print('Next experiment...')

def measure_props_df(summary_dir, dir_list):
    all_files = []
    #dir_list= dir_list[12:14] # for debugging
    for dir_ in dir_list:
        #dir_ = dir_list[7] # for debugging
        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.roi')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.roi')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        # skip if there are no rois
        if (len(all_files) < 1):
            continue
        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/*.roi')
        if (len(roi_list) == 1):
            roi_is_zip = False
        elif (len(roi_list) < 1):
            # more than one ROI
            roi_list = glob(dir_[5] + '/*.zip')
            roi_is_zip = True
        else:
            print("Missing ROI file for movie file")
            continue

        for roi_file in roi_list:
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            #roi_points_test = roi_points[1] # for debugging
            for roi_count, bbox_points in enumerate(roi_points):#enumerate(roi_points_test):
                # roi_count = 1 # for debugging
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = sep_file_[1][:-7]
                test = dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status_G.csv'
                # check for csv file with manually classified foci
                if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status_G.csv') == True):
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status_G.csv')
                else:
                    continue

                #df_foci = pd.read_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[4] + 'focus_info_full.csv')
                properties_full = []
                #df_foci_test = df_foci.iloc[26]
                for index, row in df_foci.iterrows():
                    focus_label = row['focus_label']
                    #print(index), 28
                    #index = 70
                    #row = df_foci.iloc[26]
                    print('Focus label: ' + str(row['focus_label']))
                    print('Index: ' + str(index))
                    properties = []

                    img = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[4] + '_preproc_02_clahe_roi'+ str(roi_count + 1) + '_G.tif') # import max projection image

                    # extract focus bounding box
                    bbox = row['bbox'].split('(', 2)
                    test1 = bbox[1].split(')', 2)
                    test2 = test1[0].split(',', 4)
                    results = list(map(int, test2))
                    img_test = img[results[0]:results[2], results[1]:results[3]]
                    img_test = (img_test - np.min(img_test)) / (np.max(img_test) - np.min(img_test)) # scale from 0 to 1
                    img_test = np.multiply(img_test, 255) # scale from 0 to 255
                    img_test = img_test.astype(np.float32)
                    # pixel no
                    len_img = img_test.size

                    # save focus in focus folder
                    # save images of individual foci, with and without outlines

                    # A) BLUR DETECTION
                    # A.0) blur detection with variance of laplacian
                    # 0 mean proj single foci
                    var_laplacian = cv2.Laplacian(img_test, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_0vl_test.tif', img_test) # save img
                    laplacian = cv2.Laplacian(img_test, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_0l_test.tif', laplacian) # save img

                    # 1 gaussian blur 0 of 0
                    img_test_gb0 = cv2.GaussianBlur(img_test, (3, 3), 0)
                    var_laplacian_gb0 = cv2.Laplacian(img_test_gb0, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_1vlgb0_test.tif', img_test_gb0) # save img
                    laplacian_gb0 = cv2.Laplacian(img_test_gb0, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_1l_gb0_test.tif', laplacian_gb0) # save img

                    # 2 gaussian blur 1 of 0
                    img_test_gb1 = cv2.GaussianBlur(img_test, (5, 5), 0)
                    var_laplacian_gb1 = cv2.Laplacian(img_test_gb1, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_2vlgb1_test.tif', img_test_gb1) # save img
                    laplacian_gb1 = cv2.Laplacian(img_test_gb1, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_2l_gb1_test.tif', laplacian_gb1) # save img


                    # 3 gaussian blur 2 of 0
                    img_test_gb2 = cv2.GaussianBlur(img_test, (7, 7), 0)
                    var_laplacian_gb2 = cv2.Laplacian(img_test_gb2, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_3vlgb2_test.tif', img_test_gb2) # save img
                    laplacian_gb2 = cv2.Laplacian(img_test_gb2, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_3l_gb2_test.tif', laplacian_gb2) # save img

                    # 4 gaussian blur 3 of 0
                    img_test_gb3 = cv2.GaussianBlur(img_test, (9, 9), 0)
                    var_laplacian_gb3 = cv2.Laplacian(img_test_gb3, cv2.CV_32F).var()  # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_4vlgb3_test.tif', img_test_gb3) # save img
                    laplacian_gb3 = cv2.Laplacian(img_test_gb3, cv2.CV_32F)  # laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_4l_gb3_test.tif', laplacian_gb3) # save img

                    # A.1) blur detection using measure function
                    blur_2 = measure.blur_effect(img_test)

                    # B) shannon entropy
                    shannon_entropy = measure.shannon_entropy(img_test)

                    # C) stats
                    focus_mean = np.mean(img_test)
                    focus_med = np.median(img_test)
                    focus_sum = np.sum(img_test)
                    focus_std = np.std(img_test)
                    focus_var = np.var(img_test)
                    focus_max = np.max(img_test)
                    focus_min = np.min(img_test)

                    # D) compactness
                    focus_area = row['area']
                    focus_perim = row['perimeter']

                    if (focus_perim < 1):
                        df_foci.drop(index, inplace=True)
                        print('Focus is too small. Focus dropped.')
                        continue

                    # D.0) Polsby-Popper
                    compactness_pp = (4 * pi * focus_area) / (focus_perim * focus_perim)

                    # D.1) Schwartzberg
                    compactness_s = 1 / (focus_perim / (2 * pi * sqrt(focus_area / pi)))

                    # E) other geometric properties
                    # 1.2 connected component labeling
                    # img_mask.reshape(n_rows, n_cols)
                    l_, n_ = mh.label(img_test, np.ones((3, 3), bool))  # binary_closed_hztl_k
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_CCL_mag.tif', l_)

                    # 1.3 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=img_test)
                    results = []

                    # only append focus properties if single object is detected in magnitude spectrum
                    if (len(im_props) == 1):
                        for focus_blob in im_props:
                            # if (focus_blob.area > 1000):
                            #    continue
                            # blob = im_props[30]
                            properties.append(focus_blob.euler_number)
                            properties.append(focus_blob.solidity)
                            stop = 1
                    else:
                        df_foci.drop(index, inplace=True)
                        print('Focus bbox has more or less than 1 object. Focus dropped.')
                        continue

                    # F) FFT Magnitude properties
                    # ft
                    f = np.fft.fft2(img_test)
                    # magnitude: ft
                    magnitude_spectrum_ft = 20 * np.log(np.abs(f))
                    # shift ft
                    fshift = np.fft.fftshift(f)
                    # magnitude: shifted ft
                    magnitude_spectrum_fshift = 20 * np.log(np.abs(fshift))
                    magnitude_spectrum_fshift_32F = magnitude_spectrum_fshift.astype(np.float32)  # convert to uint8

                    # F.A) Blur detection
                    # F.A.0) Blur detection, Variance of Laplacian
                    fft_var_laplacian = cv2.Laplacian(magnitude_spectrum_fshift_32F, cv2.CV_32F).var() # variance of laplacian
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_5mag.tif', magnitude_spectrum_fshift_32F) # save img
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_5vlm.tif', magnitude_spectrum_fshift_32F) # save img

                    # F.A.1) blur detection using measure function
                    fft_blur_2 = measure.blur_effect(magnitude_spectrum_fshift_32F)

                    # F.B) shannon entropy
                    fft_shannon_entropy = measure.shannon_entropy(magnitude_spectrum_fshift_32F)

                    # F.C) Stats
                    fft_mean = np.mean(magnitude_spectrum_fshift_32F)
                    fft_med = np.median(magnitude_spectrum_fshift_32F)
                    fft_sum = np.sum(magnitude_spectrum_fshift_32F)
                    fft_std = np.std(magnitude_spectrum_fshift_32F)
                    fft_var = np.var(magnitude_spectrum_fshift_32F)
                    fft_max = np.max(magnitude_spectrum_fshift_32F)
                    fft_min = np.min(magnitude_spectrum_fshift_32F)

                    # F.D) Other Geometric Properties
                    # 1.1 OTSU thresholding
                    if (np.isnan(magnitude_spectrum_fshift_32F).any() == True):
                            print("Drop focus, contains nan in magnitude spectrum.")
                            df_foci.drop(index, inplace=True)
                            continue
                    magnitude_spectrum_fshift_32F_gb = cv2.GaussianBlur(magnitude_spectrum_fshift_32F, (3, 3), 0)
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_gb3_mag.tif', magnitude_spectrum_fshift_32F_gb)

                    th = filters.threshold_otsu(magnitude_spectrum_fshift_32F_gb)
                    img_mask = magnitude_spectrum_fshift_32F_gb > th
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_OTSUthresh_mag.tif', img_mask)

                    # 1.2 connected component labeling
                    # img_mask.reshape(n_rows, n_cols)
                    l_, n_ = mh.label(img_mask, np.ones((3, 3), bool))  # binary_closed_hztl_k
                    io.imsave(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_focusNo' + str(row['focus_label']) + '_roi' + str(roi_count + 1) + '_CCL_mag.tif', l_)

                    # 1.3 measure region properties
                    rs_k = regionprops(l_)
                    im_props = regionprops(l_, intensity_image=magnitude_spectrum_fshift_32F_gb)
                    results = []

                    # only append focus properties if single object is detected in magnitude spectrum
                    if (len(im_props) == 1):
                        for blob in im_props:
                            # if (blob.area > 1000):
                            #    continue
                            # blob = im_props[30]
                            properties.append(blob.orientation)
                            properties.append(blob.area)
                            properties.append(blob.perimeter)
                            properties.append(blob.major_axis_length)
                            properties.append(blob.minor_axis_length)
                            properties.append(blob.eccentricity)
                            properties.append(blob.euler_number)
                            properties.append(blob.solidity)

                            fft_area = blob.area
                            fft_perimeter = blob.perimeter
                            # F.E) compactness
                            # F.E.0) Polsby-Popper
                            fft_compactness_pp = (4 * pi * fft_area) / (focus_perim * fft_perimeter)
                            properties.append(fft_compactness_pp)
                            # F.E.1) Schwartzberg
                            fft_compactness_s = 1 / (fft_perimeter / (2 * pi * sqrt(fft_area / pi)))
                            properties.append(fft_compactness_s)
                    else:
                        df_foci.drop(index, inplace=True)
                        print('FFT magnitude has more or less than 1 object. Focus dropped.')
                        continue

                    area = properties[1]
                    if (area > 1000):
                        df_foci.drop(index, inplace=True)
                        print('FFT magnitude object area is too large. Focus dropped.')
                        continue

                    # Append rest of fft properties
                    properties.append(fft_var_laplacian)
                    properties.append(fft_blur_2)
                    properties.append(fft_shannon_entropy)
                    properties.append(fft_mean)
                    properties.append(fft_med)
                    properties.append(fft_sum)
                    properties.append(fft_std)
                    properties.append(fft_var)
                    properties.append(fft_max)
                    properties.append(fft_min)

                    # Append rest of focus properties
                    properties.append(var_laplacian)
                    properties.append(var_laplacian_gb0)
                    properties.append(var_laplacian_gb1)
                    properties.append(var_laplacian_gb2)
                    properties.append(var_laplacian_gb3)
                    properties.append(blur_2)
                    properties.append(shannon_entropy)
                    properties.append(focus_mean)
                    properties.append(focus_med)
                    properties.append(focus_sum)
                    properties.append(focus_std)
                    properties.append(focus_var)
                    properties.append(focus_max)
                    properties.append(focus_min)
                    properties.append(compactness_pp)
                    properties.append(compactness_s)
                    properties.append(len_img)

                    if (np.isnan(properties).any() == True):
                            print("Drop focus, contains nan in metrics.")
                            df_foci.drop(index, inplace=True)
                            continue
                    properties_full.append(properties)

                if (len(df_foci) < 1):
                    print('Focus dataframe empty. Skip this ROI.')
                    continue
                properties_full_ar = np.array(properties_full)


                df_foci['euler_number'] = properties_full_ar[:, 0]
                df_foci['solidity'] = properties_full_ar[:, 1]

                df_foci['fft_orientation'] = properties_full_ar[:, 2]
                df_foci['fft_area'] = properties_full_ar[:, 3]
                df_foci['fft_perimeter'] = properties_full_ar[:, 4]
                df_foci['fft_major_axis_length'] = properties_full_ar[:, 5]
                df_foci['fft_minor_axis_length'] = properties_full_ar[:, 6]
                df_foci['fft_eccentricity'] = properties_full_ar[:, 7]
                df_foci['fft_euler_number'] = properties_full_ar[:, 8]
                df_foci['fft_solidity'] = properties_full_ar[:, 9]
                df_foci['fft_compactness_pp'] = properties_full_ar[:, 10]
                df_foci['fft_compactness_s'] = properties_full_ar[:, 11]
                df_foci['fft_var_laplacian'] = properties_full_ar[:, 12]
                df_foci['fft_blur_2'] = properties_full_ar[:, 13]
                df_foci['fft_shannon_entropy'] = properties_full_ar[:, 14]
                df_foci['fft_mean'] = properties_full_ar[:, 15]
                df_foci['fft_med'] = properties_full_ar[:, 16]
                df_foci['fft_sum'] = properties_full_ar[:, 17]
                df_foci['fft_std'] = properties_full_ar[:, 18]
                df_foci['fft_var'] = properties_full_ar[:, 19]
                df_foci['fft_max'] = properties_full_ar[:, 20]
                df_foci['fft_min'] = properties_full_ar[:, 21]

                df_foci['var_lap'] = properties_full_ar[:, 22]
                df_foci['var_lap_gb3'] = properties_full_ar[:, 23]
                df_foci['var_lap_gb5'] = properties_full_ar[:, 24]
                df_foci['var_lap_gb7'] = properties_full_ar[:, 25]
                df_foci['var_lap_gb9'] = properties_full_ar[:, 26]
                df_foci['blur_2'] = properties_full_ar[:, 27]
                df_foci['shannon_entropy'] = properties_full_ar[:, 28]
                df_foci['focus_mean'] = properties_full_ar[:, 29]
                df_foci['focus_med'] = properties_full_ar[:, 30]
                df_foci['focus_sum'] = properties_full_ar[:, 31]
                df_foci['focus_std'] = properties_full_ar[:, 32]
                df_foci['focus_var'] = properties_full_ar[:, 33]
                df_foci['focus_max'] = properties_full_ar[:, 34]
                df_foci['focus_min'] = properties_full_ar[:, 35]
                df_foci['compactness_pp'] = properties_full_ar[:, 36]
                df_foci['compactness_s'] = properties_full_ar[:, 37]
                df_foci['tot_pix_no'] = properties_full_ar[:, 38]

                df_foci.to_csv(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + 'focus_info_full_props.csv')

                stop = 1
    print('Next experiment...')

def focus_libraries(summary_dir, dir_list):
    all_files = []
    #dir_list= dir_list[21:22] # for debugging
    for dir_ in dir_list:
        # --------------
        # for debugging:
        #dir_ = dir_list[7]
        sample_no = dir_[3]
        spool_no = dir_[4]
        if ((sample_no == '3') and (spool_no == 'spool_6')):
            stop =1
        # --------------

        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.roi')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.roi')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        # skip if there are no rois
        if (len(all_files) < 1):
            continue
        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/*.roi')
        if (len(roi_list) == 1):
            roi_is_zip = False
        elif (len(roi_list) < 1):
            # more than one ROI
            roi_list = glob(dir_[5] + '/*.zip')
            roi_is_zip = True
        else:
            print("Missing ROI file for movie file")
            continue

        for roi_file in roi_list:
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            #roi_points_test = roi_points[1] # for debugging
            for roi_count, bbox_points in enumerate(roi_points):#enumerate(roi_points_test):
                # roi_count = 1 # for debugging
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)

                if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == True):
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv')
                else:
                    continue

                #df_foci_test = df_foci.iloc[8:10]
                for index, row in df_foci.iterrows():
                    #print(index), 28
                    #index = 70
                    #row = df_foci.iloc[26]
                    print('Focus label: ' + str(row['focus_label']))
                    print('Index: ' + str(index))

                    # import images
                    # max proj
                    # no outlines
                    img = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_proc_10_maxproj_roi' + str(roi_count + 1) + '.tif') # import max projection image
                    # outlines
                    img_outlines = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_procfocusDetectionCheck_Full.tif')
                    # max proj images of raw data
                    # no outlines
                    img_raw = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_proc_10_rawmaxproj_roi' + str(roi_count + 1) + '.tif') # import max projection image
                    # outlines
                    img_raw_outlines = io.imread(roi_file[:-4] + '_' + str(roi_count + 1) + '/' + dir_[0] + '_' + dir_[3] + dir_[
                        4] + '_rawfocusDetectionCheck_Full.tif')

                    # extract focus bounding box
                    bbox = row['bbox'].split('(', 2)
                    test1 = bbox[1].split(')', 2)
                    test2 = test1[0].split(',', 4)
                    results = list(map(int, test2))
                    img_r1, img_c1 = img.shape

                    # define bounding box
                    pixel_border = 3
                    r0 = results[0] - pixel_border
                    r1 = results[2] + pixel_border
                    c0 = results[1] - pixel_border
                    c1 = results[3] + pixel_border
                    # check if bounding box goes beyond image limits
                    if (r0 < 0):
                        r0 = 0
                    if (r1 > img_r1):
                        r1 = img_r1
                    if (c0 < 0):
                        c0 = 0
                    if (c1 > img_c1):
                        c1 = img_c1

                    # max projection
                    # no outlines
                    img_maxproj = img[r0:r1, c0:c1]
                    # outlines
                    img_maxproj_outlines = img_outlines[r0:r1, c0:c1]
                    # max proj images of raw data
                    # no outlines
                    img_rawmaxproj = img_raw[r0:r1, c0:c1]
                    # outlines
                    img_rawmaxproj_outlines = img_raw_outlines[r0:r1, c0:c1]

                    # save focus in focus folder
                    if (math.isnan(row['status']) == False):
                        # max proj
                        # no outlines
                        io.imsave(sep_file_[0] + '/FocusLibrary_0/FocusLibrary_0_maxproj/' + 'FocusClasses/' + str(int(row['status'])) + '/' + dir_[0] + '_' + dir_[3] + '_' + dir_[
                            4] + '__' + str(sep_file_[4][:-8]) + '_roi' + str(roi_count + 1) +'_focusNo' + str(int(row['focus_label'])) + '.tif', img_maxproj)
                        # outlines
                        io.imsave(sep_file_[0] + '/FocusLibrary_0/FocusLibrary_0_maxproj_outlines/' + 'FocusClasses/' + str(int(row['status'])) + '/' + dir_[0] + '_' + dir_[3] + '_' + dir_[
                            4] + '__' + str(sep_file_[4][:-8]) + '_roi' + str(roi_count + 1) +'_focusNo' + str(int(row['focus_label'])) + '_outline.tif', img_maxproj_outlines)

                        # raw max proj
                        # no outlines
                        io.imsave(sep_file_[0] + '/FocusLibrary_0/FocusLibrary_0_rawmaxproj/' + 'FocusClasses/' + str(int(row['status'])) + '/' + dir_[0] + '_' + dir_[3] + '_' + dir_[
                            4] + '__' + str(sep_file_[4][:-8]) + '_roi' + str(roi_count + 1) +'_focusNo' + str(int(row['focus_label'])) + '_raw.tif', img_rawmaxproj)
                        # outlines
                        io.imsave(sep_file_[0] + '/FocusLibrary_0/FocusLibrary_0_rawmaxproj_outlines/' + 'FocusClasses/' + str(int(row['status'])) + '/' + dir_[0] + '_' + dir_[3] + '_' + dir_[
                            4] + '__' + str(sep_file_[4][:-8]) + '_roi' + str(roi_count + 1) +'_focusNo' + str(int(row['focus_label'])) + '_raw_outline.tif', img_rawmaxproj_outlines)

                        text = dir_[0] + '_' + dir_[3] + '_' + dir_[
                            4] + '__' + str(sep_file_[4][:-8]) + '_roi' + str(roi_count + 1) + '_focusNo' + str(int(row['focus_label']))

                        # set focus number for images
                        if (int(row['status']) == 0):
                            focus_no = len(foci_0)
                        if (int(row['status']) == 1):
                            focus_no = len(foci_1)
                        if (int(row['status']) == 2):
                            focus_no = len(foci_2)
                        if (int(row['status']) == 3):
                            focus_no = len(foci_3)
                        if (int(row['status']) == 4):
                            focus_no = len(foci_4)
                        if (int(row['status']) == 5):
                            focus_no = len(foci_5)

                        # outlines and no outlines: embed focus image into larger foci wall
                        cols = 90
                        foci_wall = np.zeros((img_rawmaxproj.shape[0] + 5, cols), dtype=np.int)
                        foci_wall_outlines = np.zeros((img_rawmaxproj_outlines.shape[0] + 5, cols), dtype=np.int)
                        foci_wall[0: img_rawmaxproj.shape[0], 0: img_rawmaxproj.shape[1]] = img_maxproj
                        foci_wall_outlines[0: img_rawmaxproj_outlines.shape[0], 0: img_rawmaxproj_outlines.shape[1]] = img_maxproj_outlines

                        # check text doesn't go out of img bounds
                        if (img_rawmaxproj.shape[1] + 5 > foci_wall.shape[1]):
                            y = 1
                        else:
                            y = 5
                        org = (40, y)
                        text_color = 4000# raw: 110
                        foci_wall = cv2.putText(foci_wall, str(focus_no), org, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.3,
                                                color= text_color, thickness=1) # text,
                        foci_wall_outlines = cv2.putText(foci_wall_outlines, str(focus_no), org, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.3,
                                                color= text_color, thickness=1) # text
                        # file focus according to class
                        if (int(row['status']) == 0):
                            foci_info_0_pre = []
                            foci_0.append(foci_wall)
                            foci_0_outlines.append(foci_wall_outlines)
                            foci_info_0_pre.append(focus_no)
                            foci_info_0_pre.append(text)
                            foci_info_0.append(foci_info_0_pre)
                        if (int(row['status']) == 1):
                            foci_info_1_pre = []
                            foci_1.append(foci_wall)
                            foci_1_outlines.append(foci_wall_outlines)
                            foci_info_1_pre.append(focus_no)
                            foci_info_1_pre.append(text)
                            foci_info_1.append(foci_info_1_pre)
                        if (int(row['status']) == 2):
                            foci_info_2_pre = []
                            foci_2.append(foci_wall)
                            foci_2_outlines.append(foci_wall_outlines)
                            foci_info_2_pre.append(focus_no)
                            foci_info_2_pre.append(text)
                            foci_info_2.append(foci_info_2_pre)
                        if (int(row['status']) == 3):
                            foci_info_3_pre = []
                            foci_3.append(foci_wall)
                            foci_3_outlines.append(foci_wall_outlines)
                            foci_info_3_pre.append(focus_no)
                            foci_info_3_pre.append(text)
                            foci_info_3.append(foci_info_3_pre)
                        if (int(row['status']) == 4):
                            foci_info_4_pre = []
                            foci_4.append(foci_wall)
                            foci_4_outlines.append(foci_wall_outlines)
                            foci_info_4_pre.append(focus_no)
                            foci_info_4_pre.append(text)
                            foci_info_4.append(foci_info_4_pre)
                        if (int(row['status']) == 5):
                            foci_info_5_pre = []
                            foci_5.append(foci_wall)
                            foci_5_outlines.append(foci_wall_outlines)
                            foci_info_5_pre.append(focus_no)
                            foci_info_5_pre.append(text)
                            foci_info_5.append(foci_info_5_pre)


def focus_libraries_train(base_dir):
    all_files = []
    datasets = load_properties()
    df_foci_idr = datasets[0] #pd.read_csv(base_dir + '/CSV_files/foci_info_idr_full_v1.csv') #(base_dir + '/CSV_files/Normalization/average/2022_7_7_19/foci_info_idr_pixnorm_avgnorm_standscaled_2022_7_7_19.csv')
    # save csv used for record keeping
    df_foci_idr.to_csv(saving_dir + '/' + datasets[2])
    # cleanup df
    df_foci_idr = df_foci_idr.drop('focus_min', axis=1).copy()
    # select focus classes of interest
    df_foci_idr = df_foci_idr.loc[(df_foci_idr['status'] == 0) | (df_foci_idr['status'] == 1) | (df_foci_idr['status'] == 2) | (df_foci_idr['status'] == 3) | (df_foci_idr['status'] == 4) | (df_foci_idr['status'] == 5)]
    df_foci_idr = df_foci_idr.reset_index(drop = True)

    # loop through all rows in df
    for index, row in df_foci_idr.iterrows():
        print('df index: ' + str(index))
        exp_folder = row['exp_folder']
        exp_folder_split = exp_folder.split('_')
        i_roi_id = row['roi_id']
        i_roi_id_split = i_roi_id.split('_')

        # maxproj
        print('Read max proj image.')
        sample = i_roi_id_split[0] # 1
        spool = i_roi_id_split[1] + '_' + i_roi_id_split[2] # spool_10
        roi = i_roi_id  # 1_spool_10_1.tif_1GaussSmooth_2BackSub_RGB_roi_1
        roi_no = i_roi_id_split[7] + i_roi_id_split[8]
        img_name = exp_folder_split[0] + '_' + sample + spool + '_proc_10_maxproj_' + roi_no + '.tif' # 20200220_1spool_10_proc_10_maxproj_roi1.tif
        img_name_outlines = exp_folder_split[0] + '_' + sample + spool + '_procfocusDetectionCheck_Full.tif'
        img_dir = base_dir + '/' + exp_folder + '/Output/'+ sample + '/' + spool +'/' + roi
        img = io.imread(img_dir + '/' + img_name)
        img_outlines = io.imread(img_dir + '/' + img_name_outlines)

        # raw
        print('Read raw images.')
        img_name_raw = exp_folder_split[0] + '_' + sample + spool + '_proc_10_rawmaxproj_' + roi_no + '.tif' # 20200220_1spool_10_proc_10_rawmaxproj_roi1.tif
        img_raw = io.imread(img_dir + '/' + img_name_raw)
        img_raw_name_outlines = exp_folder_split[0] + '_' + sample + spool + '_rawfocusDetectionCheck_Full.tif' # 20200220_1spool_10_rawfocusDetectionCheck_Full.tif
        img_raw_outlines = io.imread(img_dir + '/' + img_raw_name_outlines)

        print(index)
        # extract focus bounding box
        bbox = row['bbox'].split('(', 2)
        test1 = bbox[1].split(')', 2)
        test2 = test1[0].split(',', 4)
        results = list(map(int, test2))
        img_r1, img_c1 = img.shape

        # define bounding box
        pixel_border = 3
        r0 = results[0] - pixel_border
        r1 = results[2] + pixel_border
        c0 = results[1] - pixel_border
        c1 = results[3] + pixel_border
        # check if bounding box goes beyond image limits
        if (r0 < 0):
            r0 = 0
        if (r1 > img_r1):
            r1 = img_r1
        if (c0 < 0):
            c0 = 0
        if (c1 > img_c1):
            c1 = img_c1

        # max projection
        print('Crop focus ROI.')
        # no outlines
        img_maxproj = img[r0:r1, c0:c1]
        # outlines
        img_maxproj_outlines = img_outlines[r0:r1, c0:c1]

        # max proj images of raw data
        # no outlines
        img_rawmaxproj = img_raw[r0:r1, c0:c1]
        # outlines
        img_rawmaxproj_outlines = img_raw_outlines[r0:r1, c0:c1]

        # set focus number for images
        print(row['status'])
        if (int(row['status']) == 0):
            focus_no = len(foci_0)
        if (int(row['status']) == 1):
            focus_no = len(foci_1)
        if (int(row['status']) == 2):
            focus_no = len(foci_2)
        if (int(row['status']) == 3):
            focus_no = len(foci_3)
        if (int(row['status']) == 4):
            focus_no = len(foci_4)
        if (int(row['status']) == 5):
            focus_no = len(foci_5)

        # outlines and no outlines: embed focus image into larger foci wall
        print('Embed focus image into larger focus wall.')
        cols = 90
        foci_wall = np.zeros((img_maxproj.shape[0] + 5, cols), dtype=np.int)
        foci_wall_outlines = np.zeros((img_maxproj_outlines.shape[0] + 5, cols), dtype=np.int)
        foci_wall[0: img_maxproj.shape[0], 0: img_maxproj.shape[1]] = img_maxproj
        foci_wall_outlines[0: img_maxproj_outlines.shape[0], 0: img_maxproj_outlines.shape[1]] = img_maxproj_outlines

        text = img_dir + '/' + img_name + '_focusNo' + str(int(row['focus_label']))

        # check text doesn't go out of img bounds
        print('Check text is not out of image bounds.')
        if (img_maxproj.shape[1] + 5 > foci_wall.shape[1]):
            y = 1
        else:
            y = 5
        org = (40, y)
        text_color = 4000  # raw: 110
        print('Build focus wall.')
        print('no outlines')
        foci_wall = cv2.putText(np.float32(foci_wall), str(focus_no), org, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.3,
                                color=text_color, thickness=1)  # text,
        print('outlines')
        foci_wall_outlines = cv2.putText(np.float32(foci_wall_outlines), str(focus_no), org, cv2.FONT_HERSHEY_SIMPLEX,
                                         fontScale=0.3,
                                         color=text_color, thickness=1)  # text
        print('Store focus according to class.')
        # file focus according to class
        if (int(row['status']) == 0):
            foci_info_0_pre = []
            foci_0.append(foci_wall)
            foci_0_outlines.append(foci_wall_outlines)
            foci_info_0_pre.append(focus_no)
            foci_info_0_pre.append(text)
            foci_info_0.append(foci_info_0_pre)
        if (int(row['status']) == 1):
            foci_info_1_pre = []
            foci_1.append(foci_wall)
            foci_1_outlines.append(foci_wall_outlines)
            foci_info_1_pre.append(focus_no)
            foci_info_1_pre.append(text)
            foci_info_1.append(foci_info_1_pre)
        if (int(row['status']) == 2):
            foci_info_2_pre = []
            foci_2.append(foci_wall)
            foci_2_outlines.append(foci_wall_outlines)
            foci_info_2_pre.append(focus_no)
            foci_info_2_pre.append(text)
            foci_info_2.append(foci_info_2_pre)
        if (int(row['status']) == 3):
            foci_info_3_pre = []
            foci_3.append(foci_wall)
            foci_3_outlines.append(foci_wall_outlines)
            foci_info_3_pre.append(focus_no)
            foci_info_3_pre.append(text)
            foci_info_3.append(foci_info_3_pre)
        if (int(row['status']) == 4):
            foci_info_4_pre = []
            foci_4.append(foci_wall)
            foci_4_outlines.append(foci_wall_outlines)
            foci_info_4_pre.append(focus_no)
            foci_info_4_pre.append(text)
            foci_info_4.append(foci_info_4_pre)
        if (int(row['status']) == 5):
            foci_info_5_pre = []
            foci_5.append(foci_wall)
            foci_5_outlines.append(foci_wall_outlines)
            foci_info_5_pre.append(focus_no)
            foci_info_5_pre.append(text)
            foci_info_5.append(foci_info_5_pre)
        stop = 1

def focus_libraries_idr(base_dir,summary_dir):

    all_files = []
    datasets = load_properties()
    df_foci_idr = datasets[1] #pd.read_csv(base_dir + '/CSV_files/foci_info_idr_full_v1.csv') #(base_dir + '/CSV_files/Normalization/average/2022_7_7_19/foci_info_idr_pixnorm_avgnorm_standscaled_2022_7_7_19.csv')
    # save csv used for record keeping
    df_foci_idr.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir + '/' + datasets[3])

    df_foci_idr_ = pd.read_csv(base_dir + '/2022_idr_data/Output/FocusLibrary_idr_v1/2022_7_8_11/foci_info_idr_full_norm_knn.csv')#base_dir + '/foci_info_idr_full_norm_knn.csv')
    #test_preds = pd.read_csv(base_dir + '/Results/Classification/2022_7_8_18/class_preds_test_2022_7_8_18.csv')
    df_foci_idr = df_foci_idr.drop('focus_min', axis=1).copy()
    #df_foci_idr = df_foci_idr.dropna()
    df_foci_idr = df_foci_idr.loc[ (df_foci_idr['status'] == 0) | (df_foci_idr['status'] == 1) |  (df_foci_idr['status'] == 2) | (df_foci_idr['status'] == 3) | (df_foci_idr['status'] == 4) | (df_foci_idr['status'] == 5)]
    df_foci_idr = df_foci_idr.reset_index(drop = True)
    exp_folder = df_foci_idr['exp_folder'][0]
    exp_folder_split = exp_folder.split('_')
    roi_id_list = df_foci_idr['roi_id'].unique()
    for i_roi_id in roi_id_list:
        print(i_roi_id)
        i_roi_id_split = i_roi_id.split('_')
        # import images
        # H:\2021_FociData\2022_idr_data\Output\1\spool_1\001-23 53bp10011_roi_1
        # 2022_1spool_1_preproc_01_raw_roi1_afterResize.tif
        # 2022_1spool_1_rawfocusDetectionCheck_Full
        # 2022_1spool_1_preproc_05_bskmeans_roi1.tif
        # 2022_1spool_1_procfocusDetectionCheck_Full

        # max proj
        img_dir = base_dir + '/' + exp_folder + '/Output/1/spool_' +  i_roi_id_split[1] + '/' + i_roi_id_split[2] + '_' \
               + i_roi_id_split[3] + '_' + i_roi_id_split[4]
        img_name = exp_folder_split[0] + '_1spool_' +  i_roi_id_split[1] + '_preproc_05_bskmeans_roi' + i_roi_id_split[4]+ '.tif'
        img = io.imread(img_dir + '/' + img_name)
        img_name_outlines = exp_folder_split[0] + '_1spool_' +  i_roi_id_split[1] + '_procfocusDetectionCheck_Full.tif'
        img_outlines = io.imread(img_dir + '/' + img_name_outlines)
        print('Read max proj image.')
        # raw
        img_name_raw = exp_folder_split[0] + '_1spool_' +  i_roi_id_split[1] + '_preproc_01_raw_roi' + i_roi_id_split[4] + '_afterResize.tif'
        img_raw = io.imread(img_dir + '/' + img_name_raw)
        img_raw_name_outlines = exp_folder_split[0] + '_1spool_' +  i_roi_id_split[1] + '_rawfocusDetectionCheck_Full.tif'
        img_raw_outlines = io.imread(img_dir + '/' + img_raw_name_outlines)
        print('Read raw images.')

        # raw
        # outlines
        df_i_roi_id = df_foci_idr.loc[df_foci_idr['roi_id'] == i_roi_id]
        for index, row in df_i_roi_id.iterrows():
            print(index)
            # extract focus bounding box
            bbox = row['bbox'].split('(', 2)
            test1 = bbox[1].split(')', 2)
            test2 = test1[0].split(',', 4)
            results = list(map(int, test2))
            img_r1, img_c1 = img.shape

            # define bounding box
            pixel_border = 3
            r0 = results[0] - pixel_border
            r1 = results[2] + pixel_border
            c0 = results[1] - pixel_border
            c1 = results[3] + pixel_border
            # check if bounding box goes beyond image limits
            if (r0 < 0):
                r0 = 0
            if (r1 > img_r1):
                r1 = img_r1
            if (c0 < 0):
                c0 = 0
            if (c1 > img_c1):
                c1 = img_c1

            # max projection
            print('Crop focus ROI.')
            # no outlines
            img_maxproj = img[r0:r1, c0:c1]
            # outlines
            img_maxproj_outlines = img_outlines[r0:r1, c0:c1]

            # max proj images of raw data
            # no outlines
            img_rawmaxproj = img_raw[r0:r1, c0:c1]
            # outlines
            img_rawmaxproj_outlines = img_raw_outlines[r0:r1, c0:c1]

            # set focus number for images
            print(row['status'])
            if (int(row['status']) == 0):
                focus_no = len(foci_0)
            if (int(row['status']) == 1):
                focus_no = len(foci_1)
            if (int(row['status']) == 2):
                focus_no = len(foci_2)
            if (int(row['status']) == 3):
                focus_no = len(foci_3)
            if (int(row['status']) == 4):
                focus_no = len(foci_4)
            if (int(row['status']) == 5):
                focus_no = len(foci_5)

            # outlines and no outlines: embed focus image into larger foci wall
            print('Embed focus image into larger focus wall.')
            cols = 90
            foci_wall = np.zeros((img_maxproj.shape[0] + 5, cols), dtype=np.int)
            foci_wall_outlines = np.zeros((img_maxproj_outlines.shape[0] + 5, cols), dtype=np.int)
            foci_wall[0: img_maxproj.shape[0], 0: img_maxproj.shape[1]] = img_rawmaxproj
            foci_wall_outlines[0: img_maxproj_outlines.shape[0], 0: img_maxproj_outlines.shape[1]] = img_rawmaxproj_outlines

            text = img_dir + '/' + img_name + '_focusNo' + str(int(row['focus_label']))

            # check text doesn't go out of img bounds
            print('Check text is not out of image bounds.')
            if (img_maxproj.shape[1] + 5 > foci_wall.shape[1]):
                y = 1
            else:
                y = 5
            org = (40, y)
            text_color = 4000  # raw: 110
            print('Build focus wall.')
            print('no outlines')
            foci_wall = cv2.putText(np.float32(foci_wall), str(focus_no), org, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.3,
                                    color=text_color, thickness=1)  # text,
            print('outlines')
            foci_wall_outlines = cv2.putText(np.float32(foci_wall_outlines), str(focus_no), org, cv2.FONT_HERSHEY_SIMPLEX,
                                             fontScale=0.3,
                                             color=text_color, thickness=1)  # text
            print('Store focus according to class.')
            # file focus according to class
            if (int(row['status']) == 0):
                foci_info_0_pre = []
                foci_0.append(foci_wall)
                foci_0_outlines.append(foci_wall_outlines)
                foci_info_0_pre.append(focus_no)
                foci_info_0_pre.append(text)
                foci_info_0.append(foci_info_0_pre)
            if (int(row['status']) == 1):
                foci_info_1_pre = []
                foci_1.append(foci_wall)
                foci_1_outlines.append(foci_wall_outlines)
                foci_info_1_pre.append(focus_no)
                foci_info_1_pre.append(text)
                foci_info_1.append(foci_info_1_pre)
            if (int(row['status']) == 2):
                foci_info_2_pre = []
                foci_2.append(foci_wall)
                foci_2_outlines.append(foci_wall_outlines)
                foci_info_2_pre.append(focus_no)
                foci_info_2_pre.append(text)
                foci_info_2.append(foci_info_2_pre)
            if (int(row['status']) == 3):
                foci_info_3_pre = []
                foci_3.append(foci_wall)
                foci_3_outlines.append(foci_wall_outlines)
                foci_info_3_pre.append(focus_no)
                foci_info_3_pre.append(text)
                foci_info_3.append(foci_info_3_pre)
            if (int(row['status']) == 4):
                foci_info_4_pre = []
                foci_4.append(foci_wall)
                foci_4_outlines.append(foci_wall_outlines)
                foci_info_4_pre.append(focus_no)
                foci_info_4_pre.append(text)
                foci_info_4.append(foci_info_4_pre)
            if (int(row['status']) == 5):
                foci_info_5_pre = []
                foci_5.append(foci_wall)
                foci_5_outlines.append(foci_wall_outlines)
                foci_info_5_pre.append(focus_no)
                foci_info_5_pre.append(text)
                foci_info_5.append(foci_info_5_pre)
            stop = 1


def full_focus_info(summary_dir, dir_list):
    all_files = []

    for dir_ in dir_list:
        print(dir_)
        #dir_ = dir_list[44]
        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.roi')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.roi')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        # skip if there are no rois
        if (len(all_files) < 1):
            continue
        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/*.roi')
        if (len(roi_list) == 1):
            roi_is_zip = False
        elif (len(roi_list) < 1):
            # more than one ROI
            roi_list = glob(dir_[5] + '/*.zip')
            roi_is_zip = True
        else:
            print("Missing ROI file for movie file")
            continue

        for roi_file in roi_list:
            print(roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            for roi_count, bbox_points in enumerate(roi_points):
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = sep_file_[1][:-7]

                test1 = dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv'
                test2 = dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv'
                if ((os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv') == True) & (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == False)):
                    os.remove(os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv'))
                    print("focus_info_full_props.csv deleted")
                    continue

                elif ((os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv') == True) & (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == True)):
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv')
                    gen_info.append([sample_to_protein[dir_[3]], date_file, roi_folder, int(sample_counts_dir["count"]), len(df_foci["focus_label"])])
                    len_test = len(df_foci.columns)
                    all_col_names = list(df_foci.columns)
                    col_names_df = all_col_names[2:len_test]
                    col_names = []
                    col_names.append('protein')
                    col_names.append('exp_folder')
                    col_names.append('roi_id')
                    for col_name in col_names_df:
                        col_names.append(col_name)
                    col_foci_info.append(col_names)

                    # extract general and foci info from each spool
                    for focus_i, focus_row in df_foci.iterrows():
                        focus_list = []
                        print(focus_i)
                        '''
                        foci_info.append(
                            [sample_to_protein[dir_[3]], date_file, roi_folder, focus_row["focus_label"],
                             focus_row["status"],
                             focus_row["centroid-0"],
                             focus_row["centroid-1"], focus_row["orientation"], focus_row["area"],
                             focus_row["perimeter"], df_foci["major_axis_length"],
                             focus_row["minor_axis_length"],
                             focus_row["eccentricity"], focus_row["coords"], focus_row["bbox"],
                             focus_row["compare_val_correl"], focus_row["compare_val_chisq"],
                             focus_row["compare_val_intrsct"],
                             focus_row["compare_val_bc"], focus_row["compare_val_chisqalt"],
                             focus_row["compare_val_hellngr"],
                             focus_row["compare_val_kl"],
                             focus_row['var_lap'],
                             focus_row['var_lap_gb'],
                             focus_row['var_lap_mag'],
                             focus_row['var_lap_mag_gb']])
                        '''
                        focus_cols = focus_row[2:len_test]
                        focus_list.append(sample_to_protein[dir_[3]])
                        focus_list.append(date_file)
                        focus_list.append(roi_folder)
                        # add each col separately to list
                        for focus_cols_i, focus_cols_col in focus_cols.iteritems():
                            focus_list.append(focus_cols_col)
                        foci_info.append(focus_list)
                        test = 1

                else:
                    print(" No focus_info_full_props.csv file.")
                    continue

    print("Done with: " + summary_dir)


'''
col_foci_info = ['focus_label', 'centroid-0', 'centroid-1', 'orientation', 'area',
                                              'perimeter', 'major_axis_length',
                                              'minor_axis_length', 'eccentricity', 'coords', 'bbox',
                                              'compare_val_correl',
                                              'compare_val_chisq', 'compare_val_intrsct', 'compare_val_bc',
                                              'compare_val_chisqalt', 'compare_val_hellngr', 'compare_val_kl']
'''
def del_prev_status_idr(summary_dir, dir_list):
    all_files = []
    col_name_idx = 0
    #dir_list = [dir_list[i] for i in [0, 11, 13, 14, 15, 16, 17,18, 19, 1]]#dir_list[0:1] # for debugging
    for dir_ in dir_list:
        print(dir_)
        #dir_ = dir_list[44] # for debugging

        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.zip')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.zip')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        spool_list_file = spool_list[0]
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip')
        roi_is_zip = True

        for roi_file in roi_list:
            print(roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            for roi_count, bbox_points in enumerate(roi_points):
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = sep_file_[1][:-7]
                sep_file_date = date_file.split('_')
                sep_file_roi = roi_folder.split('_')

                # load roi img
                roi_img = sep_file_date[0] + '_' + sep_file_[2] + sep_file_[3] + '_preproc_01_raw_roi' + sep_file_roi[2] + '_afterResize_test1.tif'
                img_dir = sep_file_[0] + '/' + sep_file_[1] + '/' + sep_file_[2] + '/' + sep_file_[3] + '/' + roi_folder + '/' + roi_img# idr
                img = io.imread(img_dir)
                # take roi measurements
                shape = img.shape
                tot_pix_no_roi = (shape[0]) * (shape[1])

                if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') == True): # focus_info_full_props
                    os.remove(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status.csv') # focus_info_full_props

    print("Done with: " + summary_dir)

def full_focus_info_idr(summary_dir, dir_list):
    all_files = []
    col_name_idx = 0
    #dir_list = [dir_list[i] for i in [0, 11, 13, 14, 15, 16, 17,18, 19, 1]]#dir_list[0:1] # for debugging
    for dir_ in dir_list:
        print(dir_)
        #dir_ = dir_list[11] # for debugging

        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.zip')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.zip')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        spool_list_file = spool_list[0]
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip')
        roi_is_zip = True

        for roi_file in roi_list:
            print(roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            for roi_count, bbox_points in enumerate(roi_points):
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = sep_file_[1][:-7]
                sep_file_date = date_file.split('_')
                sep_file_roi = roi_folder.split('_')

                # load roi img
                roi_img = sep_file_date[0] + '_' + sep_file_[2] + sep_file_[3] + '_preproc_01_raw_roi' + sep_file_roi[2] + '_afterResize.tif'
                img_dir = sep_file_[0] + '/' + sep_file_[1] + '/' + sep_file_[2] + '/' + sep_file_[3] + '/' + roi_folder + '/' + roi_img# idr
                img = io.imread(img_dir)
                # take roi measurements
                shape = img.shape
                tot_pix_no_roi = (shape[0]) * (shape[1])

                if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv') == True): # focus_info_full_props
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv') # focus_info_full_props
                    roi_id = sep_file_[2] + sep_file_[3] + '_' + roi_folder
                    gen_info.append([date_file, roi_id, len(df_foci["focus_label"]), shape[0], shape[1], tot_pix_no_roi ])
                    len_test = len(df_foci.columns)

                    if (col_name_idx == 0):
                        # general info column names
                        gen_col_names = []
                        gen_col_names.append('exp_folder')
                        gen_col_names.append('roi_id')
                        gen_col_names.append('num_foci')
                        gen_col_names.append('roi_total_rows')
                        gen_col_names.append('roi_total_cols')
                        gen_col_names.append('roi_tot_pix_no')
                        col_gen_info.append(gen_col_names)

                        # focus info column names
                        all_col_names = list(df_foci.columns)
                        col_names_df = all_col_names[1:len_test]
                        col_names = []
                        col_names.append('exp_folder')
                        col_names.append('roi_id')
                        col_names.append('num_foci')
                        col_names.append('roi_total_rows')
                        col_names.append('roi_total_cols')
                        col_names.append('roi_tot_pix_no')
                        for col_name in col_names_df:
                            col_names.append(col_name)
                        col_foci_info.append(col_names)

                    # focus info .csv file: extract general and foci info from each spool
                    for focus_i, focus_row in df_foci.iterrows():
                        focus_list = []
                        print(focus_i)
                        focus_cols = focus_row[1:len_test]
                        focus_list.append(date_file)
                        focus_list.append(roi_id)
                        focus_list.append(len(df_foci["focus_label"]))
                        focus_list.append(shape[0])
                        focus_list.append(shape[1])
                        focus_list.append(tot_pix_no_roi)

                        # add each col separately to list
                        for focus_cols_i, focus_cols_col in focus_cols.iteritems():
                            focus_list.append(focus_cols_col)
                        foci_info.append(focus_list)
                        col_name_idx = col_name_idx + 1

                else:
                    print(" No focus_info_full_props.csv file.")
                    continue

    print("Done with: " + summary_dir)


def full_focus_info_df_twochannel(summary_dir, dir_list):
    all_files = []
    col_name_idx = 0
    #dir_list = [dir_list[i] for i in [0, 11, 13, 14, 15, 16, 17,18, 19, 1]]#dir_list[0:1] # for debugging
    for dir_ in dir_list:
        print(dir_)
        #dir_ = dir_list[44] # for debugging
        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.zip')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.zip')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        dir_files = os.listdir(dir_[5])
        spool_list = sorted(dir_files)
        spool_list_file = spool_list[0]
        roi_list = glob(dir_[5] + '/' + spool_list_file[:-4] + '_roi' + '*.zip')
        roi_is_zip = True

        for roi_file in roi_list:
            print(roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            for roi_count, bbox_points in enumerate(roi_points):
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = re.compile(r"[//]").split(summary_dir)[2]
                sep_file_date = date_file.split('_')
                sep_file_roi = roi_folder.split('_')

                # load roi img
                roi_img = sep_file_date[0] + '_'+ sep_file_[1] + sep_file_[2] + '_preproc_01_raw_roi' + sep_file_roi[3] + '_afterResize.tif'
                img_dir = sep_file_[0] + '/' + sep_file_[1] + '/' + sep_file_[2] + '/' + roi_folder + '/' + roi_img # idr
                img = io.imread(img_dir)
                # take roi measurements
                shape = img.shape
                tot_pix_no_roi = (shape[0]) * (shape[1])

                # loop through channels
                channel_names = ['R','G']
                for i_chanl in channel_names:
                    if (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_' + i_chanl + '.csv') == True): # focus_info_full_props
                        df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_' + i_chanl + '.csv') # focus_info_full_props
                        roi_id = sep_file_[1] + roi_folder
                        # general info .csv files: save in corresponding channel
                        gen_list = [date_file, roi_id, len(df_foci["focus_label"]), shape[0], shape[1], tot_pix_no_roi]
                        if (i_chanl == 'R'):
                            dfs_gen_R.append(gen_list)
                        if (i_chanl == 'G'):
                            dfs_gen_G.append(gen_list)
                        len_test = len(df_foci.columns)

                        # both .csv files: define column names
                        if (col_name_idx == 0):
                            # general info column names
                            gen_col_names = []
                            gen_col_names.append('exp_folder')
                            gen_col_names.append('roi_id')
                            gen_col_names.append('num_foci')
                            gen_col_names.append('roi_total_rows')
                            gen_col_names.append('roi_total_cols')
                            gen_col_names.append('roi_tot_pix_no')
                            col_gen_info.append(gen_col_names)

                            # focus info column names
                            all_col_names = list(df_foci.columns)
                            col_names_df = all_col_names[1:len_test]
                            col_names = []
                            col_names.append('exp_folder')
                            col_names.append('roi_id')
                            col_names.append('channel')
                            col_names.append('num_foci')
                            col_names.append('roi_total_rows')
                            col_names.append('roi_total_cols')
                            col_names.append('roi_tot_pix_no')
                            for col_name in col_names_df:
                                col_names.append(col_name)
                            col_foci_info.append(col_names)

                        # focus info .csv file: extract general and foci info from each spool
                        for focus_i, focus_row in df_foci.iterrows():
                            focus_list = []
                            print(focus_i)
                            focus_cols = focus_row[1:len_test]
                            focus_list.append(date_file)
                            focus_list.append(roi_id)
                            focus_list.append(i_chanl)
                            focus_list.append(len(df_foci["focus_label"]))
                            focus_list.append(shape[0])
                            focus_list.append(shape[1])
                            focus_list.append(tot_pix_no_roi)

                            # add each col separately to list
                            for focus_cols_i, focus_cols_col in focus_cols.iteritems():
                                focus_list.append(focus_cols_col)
                            # save focus information in corresponding channel
                            if (i_chanl == 'R'):
                                dfs_foci_R.append(focus_list)
                            if (i_chanl == 'G'):
                                dfs_foci_G.append(focus_list)

                            col_name_idx = col_name_idx + 1

                    else:
                        print(" No focus_info_full_props.csv file.")
                        continue

    print("Done with: " + summary_dir)

def full_focus_info_df(summary_dir, dir_list):
    all_files = []
    col_name_idx = 0
    for dir_ in dir_list:
        print(dir_)
        #dir_ = dir_list[44]
        # load all tiff movie files that end in _N_roi.tif (N == a number)
        files = glob(dir_[5] + '/*.roi')
        for dir_i in dir_list:
            some_files = glob(dir_i[5] + '/*.roi')  # change to _roi.tif later
            if (len(some_files) > 0):
                all_files.append(some_files)
                some_files = []
            else:
                continue

        # skip if there are no rois
        if (len(all_files) < 1):
            continue
        all_spools = pd.DataFrame(all_files)
        spools = all_spools.apply(pd.Series).stack().unique()
        lspools = spools.tolist()

        # obtain number of samples
        sep_files = []
        for num in range(0, len(lspools)):
            sep_file = re.compile(r"[\\]").split(lspools[num])
            sep_files.append(sep_file[2])
            sep_file = []

        counts, values = pd.Series(sep_files).value_counts().values, pd.Series(sep_files).value_counts().index
        df_results = pd.DataFrame(list(zip(values, counts)), columns=["value", "count"])
        sample_counts_dir = df_results.loc[df_results["value"] == dir_[3]]

        # Find ROI file
        # single ROI
        roi_list = glob(dir_[5] + '/*.roi')
        if (len(roi_list) == 1):
            roi_is_zip = False
        elif (len(roi_list) < 1):
            # more than one ROI
            roi_list = glob(dir_[5] + '/*.zip')
            roi_is_zip = True
        else:
            print("Missing ROI file for movie file")
            continue

        for roi_file in roi_list:
            print(roi_file)
            # load the ROI and extract the coords
            # make a mask from the coords
            # create folder for this cropped out cell
            # use mask to save the cropped movie file
            if (roi_is_zip):
                roi_points = read_roi.read_roi_zip(roi_file)
            else:
                fobj = open(roi_file, 'r+b')
                roi_points = read_roi.read_roi(fobj)
                roi_points = [roi_points, ]  # make format same as if its a zip

            # ROI is bounding box, make mask
            # go through each frame and apply masks, save cropped movie file
            for roi_count, bbox_points in enumerate(roi_points):
                file = []
                for ext in ('/*.roi', '/*.zip'):
                    roifile = glob(dir_[5] + ext)
                    if (len(roifile) > 0):
                        file.append(''.join(roifile))
                file = ''.join(file)
                print('Processing ', file[:-4] + '_' + str(roi_count + 1))
                file_root = os.path.split(file)[1][:-4]
                sep_file_ = re.compile(r"[\\]").split(file)
                roi_folder = file_root + '_' + str(roi_count + 1)
                date_file = re.compile(r"[//]").split(summary_dir)[2]

                if ((os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv') == True) & (os.path.isfile(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_status_G.csv') == True)):
                    df_foci = pd.read_csv(dir_[5] + '/' + roi_folder + '/' + 'focus_info_full_props.csv')
                    gen_info.append([date_file, roi_folder, len(df_foci["focus_label"])])
                    len_test = len(df_foci.columns)

                    # both .csv files: define column names
                    if (col_name_idx == 0):
                        # general info column names
                        gen_col_names = []
                        gen_col_names.append('exp_folder')
                        gen_col_names.append('roi_id')
                        # gen_col_names.append('NumROIs')
                        gen_col_names.append('num_foci')
                        col_gen_info.append(gen_col_names)

                        # focus info column names
                        all_col_names = list(df_foci.columns)
                        col_names_df = all_col_names[2:len_test]
                        col_names = []
                        col_names.append('exp_folder')
                        col_names.append('roi_id')
                        for col_name in col_names_df:
                            col_names.append(col_name)
                        col_foci_info.append(col_names)

                    # extract general and foci info from each spool
                    for focus_i, focus_row in df_foci.iterrows():
                        focus_list = []
                        print(focus_i)
                        focus_cols = focus_row[2:len_test]
                        focus_list.append(date_file)
                        focus_list.append(roi_folder)
                        # add each col separately to list
                        for focus_cols_i, focus_cols_col in focus_cols.iteritems():
                            focus_list.append(focus_cols_col)
                        foci_info.append(focus_list)
                        test = 1
                        col_name_idx = col_name_idx + 1
                else:
                    print(" No focus_info_full_props.csv file.")
                    continue

    print("Done with: " + summary_dir)

def heatmaps(df_foci, df_foci_idr, df_foci_name, df_foci_idr_name, props):

    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/Results/' + time_dir)):
        os.mkdir(base_dir + '/Results/' + time_dir)
    save_dir = base_dir + '/Results/' + time_dir + '/Heatmaps'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)

    # ******************************************************************************
    # START : LOAD PROPERTIES
    # save csv used for record keeping
    df_props = pd.DataFrame(props)
    df_props.to_csv(save_dir + '/' + 'properties_' + time_dir + '.csv')
    df_foci.to_csv(save_dir + '/' + df_foci_name + '.csv')
    df_foci_idr.to_csv(save_dir + '/' + df_foci_idr_name + '.csv')

    df_foci_v0 = df_foci.copy()
    # For resizing comparison: Index subset in df_foci_og, same foci as df_foci_resize
    iloc_rows = []
    df_foci_og_subset = df_foci.iloc[iloc_rows, :]

    # END : LOAD PROPERTIES
    # ******************************************************************************

    # ******************************************************************************
    # START : PROPERTIES HEATMAP (CORRLTN MATRIX)
    # *******************************************
    # START: SELECT PROPERTIES, CREATE NUMERIC LABELS
    props_conv = {}
    col_names = df_foci_v0.columns.values.tolist()

    # drop properties
    # define dropped properties
    dropped_props = df_foci.drop(columns = 'status', axis=1).copy()
    dropped_props_idr = df_foci_idr.drop(columns = 'status', axis=1).copy()

    print('dropped props: ')
    print(dropped_props)
    # assign focus and fft col names
    col_names = dropped_props.columns

    # change names to numbers for easier labeling
    props_nums = [str(x) for x in range(len(col_names))]
    for i in range(len(col_names)):
        props_conv[col_names[i]] =  props_nums[i] # dictionary with name keys
    props_nums_hist = tuple(props_nums)
    props_conv_df = pd.DataFrame(props_conv.items())
    props_conv_df.to_csv(save_dir + '/properties_converter.csv')

    # organize correlation histogram
    props_nums = ['30','1', '2', '44','3', '27','33','4', '16',
    '17', '19', '18', '40','41', '26',  '8',  '7', '10', '12',
      '29', '28', '5' , '11', '9','45','46', '20', '0',
      '31','32','25', '15','6',
    '13',  '21','34' , '14', '22', '39',
    '43', '42',  '48',  '47',  '38','35', '36','37', '23','24']

    # dictionaries with number keys
    inv_props_conv = {v: k for k, v in props_conv.items()}

    # END: SELECT PROPERTIES, CREATE NUMERIC LABELS
    # *******************************************
    # *******************************************
    # START: PLOT HEATMAPS
    # *******************************************
    # correlation heatmap, train set
    plt.close()
    df_foci.rename(columns= props_conv, inplace= True)
    print(df_foci)
    props = df_foci[props_nums]
    corrltn_vals = props._convert(numeric=True).corr()
    dataplot = sns.heatmap(corrltn_vals, cmap='BrBG', vmin=-1, vmax=1, xticklabels=True, yticklabels=True)
    dataplot.set_xticklabels(dataplot.get_xmajorticklabels(), fontsize=5)
    dataplot.set_yticklabels(dataplot.get_ymajorticklabels(), fontsize=5)
    # dataplot.set_title('Correlation Heatmap',  fontdict={'fontsize': 18}, pad=12);
    plt.savefig(save_dir + '/heatmap_num_allprops_'+ df_foci_name + '_' + time_dir +'.pdf', dpi=300, bbox_inches='tight')
    corrltn_vals.to_csv(save_dir + '/correlation_values_train.csv')
    print(corrltn_vals)

    # plt.show()
    plt.close()

    # correlation heatmap, test set
    df_foci_idr.rename(columns= props_conv, inplace=True)
    props_idr = df_foci_idr[props_nums]
    corrltn_vals_idr = props_idr._convert(numeric=True).corr()
    dataplot_idr = sns.heatmap(corrltn_vals_idr, cmap='BrBG', vmin=-1, vmax=1, xticklabels=True, yticklabels=True)
    dataplot_idr.set_xticklabels(dataplot_idr.get_xmajorticklabels(), fontsize=5)
    dataplot_idr.set_yticklabels(dataplot_idr.get_ymajorticklabels(), fontsize=5)
    # dataplot.set_title('Correlation Heatmap',  fontdict={'fontsize': 18}, pad=12);
    # plt.show()
    plt.savefig(save_dir + '/heatmap_num_allprops_'+ df_foci_idr_name + '_'+ time_dir +'.pdf', dpi=300, bbox_inches='tight')
    corrltn_vals_idr.to_csv(save_dir + '/correlation_values_test.csv')
    plt.close()

    # scatter plot of correlation heatmaps
    scatter_corrlt = []
    fig, ax = plt.subplots()
    plt.plot([-1, 1], [-1, 1], color='black', linewidth=2)
    plt.xlim(-1.4, 1.4)
    plt.ylim(-1.4, 1.4)
    y = np.random.random((props.shape[1], 5))
    markers = cycler(marker=['|', '_', '.', '*', 'd', '^', 'h', 'x'])
    colors = cycler(color=['darkgoldenrod','darkturquoise','tomato','pink','purple','darkgreen', 'gold'])
    style = iter(markers * colors)

    corrltn_vals_idr = corrltn_vals_idr.reset_index(drop=True)
    corrltn_vals = corrltn_vals.reset_index(drop=True)
    for focus_i, focus_row in corrltn_vals.iterrows():
        print(focus_i)
        idr_vals = corrltn_vals_idr.iloc[int(focus_i)]
        plt.scatter(focus_row, idr_vals, alpha=0.5, label=focus_i, **next(style)) # marker=next(marker)
        plt.xlabel('train')
        plt.ylabel('test')
    ax.legend(fancybox=True, ncol=2, bbox_to_anchor=(1, 0.5), loc='center left') #(0.9, 0.5)
    #plt.show()
    plt.savefig(save_dir + '/corr_scatterplt_allprops_' + df_foci_name + '_and_' + df_foci_idr_name + '_' + time_dir +'.pdf', dpi=300, bbox_inches='tight')
    plt.close()

    print('Heatmaps completed.')

    # END: PLOT HEATMAPS
    # *******************************************
    # END : PROPERTIES HEATMAP (CORRLTN MATRIX)
    # ******************************************************************************

def exploration_plots(summary_dir):

    # ******************************************************************************
    # START : LOAD PROPERTIES
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    #df_foci = pd.read_csv(base_dir + '/foci_info_norm.csv')  # pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci_v0 = df_foci.copy()
    #df_foci_idr = pd.read_csv(base_dir + '/foci_info_idr_full_norm_knn.csv')

    # For resizing comparison: Index subset in df_foci_og, same foci as df_foci_resize
    iloc_rows = []
    df_foci_og_subset = df_foci.iloc[iloc_rows, :]

    df_foci = df_foci.loc[ (df_foci['status'] == 1) | (df_foci['status'] == 3) | (df_foci['status'] == 4) | (df_foci['status'] == 5)]
    df_foci_idr = df_foci_idr.loc[ (df_foci_idr['status'] == 1) | (df_foci_idr['status'] == 3) | (df_foci_idr['status'] == 4) | (df_foci_idr['status'] == 5)]
    # END : LOAD PROPERTIES
    # ******************************************************************************

    # ******************************************************************************
    # START : CLASS PREDICTION ERROR
    '''
    # Class prediction error
    # Perform 80/20 training/test split
    X_train, X_test, y_train, y_test = train_test_split(X_raw_clsfrs, Y_raw_clsfrs, test_size=0.20, random_state=42)
    # Instantiate the classification model and visualizer
    visualizer = ClassPredictionError(RandomForestClassifier(random_state=42, n_estimators=10), classes=[0, 1, 2, 3, 4, 5])
    # Fit the training data to the visualizer
    visualizer.fit(X_train, y_train)
    # Evaluate the model on the test data
    visualizer.score(X_test, y_test)
    # Draw visualization
    visualizer.show()
    plt.close()

    # include weights
    # Instantiate the classification model and visualizer
    visualizer = ClassPredictionError(RandomForestClassifier(random_state=42, n_estimators=10, class_weight='balanced'), classes=[0, 1, 2, 3, 4, 5])
    # Fit the training data to the visualizer
    visualizer.fit(X_train, y_train)
    # Evaluate the model on the test data
    visualizer.score(X_test, y_test)
    # Draw visualization
    visualizer.show()
    plt.close()

    # other weights
    # Instantiate the classification model and visualizer
    visualizer = ClassPredictionError(RandomForestClassifier(random_state=42, n_estimators=10, class_weight= class_weight), classes=[0, 1, 2, 3, 4, 5])
    # Fit the training data to the visualizer
    visualizer.fit(X_train, y_train)
    # Evaluate the model on the test data
    visualizer.score(X_test, y_test)
    # Draw visualization
    visualizer.show()
    '''
    # END : CLASS PREDICTION ERROR
    # ******************************************************************************


    # ******************************************************************************
    # START : PROPERTIES HEATMAP (CORRLTN MATRIX)
    # eliimnate: 'fft_eccentricity','orientation', 'compare_val_correl','focus_med', 'focus_mean': '35', , 'fft_min': '34', 'fft_std': '25', 'fft_var_laplacian': '32'
    # 'var_lap_gb9': '31', 'focus_var': '30', 'focus_std': '29', 'fft_euler_number': '28', 'euler_number': '26', 'fft_orientation': '31','fft_blur_2': '12',
    # 'fft_major_axis_length': '18', 'fft_minor_axis_length': '19','solidity': '24',  'fft_mean': '17', 'fft_med': '18',

    '''
    # best properties
    props_conv = {'area': '0', 'perimeter': '1', 'major_axis_length': '2','minor_axis_length': '3',
                  'focus_sum': '4', 'fft_area': '5', 'fft_perimeter': '6', 'fft_max': '7', 'fft_sum': '8', 'tot_pix_no': '9',
                 'fft_shannon_entropy': '10','blur_2': '11', 'shannon_entropy': '12','compare_val_intrsct': '13',
                  'compare_val_chisq': '14','compare_val_chisqalt': '15',
                 'compare_val_kl': '16', 'fft_var': '17','compare_val_bc': '18', 'compare_val_hellngr': '19',
                 'eccentricity': '20', 'fft_solidity': '21',
                 'compactness_pp': '22', 'compactness_s': '23', 'var_lap': '24','var_lap_gb7': '25',
                 'var_lap_gb3': '26', 'var_lap_gb5': '27', 'fft_compactness_pp': '28', 'fft_compactness_s': '29'}

    props_nums = ['0', '1', '2', '3', '4','5' ,'6', '7', '8', '9', '10', '11', '12', '13',
                             '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27','28', '29']

    props_nums_hist = ('0', '1', '2', '3', '4','5' ,'6', '7', '8', '9', '10', '11', '12', '13',
                             '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27','28', '29')
    props_names = ('area', 'perimeter', 'major_axis_length','minor_axis_length',
                  'focus_sum', 'fft_area', 'fft_perimeter', 'fft_max', 'fft_sum', 'tot_pix_no',
                 'fft_shannon_entropy','blur_2', 'shannon_entropy','compare_val_intrsct',
                  'compare_val_chisq','compare_val_chisqalt',
                 'compare_val_kl', 'fft_var','compare_val_bc', 'compare_val_hellngr',
                 'eccentricity', 'fft_solidity', 'compactness_pp', 'compactness_s', 'var_lap','var_lap_gb7',
                 'var_lap_gb3', 'var_lap_gb5', 'fft_compactness_pp', 'fft_compactness_s')
    '''

    # all properties----- START
    props_conv = {}
    col_names = df_foci_v0.columns.values.tolist()
    col_names = [e for e in col_names if e not in ('Unnamed: 0', 'protein', 'exp_folder', 'roi_id', 'focus_label', 'status', 'centroid-0', 'centroid-1', 'coords', 'bbox', 'focus_max', 'focus_min')] # drop these cols
    print(col_names)
    fft_col_names = ['fft_orientation', 'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_eccentricity', 'fft_euler_number', 'fft_solidity', 'fft_compactness_pp','fft_compactness_s', 'fft_var_laplacian', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_sum', 'fft_std', 'fft_var', 'fft_max', 'fft_min']
    focus_col_names = [f for f in col_names if f not in ('fft_orientation', 'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_eccentricity', 'fft_euler_number', 'fft_solidity', 'fft_compactness_pp','fft_compactness_s', 'fft_var_laplacian', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_sum', 'fft_std', 'fft_var', 'fft_max', 'fft_min')] # drop these cols
    props_nums = [str(x) for x in range(len(col_names))]
    # change names to numbers for easier labeling
    for i in range(len(col_names)):
        props_conv[col_names[i]] =  props_nums[i] # dictionary with name keys
    props_nums_hist = tuple(props_nums)
    # index dictionaries to find numbers associated with names
    focus_col_nums = [props_conv[x] for x in focus_col_names]
    fft_col_nums = [props_conv[x] for x in fft_col_names]
    print(props_conv)

    # tuples
    focus_col_nums_t = tuple(focus_col_nums)
    fft_col_nums_t = tuple(fft_col_nums)

    # dictionaries with number keys
    inv_props_conv = {v: k for k, v in props_conv.items()}
    # all properties----- END

    # index names for sym and blur----- START
    # sym
    sym_props_focus = ['2','3','4','5','9','11','14','35','36','37','38','39','40','41','42','43','44','45','46','47','48','49']
    sym_props_fft = ['16', '17','18','19','23','24','26','27','28','29','33']
    sym_props = sym_props_focus + sym_props_fft
    sym_props_names = [inv_props_conv[x] for x in  sym_props]
    print(sym_props_names) #['eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'focus_med', 'compactness_pp', 'compactness_s', 'fft_mean', 'fft_med']
    sym_props_names = ['eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'focus_med', 'compactness_pp', 'compactness_s',
     'fft_mean', 'fft_med']

    # blur
    blur_props_focus = ['5', '9', '11', '43', '47', '48']
    blur_props_fft = ['28', '29']
    blur_props = blur_props_focus + blur_props_fft
    blur_props_names = [inv_props_conv[x] for x in  blur_props]
    print(blur_props_names) # ['perimeter', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'solidity', 'var_lap', 'var_lap_gb3', 'var_lap_gb5', 'var_lap_gb7', 'var_lap_gb9', 'blur_2', 'shannon_entropy', 'focus_mean', 'focus_med', 'focus_sum', 'focus_std', 'focus_var', 'compactness_pp', 'compactness_s', 'tot_pix_no', 'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_compactness_pp', 'fft_compactness_s', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_max']

    blur_props_names = ['perimeter', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'compare_val_bc', 'compare_val_hellngr',
     'solidity', 'var_lap', 'var_lap_gb3', 'var_lap_gb5', 'var_lap_gb7', 'var_lap_gb9', 'blur_2', 'shannon_entropy',
     'focus_mean', 'focus_med', 'focus_sum', 'focus_std', 'focus_var', 'compactness_pp', 'compactness_s', 'tot_pix_no',
     'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_compactness_pp',
     'fft_compactness_s', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_max']

    sym_props_names = ['eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'focus_med', 'compactness_pp', 'compactness_s',
     'fft_mean', 'fft_med']

    # BP1-2 correlation heatmap
    df_foci.rename(columns= props_conv, inplace= True)
    props = df_foci[props_nums]
    corrltn_vals = props._convert(numeric=True).corr()
    dataplot = sns.heatmap(corrltn_vals, cmap='BrBG', vmin=-1, vmax=1)
    dataplot.set_xticklabels(dataplot.get_xmajorticklabels(), fontsize=5)
    dataplot.set_yticklabels(dataplot.get_ymajorticklabels(), fontsize=5)
    # dataplot.set_title('Correlation Heatmap',  fontdict={'fontsize': 18}, pad=12);
    #plt.savefig(base_dir + '/heatmap_num_BP1-2_allprops_test.pdf', dpi=300, bbox_inches='tight')
    # plt.show()
    plt.close()

    # 53BP1 correlation heatmap
    df_foci_idr.rename(columns= props_conv, inplace=True)
    props_idr = df_foci_idr[props_nums]
    corrltn_vals_idr = props_idr._convert(numeric=True).corr()
    dataplot_idr = sns.heatmap(corrltn_vals_idr, cmap='BrBG', vmin=-1, vmax=1)
    dataplot_idr.set_xticklabels(dataplot_idr.get_xmajorticklabels(), fontsize=5)
    dataplot_idr.set_yticklabels(dataplot_idr.get_ymajorticklabels(), fontsize=5)
    # dataplot.set_title('Correlation Heatmap',  fontdict={'fontsize': 18}, pad=12);
    # plt.show()
    #plt.savefig(base_dir + '/heatmap_num_idr_53BP1_allprops.pdf', dpi=300, bbox_inches='tight')
    plt.close()

    # scatter plot of correlation heatmaps
    scatter_corrlt = []
    fig, ax = plt.subplots()
    plt.plot([-1, 1], [-1, 1], color='black', linewidth=2)
    plt.xlim(-1.4, 1.4)
    plt.ylim(-1.4, 1.4)
    y = np.random.random((props.shape[1], 5))
    markers = cycler(marker=['|', '_', '.', '*', 'd', '^', 'h', 'x'])
    colors = cycler(color=['darkgoldenrod','darkturquoise','tomato','pink','purple','darkgreen', 'gold'])
    style = iter(markers * colors)

    corrltn_vals_idr = corrltn_vals_idr.reset_index(drop=True)
    corrltn_vals = corrltn_vals.reset_index(drop=True)
    for focus_i, focus_row in corrltn_vals.iterrows():
        if (focus_i == 48):
            stop = 1
        print(focus_i)
        idr_vals = corrltn_vals_idr.iloc[int(focus_i)]
        plt.scatter(focus_row, idr_vals, alpha=0.5, label=focus_i, **next(style)) # marker=next(marker)
        plt.xlabel('BP1-2')
        plt.ylabel('53BP1')
    ax.legend(fancybox=True, ncol=2, bbox_to_anchor=(0.9, 0.5), loc='center left')
    plt.show()
    #plt.savefig(base_dir + '/corr_scatterplt_allprops.pdf', dpi=300, bbox_inches='tight')
    plt.close()

    # END : PROPERTIES HEATMAP (CORRLTN MATRIX)
    # ******************************************************************************


    #******************************************************************************
    # START: PLOT PROPERTIES HISTOGRAMS: FOR LOOP
    classes = df_foci['status'].unique()
    classes = np.array([1, 3])
    classes = classes[~np.isnan(classes)] # drop nans
    classes.sort(axis=0)
    classes_l = ''.join(map(str, classes. astype(int).tolist()))
    # plot blur/sym props
    focus_col_nums = ['5', '29']#sym_props
    focus_col_nums_t = tuple(focus_col_nums)
    # user defined variables
    df_foci = df_foci_idr
    props = df_foci[focus_col_nums] # props_nums, focus_col_nums, fft_col_nums
    props_plt = 'focus_props' # all_props, corrltd_props focus_props, fft_props
    bins = 20
    norm = 1 # 1: apply normalization weights or 0: don't apply normalization weights
    len_props = len(props.columns)
    sq_len_props = int(sqrt(len_props))
    fignum = 1
    fig, axes = plt.subplots( sq_len_props, (sq_len_props + 1), figsize=(20, 20)) #sharey=True, figsize=(16, 16)
    # loop through all subplots
    for ii, ax in enumerate(axes.flat):
        if (ii == len_props):
            break

        # set prop_nums var
        if (props_plt == 'focus_props'):
            prop_nums = focus_col_nums_t
        elif (props_plt == 'fft_props'):
            prop_nums = fft_col_nums_t
        prop = prop_nums[ii]

        # histogram for each class
        for i_class, cls in enumerate(classes):
            data = df_foci.loc[(df_foci['status'] == cls)][prop]
            # normalization weights
            data_w = np.empty(data.shape)
            data_w.fill(1 / data.shape[0])
            # normalization application
            if (norm == 1):
                n, bins_, patches = ax.hist(data, weights=data_w, bins=bins, alpha=0.5, label=str(cls))
                norm_plt = 'norm'
            elif (norm == 0):
                n, bins_, patches = ax.hist(data, bins = bins, alpha=0.5, label = str(cls))
                norm_plt = 'noNorm'
            del data, data_w
        ax.set_title(prop)
        fignum = fignum + 1
        #plt.show()
    # delete unused axes
    for ax in axes.flat[len_props:]:
        ax.remove()
    # label and save
    #fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical') # shared y label
    plt.tight_layout()
    fig.legend(classes, fancybox=True, loc='center left', bbox_to_anchor= (0.9, 0.5)) #
    plt.show()
    #plt.savefig(base_dir + '/foci_props_hist_'+ props_plt +'_'+ norm_plt +'_' + classes_l +'_blur_best_idr.pdf')
    plt.close()

    # names
    # 'eccentricity_compactness_s_compactness_pp_shannon_entropy'
    # 'tot_pix_no_focus_sum_focus_mean_focus_med'
    # 'focus_std_focus_var'
    # 'blur_2_var_lap_var_lap_gb3_var_lap_gb5'
    # 'var_lap_gb7_var_lap_gb9'
    # 'fft_compactness_s_fft_compactness_pp_fft_shannon_entropy_fft_blur_2'
    # 'fft_mean_fft_med_fft_std_fft_var'
    # 'fft_sum_fft_max_fft_min'

    # END: PLOT PROPERTIES HISTOGRAMS: FOR LOOP
    #******************************************************************************
    # ----
    df_foci_simple = df_foci.copy()
    # df_foci_simple = df_foci_resize.copy()
    df_foci_simple_idr = df_foci_idr.copy()

    '''
    # ******************************************************************************
    # START: PLOT PIX VS VAR OF LAP
    x_var = 'tot_pix_no'
    y_var = 'var_lap_gb7'
    test = df_foci_simple['area'].iloc[0]
    df_foci_1 = df_foci_simple.loc[(df_foci_simple['status'] == 1)]
    df_foci_3 = df_foci_simple.loc[(df_foci_simple['status'] == 3)]
    y3_0 = df_foci_3[y_var]
    x3_0 = df_foci_3[x_var]
    y1_0 = df_foci_1[y_var]
    x1_0 = df_foci_1[x_var]
    plt.scatter(x3_0, y3_0, alpha=0.5, label = 'sym')
    plt.scatter(x1_0, y1_0, alpha=0.5, label = 'blur')
    plt.title("Effect of total pixel no. on blur metric")
    plt.ylabel(y_var)
    plt.xlabel(x_var)
    plt.legend()
    plt.xlim(0, 1000)
    #plt.show()
    plt.close()

    # END: PLOT PIX VS VAR OF LAP
    # ******************************************************************************
    '''
def focus_normalization(datasets):
    # ******************************************************************************
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/Results/' + time_dir + '/CSV_files')):
        os.mkdir(base_dir + '/Results/' + time_dir + '/CSV_files')
    save_dir = base_dir + '/Results/' + time_dir + '/CSV_files/FocusNormalization'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)

    # load properties
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    df_foci_name = datasets[2].split('.')[0]
    df_foci_idr_name = datasets[3].split('.')[0]
    props = datasets[4]

    # save inputs
    df_foci.to_csv(save_dir + '/focusnorm_train_input_'+ time_dir +'.csv')
    df_foci_idr.to_csv(save_dir + '/focusnorm_test_input_'+ time_dir +'.csv')
    df_props = pd.DataFrame(props)
    df_props.to_csv(save_dir + '/focusnorm_props_input_'+ time_dir +'.csv')

    # drop rows with nans
    df_foci = df_foci.dropna()
    df_foci_idr = df_foci_idr.dropna()

    # temporarily drop non-numerical columns
    # define dropped properties
    dropped_props = df_foci.columns.difference(props)
    dropped_props_idr = df_foci_idr.columns.difference(props)

    # drop dropped properties
    df_foci_ = df_foci.drop(dropped_props, axis=1).copy()
    df_foci_idr_ = df_foci_idr.drop(dropped_props_idr, axis=1).copy()

    # normalize all focus properties wrt to its total pixel number in focus roi
    labels = ['train', 'test']
    datasets = [df_foci_, df_foci_idr_]
    for i_dataset in range(len(labels)):
        foci_props_normalized = []
        i_df = datasets[i_dataset]
        dataset_label = labels[i_dataset]
        for i_train, row_train in i_df.iterrows():
            print(i_train)
            foci_props_normalized.append(row_train/int(row_train['tot_pix_no']))
        cols = df_foci_.columns
        if (dataset_label == 'train'):
            df_train = pd.DataFrame(foci_props_normalized, columns= cols)
            # return non-numerical columns
            for i_col in dropped_props:
                df_train[i_col] = df_foci[i_col]
        if (dataset_label == 'test'):
            df_test = pd.DataFrame(foci_props_normalized, columns= cols)
            # return non-numerical columns
            for i_col in dropped_props_idr:
                df_test[i_col] = df_foci_idr[i_col]

    # save normalized properties
    df_foci_pixnorm_name = df_foci_name + '_pixnorm_' + time_dir
    df_foci_idr_pixnorm_name = df_foci_idr_name + '_pixnorm_' + time_dir
    df_foci_pixnorm_path = save_dir + '/' + df_foci_pixnorm_name + '.csv'
    df_foci_idr_pixnorm_path = save_dir + '/' + df_foci_idr_pixnorm_name + '.csv'

    df_train.to_csv(df_foci_pixnorm_path)
    df_test.to_csv(df_foci_idr_pixnorm_path)
    print('Focus pixel normalization completed.')
    return df_train, df_test, df_foci_pixnorm_name, df_foci_idr_pixnorm_name, df_foci_pixnorm_path, df_foci_idr_pixnorm_path

def property_normalization_and_standardization(df_foci, df_foci_idr, df_foci_name,df_foci_idr_name, props):
    # ******************************************************************************
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/Results/' + time_dir + '/CSV_files')):
        os.mkdir(base_dir + '/Results/' + time_dir + '/CSV_files')
    save_dir = base_dir + '/Results/' + time_dir + '/CSV_files/PropertyNormaAndStand'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)
    # save inputs
    df_foci.to_csv(save_dir + '/prop-norm-stndrztn_train_input_'+ time_dir +'.csv')
    df_foci_idr.to_csv(save_dir + '/prop-norm-stndrztn_test_input_'+ time_dir +'.csv')
    df_props = pd.DataFrame(props)
    df_props.to_csv(save_dir + '/prop-norm-stndrztn_props_input_'+ time_dir +'.csv')

    # drop rows with nans
    df_foci = df_foci.dropna()
    df_foci_idr = df_foci_idr.dropna()

    # temporarily drop non-numerical columns
    # define dropped properties
    dropped_props = df_foci.columns.difference(props)
    dropped_props_idr = df_foci_idr.columns.difference(props)

    # drop dropped properties
    df_foci_ = df_foci.drop(dropped_props, axis=1).copy()
    df_foci_idr_ = df_foci_idr.drop(dropped_props_idr, axis=1).copy()

    # normalize all focus properties wrt to its total pixel number in focus roi
    labels = ['train', 'test']
    datasets = [df_foci_, df_foci_idr_]
    for i_dataset in range(len(labels)):
        foci_props_normalized = []
        i_df = datasets[i_dataset]
        dataset_label = labels[i_dataset]
        for i_train, col_train in i_df.iteritems():
            print(i_train)
            foci_props_normalized.append(col_train/col_train.mean()) # scale(col_train/col_train.mean())
        cols = df_foci_.columns
        if (dataset_label == 'train'):
            foci_props_normalized_ = list(map(list, zip(*foci_props_normalized)))# transpose list
            # fit scaler on training data
            norm = MinMaxScaler().fit(foci_props_normalized_)
            foci_props_normalized_  = norm.transform(foci_props_normalized_) #scale(foci_props_normalized_ )
            df_train = pd.DataFrame(foci_props_normalized_, columns= cols)
            # return non-numerical columns
            for i_col in dropped_props:
                df_train[i_col] = df_foci[i_col]
        if (dataset_label == 'test'):
            foci_props_normalized_ = list(map(list, zip(*foci_props_normalized)))  # transpose list
            foci_props_normalized_ = norm.transform(foci_props_normalized_) #scale(foci_props_normalized_)
            df_test = pd.DataFrame(foci_props_normalized_, columns=cols)
            # return non-numerical columns
            for i_col in dropped_props_idr:
                df_test[i_col] = df_foci_idr[i_col]

    # save normalized properties
    df_foci_prop_norm_stndrztn_name = df_foci_name + '_prop-norm-stndrztn_standscaled_'+ time_dir
    df_foci_idr_prop_norm_stndrztn_name = df_foci_idr_name + '_prop-norm-stndrztn_standscaled_'+ time_dir
    df_train.to_csv(save_dir + '/' + df_foci_prop_norm_stndrztn_name +'.csv')
    df_test.to_csv(save_dir +  '/' + df_foci_idr_prop_norm_stndrztn_name +'.csv')

    print('Property normalization and standardization completed.')
    return df_train, df_test, df_foci_prop_norm_stndrztn_name, df_foci_idr_prop_norm_stndrztn_name

def histogram_plots_train_test(summary_dir):

    # ******************************************************************************
    # START : LOAD PROPERTIES
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    df_foci_v0 = df_foci.copy()

    iloc_rows = []

    df_foci = df_foci.loc[ (df_foci['status'] == 1) | (df_foci['status'] == 3) | (df_foci['status'] == 4) | (df_foci['status'] == 5)]
    df_foci_idr = df_foci_idr.loc[ (df_foci_idr['status'] == 1) | (df_foci_idr['status'] == 3) | (df_foci_idr['status'] == 4) | (df_foci_idr['status'] == 5)]

    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/' + '/Plots/Histograms/' + time_dir)):
        os.mkdir(base_dir + '/' + '/Plots/Histograms/' + time_dir)
    # END : LOAD PROPERTIES
    # ******************************************************************************

    # ******************************************************************************
    # all properties----- START
    props_conv = {}
    col_names = df_foci_v0.columns.values.tolist()
    col_names = [e for e in col_names if e not in ('Unnamed: 0', 'protein', 'exp_folder', 'roi_id', 'focus_label', 'status', 'centroid-0', 'centroid-1', 'coords', 'bbox', 'focus_max', 'focus_min')] # drop these cols
    print(col_names)
    fft_col_names = ['fft_orientation', 'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_eccentricity', 'fft_euler_number', 'fft_solidity', 'fft_compactness_pp','fft_compactness_s', 'fft_var_laplacian', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_sum', 'fft_std', 'fft_var', 'fft_max', 'fft_min']
    focus_col_names = [f for f in col_names if f not in ('fft_orientation', 'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_eccentricity', 'fft_euler_number', 'fft_solidity', 'fft_compactness_pp','fft_compactness_s', 'fft_var_laplacian', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_sum', 'fft_std', 'fft_var', 'fft_max', 'fft_min')] # drop these cols
    props_nums = [str(x) for x in range(len(col_names))]
    # change names to numbers for easier labeling
    for i in range(len(col_names)):
        props_conv[col_names[i]] =  props_nums[i] # dictionary with name keys
    props_nums_hist = tuple(props_nums)
    # index dictionaries to find numbers associated with names
    focus_col_nums = [props_conv[x] for x in focus_col_names]
    fft_col_nums = [props_conv[x] for x in fft_col_names]
    print(props_conv)

    # tuples
    focus_col_nums_t = tuple(focus_col_nums)
    fft_col_nums_t = tuple(fft_col_nums)

    # dictionaries with number keys
    inv_props_conv = {v: k for k, v in props_conv.items()}
    # all properties----- END

    # index names for sym and blur----- START
    # sym
    sym_props_focus = ['2','3','4','5','9','11','14','35','36','37','38','39','40','41','42','43','44','45','46','47','48','49']
    sym_props_fft = ['16', '17','18','19','23','24','26','27','28','29','33']
    sym_props = sym_props_focus + sym_props_fft
    sym_props_names = [inv_props_conv[x] for x in  sym_props]
    print(sym_props_names) #['eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'focus_med', 'compactness_pp', 'compactness_s', 'fft_mean', 'fft_med']
    sym_props_names = ['eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'focus_med', 'compactness_pp', 'compactness_s',
     'fft_mean', 'fft_med']

    # blur
    blur_props_focus = ['5', '9', '11', '43', '47', '48']
    blur_props_fft = ['28', '29']
    blur_props = blur_props_focus + blur_props_fft
    blur_props_names = [inv_props_conv[x] for x in  blur_props]
    print(blur_props_names) # ['perimeter', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'solidity', 'var_lap', 'var_lap_gb3', 'var_lap_gb5', 'var_lap_gb7', 'var_lap_gb9', 'blur_2', 'shannon_entropy', 'focus_mean', 'focus_med', 'focus_sum', 'focus_std', 'focus_var', 'compactness_pp', 'compactness_s', 'tot_pix_no', 'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_compactness_pp', 'fft_compactness_s', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_max']

    blur_props_names = ['perimeter', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'compare_val_bc', 'compare_val_hellngr',
     'solidity', 'var_lap', 'var_lap_gb3', 'var_lap_gb5', 'var_lap_gb7', 'var_lap_gb9', 'blur_2', 'shannon_entropy',
     'focus_mean', 'focus_med', 'focus_sum', 'focus_std', 'focus_var', 'compactness_pp', 'compactness_s', 'tot_pix_no',
     'fft_area', 'fft_perimeter', 'fft_major_axis_length', 'fft_minor_axis_length', 'fft_compactness_pp',
     'fft_compactness_s', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean', 'fft_med', 'fft_max']

    sym_props_names = ['eccentricity', 'compare_val_bc', 'compare_val_hellngr', 'focus_med', 'compactness_pp', 'compactness_s',
     'fft_mean', 'fft_med']

    #******************************************************************************
    # START: PLOT PROPERTIES HISTOGRAMS: FOR LOOP
    classes = df_foci['status'].unique()
    classes = np.array([5])
    classes = classes[~np.isnan(classes)] # drop nans
    classes.sort(axis=0)
    classes_l = ''.join(map(str, classes. astype(int).tolist()))
    # plot blur/sym props
    #focus_col_nums = ['5', '29'] #sym_props
    focus_col_nums_t = tuple(focus_col_names)
    fft_col_nums_t = tuple(fft_col_names)
    # user defined variables
    #df_foci = df_foci_idr
    props = df_foci[fft_col_names] # props_nums, focus_col_nums, fft_col_nums
    props_plt = 'fft_props' # all_props, corrltd_props focus_props, fft_props
    totpixno_norm = 'pixnorm' # 'nopixnorm': no normalization , 'pixnorm': properties normalized wrt to focus roi total pixel number
    avg_norm = 'avgnorm' # 'noavgnorm': no normalization , 'avgnorm': properties normalized wrt to prop means
    scaled = 'standscaled' # 'scaled': scaled after avg normalization  'noscaling': no scaling
    bins = 20
    norm = 1 # 1: apply normalization weights or 0: don't apply normalization weights
    len_props = len(props.columns)
    sq_len_props = int(sqrt(len_props))
    fignum = 1
    fig, axes = plt.subplots(sq_len_props, (sq_len_props + 1), figsize=(10, 10)) #sharey=True, figsize=(16, 16)
    # loop through all properties subplots
    for ii, ax in enumerate(axes.flat):
        print(ii)
        if (ii == len_props):
            break
        # set prop_nums var
        if (props_plt == 'focus_props'):
            prop_nums = focus_col_nums_t
        elif (props_plt == 'fft_props'):
            prop_nums = fft_col_nums_t

        prop = prop_nums[ii]

        # histogram for each class
        for i_class, cls in enumerate(classes):
            print(i_class)
            data_train = df_foci.loc[(df_foci['status'] == cls)][prop]
            data_test = df_foci_idr.loc[(df_foci_idr['status'] == cls)][prop]
            # normalization weights
            data_train_w = np.empty(data_train.shape)
            data_train_w.fill(1 / data_train.shape[0])
            data_test_w = np.empty(data_test.shape)
            data_test_w.fill(1 / data_test.shape[0])
            # normalization application
            if (norm == 1):
                ax.hist(data_train, weights=data_train_w, bins=bins, alpha=0.5, label= 'train') # n, bins_, patches =
                ax.hist(data_test, weights=data_test_w, bins=bins, alpha=0.5, label= 'test') # n, bins_, patches =
                norm_plt = 'norm'
            elif (norm == 0):
                ax.hist(data_train, bins = bins, alpha=0.5, label = 'train') # n, bins_, patches =
                ax.hist(data_test, bins = bins, alpha=0.5, label = 'test') # n, bins_, patches =
                norm_plt = 'noNorm'
            del data_train, data_train_w, data_test, data_test_w
        ax.set_title(prop)
        fignum = fignum + 1
        #plt.show()

    # delete unused axes
    for ax in axes.flat[len_props:]:
        ax.remove()
    # label and save
    #fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical') # shared y label
    plt.tight_layout()
    fig.legend(['train', 'test'], fancybox=True, loc='center left', bbox_to_anchor= (0.9, 0.5)) #
    #plt.show()
    plt.savefig(base_dir + '/' + '/Plots/Histograms/' + time_dir + '/hist_'+ props_plt +'_'+ norm_plt + '_'+ totpixno_norm +'_'+ avg_norm + '_' + scaled +'_' + classes_l +'_test_and_train_' + ml_classifier + '_' + time_dir +'.pdf')

    # END: PLOT TRAIN AND TEST HISTOGRAMS

    print('Done.')


    # END: PLOT PROPERTIES HISTOGRAMS: FOR LOOP
#******************************************************************************
#******************************************************************************
def histograms(df_foci, df_foci_idr, df_foci_name, df_foci_idr_name, props, blur_props, sym_props):

    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/Results/' + time_dir )):
        os.mkdir(base_dir + '/Results/' + time_dir )
    save_dir = base_dir + '/Results/' + time_dir + '/Histograms'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)

    # ******************************************************************************
    # START : LOAD PROPERTIES
    df_foci_v0 = df_foci.copy()

    #  save csvs used for record keeping
    df_props = pd.DataFrame(props)
    df_props.to_csv(save_dir + '/' + 'properties_' + time_dir + '.csv')
    df_sym_props = pd.DataFrame(sym_props)
    df_sym_props.to_csv(save_dir + '/' + 'symmetry-properties_' + time_dir + '.csv')
    df_blur_props = pd.DataFrame(blur_props)
    df_blur_props.to_csv(save_dir + '/' + 'blur_properties_' + time_dir + '.csv')
    df_foci.to_csv(save_dir + '/' + df_foci_name + '.csv')
    df_foci_idr.to_csv(save_dir + '/' + df_foci_idr_name + '.csv')

    iloc_rows = []

    # drop properties
    df_foci_v0 = df_foci_v0.drop(columns = df_foci_v0.columns.difference(props), axis=1).copy()

    # END : LOAD PROPERTIES
    # ******************************************************************************

    # ******************************************************************************
    # START: SELECT PROPERTIES, CREATE NUMERIC LABELS
    # get property names
    props_conv = {}
    col_names = df_foci_v0.columns.values.tolist()
    col_names = props
    fft_col_names = [col for col in df_foci_v0.columns if 'fft_' in col]
    focus_col_names = [f for f in col_names if f not in tuple(fft_col_names)]

    # convert property name labels to number labels
    props_nums = [str(x) for x in range(len(col_names))]
    for i in range(len(col_names)):
        props_conv[col_names[i]] =  props_nums[i] # dictionary with name keys
    # index dictionaries to find numbers associated with names
    focus_col_nums = [props_conv[x] for x in focus_col_names]
    fft_col_nums = [props_conv[x] for x in fft_col_names]
    sym_col_nums = [props_conv[x] for x in sym_props]
    blur_col_nums = [props_conv[x] for x in blur_props]

    print(props_conv)
    df_prop_conv = pd.DataFrame(props_conv.items())
    df_prop_conv.to_csv(save_dir + '/' + 'properties-converter_' + time_dir + '.csv')

    # convert property number labels to tuples
    props_nums_hist = tuple(props_nums)
    focus_col_nums_t = tuple(focus_col_nums)
    fft_col_nums_t = tuple(fft_col_nums)

    # dictionary with number keys
    inv_props_conv = {v: k for k, v in props_conv.items()}

    # END: SELECT PROPERTIES, CREATE NUMERIC LABELS
    #******************************************************************************
    #******************************************************************************
    # START: PLOT PROPERTIES HISTOGRAMS: FOR LOOP

    # user defined variables
    classes_full=  np.array([[1, 3], [3, 4], [1, 3 ,4 ,5]])
    classes_dirs = ['13','34','1345']
    classes_colors_full = np.array([['darkred', 'khaki'],['khaki','darkgreen'],['darkred','khaki', 'darkgreen', 'mediumpurple']])
    dataset_names = [df_foci_name, df_foci_idr_name]
    datasets = [df_foci, df_foci_idr]
    props_plt_full = [ 'all_props', 'focus_props', 'fft_props', 'sym_props', 'blur_props',
     'all_props_nums', 'focus_props_nums', 'fft_props_nums', 'sym_props_nums', 'blur_props_nums'] # for debugging,
    totpixno_norm = 'pixnorm' # 'nopixnorm': no normalization , 'pixnorm': properties normalized wrt to focus roi total pixel number
    avg_norm = 'avgnorm' # 'noavgnorm': no normalization , 'avgnorm': properties normalized wrt to prop means
    scaled = 'standscaled' # 'scaled': scaled after avg normalization  'noscaling': no scaling
    #bins = 20
    w = 0.05
    norm = 1 # 1: apply normalization weights or 0: don't apply normalization weights

    # plot group of classes
    for idx_class_ar, i_class_ar in enumerate(classes_full):
        if (not os.path.isdir(save_dir +'/' + classes_dirs[idx_class_ar])):
            os.mkdir(save_dir +'/' + classes_dirs[idx_class_ar])
        print('class combination: ' + str(i_class_ar))
        classes_l = ''.join(map(str, np.array(i_class_ar).astype(int).tolist()))
        colors_i = classes_colors_full[idx_class_ar]
        i_legend = 0
        s_legend = ''
        # plot train and test sets
        for idx_dataset, i_dataset in enumerate(datasets):
            print('dataset: ' + str(idx_dataset))
            # plot all, focus, and fft properties
            for i_idx, i_props in enumerate(props_plt_full):
                plt.close()
                print(i_props)
                i_dataset_nums = i_dataset.copy()

                # select properties to plot
                if (i_props == 'all_props'):
                    prop_names = tuple(col_names)
                elif (i_props == 'focus_props'):
                    prop_names = tuple(focus_col_names)
                elif (i_props == 'fft_props'):
                    prop_names = tuple(fft_col_names)
                elif (i_props == 'blur_props'):
                    prop_names = tuple(blur_props)
                elif (i_props == 'sym_props'):
                    prop_names = tuple(sym_props)

                elif (i_props == 'all_props_nums'):
                    i_dataset_nums.rename(columns=props_conv, inplace=True)
                    i_dataset = i_dataset_nums
                    prop_names = tuple(props_nums)
                elif (i_props == 'focus_props_nums'):
                    prop_names = tuple(focus_col_nums)
                elif (i_props == 'fft_props_nums'):
                    prop_names = tuple(fft_col_nums)
                elif (i_props == 'blur_props_nums'):
                    prop_names = tuple(blur_col_nums)
                elif (i_props == 'sym_props_nums'):
                    prop_names = tuple(sym_col_nums)

                print(prop_names)
                # set plot dimensions
                fignum = 1
                len_props = len(prop_names)
                sq_len_props = int(round(sqrt(len_props)))
                fig, axes = plt.subplots((sq_len_props + 1), (sq_len_props + 2), figsize=(10, 10)) #sharey=True, figsize=(16, 16)
                # loop through all properties subplots
                for ii, ax in enumerate(axes.flat):
                    print('ax id: ' + str(ii))
                    if (ii == len_props):
                        break
                    prop = prop_names[ii]
                    # histogram for each group of classes
                    for idx_class, i_class in enumerate(i_class_ar):
                        print('prop name: ' + str(prop))
                        print('class: ' + str(i_class))
                        print(type(i_dataset))
                        print(i_dataset.loc[(i_dataset['status'] == i_class)])
                        dataset = i_dataset.loc[(i_dataset['status'] == i_class)][prop]
                        # normalization weights
                        data_w = np.empty(dataset.shape)
                        print('dataset_shape: ' + str(dataset.shape[0]))
                        data_w.fill(1 / dataset.shape[0])

                        if (i_legend == 0):
                            s_legend += str(i_class) + colors_i[idx_class]

                        # normalization application
                        if (norm == 1):
                            bins= np.arange(min(dataset), max(dataset) + w, w)
                            ax.hist(dataset, weights=data_w ,bins= bins, alpha=0.5, label= str(i_class), color = colors_i[idx_class])
                            norm_plt = 'norm'
                        elif (norm == 0):
                            ax.hist(dataset, bins = bins, alpha=0.5, label = str(i_class))
                            norm_plt = 'noNorm'
                        del dataset, data_w
                    i_legend = 1
                    ax.set_title(prop)
                    fignum = fignum + 1

                # delete unused axes
                for ax in axes.flat[len_props:]:
                    ax.remove()
                # label and save
                #fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical') # shared y label
                plt.tight_layout()
                #fig.legend(i_class_ar, fancybox=True, loc='center left', bbox_to_anchor= (0.9, 0.5)) #
                plt.savefig(save_dir +'/' + classes_dirs[idx_class_ar] +'/foci_props_hist_' + i_props +'_data-'+ dataset_names[idx_dataset] + '_' + norm_plt + '_' +'_classes' + classes_l +'__legend-' + s_legend + '_' + time_dir + '.pdf')


    # END: PLOT HISTOGRAMS

    # END: PLOT PROPERTIES HISTOGRAMS: FOR LOOP
    print('Histograms completed.')
#******************************************************************************

#******************************************************************************

def deepfoci_histograms(summary_dir, dir_list):
    df_deepfoci_data = pd.read_csv(base_dir + '/df_tumor_data_slices.csv')
    df_tissue = df_deepfoci_data[['tissue','spool_no']]
    df_foci_df = pd.read_csv(base_dir + '/foci_info_df.csv')
    df_foci_df = df_foci_df.loc[
        (df_foci_df['status'] == 1) | (df_foci_df['status'] == 3) | (df_foci_df['status'] == 4) | (df_foci_df['status'] == 5)]
    df_foci_df = df_foci_df.dropna()
    df_foci_df = df_foci_df.reset_index(drop = True)


    roi_ids_raw = df_foci_df['roi_id']
    roi_ids = []
    for roi_id in roi_ids_raw:
        roi_id_split = roi_id.split('_')
        roi_ids.append(roi_id_split[0] + '_' + roi_id_split[1])
    roi_ids = np.array(roi_ids)
    roi_ids_unique = np.unique(roi_ids)

    tissues_ = []
    spool_ids_ = []
    for i, row in df_foci_df.iterrows():
        roi_id_raw = row['roi_id'].split('_')
        roi_id = roi_id_raw[0] + '_' + roi_id_raw[1]
        t = df_tissue['spool_no'].eq(roi_id)
        t_idx = np.where(t)[0]
        tissues_.append(df_tissue['tissue'][t_idx].tolist())
        spool_ids_.append(df_tissue['spool_no'][t_idx].tolist())
    tissues_l = list(itertools.chain.from_iterable(tissues_))
    spool_ids_l = list(itertools.chain.from_iterable(spool_ids_))
    df_foci_df['tissue'] = tissues_l
    df_foci_df['spool_no'] = spool_ids_l

    # replace 'tumor ' instances with 'tumor'
    tissues = df_foci_df['tissue'].unique()
    tiss = df_foci_df['tissue'].eq(tissues[2])
    tiss_idx = np.where(tiss)[0]
    df_foci_df['tissue'][tiss_idx] = tissues[0]

    # focus percentages
    #tumor
    df_foci_df_tumor = df_foci_df.loc[(df_foci_df['tissue'] == tissues[0])]
    foci_tot_tumor = len(df_foci_df_tumor)
    focus_tumor_1 = len(df_foci_df_tumor.loc[(df_foci_df_tumor['status'] == 1)])
    focus_tumor_3 = len(df_foci_df_tumor.loc[(df_foci_df_tumor['status'] == 3)])
    focus_tumor_4 = len(df_foci_df_tumor.loc[(df_foci_df_tumor['status'] == 4)])
    focus_tumor_5 = len(df_foci_df_tumor.loc[(df_foci_df_tumor['status'] == 5)])
    focus_tumor_1_per = focus_tumor_1/foci_tot_tumor
    focus_tumor_3_per = focus_tumor_3 / foci_tot_tumor
    focus_tumor_4_per = focus_tumor_4 / foci_tot_tumor
    focus_tumor_5_per = focus_tumor_5 / foci_tot_tumor

    # adjacent to tumor
    df_foci_df_attumor = df_foci_df.loc[(df_foci_df['tissue'] == tissues[1])]
    foci_tot_attumor = len(df_foci_df_attumor)
    focus_attumor_1 = len(df_foci_df_attumor.loc[(df_foci_df_attumor['status'] == 1)])
    focus_attumor_3 = len(df_foci_df_attumor.loc[(df_foci_df_attumor['status'] == 3)])
    focus_attumor_4 = len(df_foci_df_attumor.loc[(df_foci_df_attumor['status'] == 4)])
    focus_attumor_5 = len(df_foci_df_attumor.loc[(df_foci_df_attumor['status'] == 5)])
    focus_attumor_1_per = focus_attumor_1/foci_tot_attumor
    focus_attumor_3_per = focus_attumor_3 / foci_tot_attumor
    focus_attumor_4_per = focus_attumor_4 / foci_tot_attumor
    focus_attumor_5_per = focus_attumor_5 / foci_tot_attumor

    sns.histplot(binwidth=0.5, x="status", hue="tissue", data=df_foci_df, stat="percent")
    plt.show()
    stop = 1


def classifier_optimization(summary_dir, dir_list):

    #******************************************************************************
    # START : LOAD PROPERTIES
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    #pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci_og = df_foci

    df_foci_idr_clsfrs0 = df_foci_idr.loc[(df_foci_idr['status'] == 0)]
    df_foci_og_clsfrs = df_foci.loc[(df_foci['status'] == 3)]


    iloc_rows = []
    df_foci_og_subset = df_foci_og.iloc[iloc_rows,:]

    # END : LOAD PROPERTIES
    #******************************************************************************

    df_foci_simple = df_foci_og.copy()
    #df_foci_simple = df_foci_resize.copy()
    df_foci_simple_idr = df_foci_idr.copy()

    '''
    # CLASSIFICATION -- OPTIMIZATION
    # C: regularization parameter
    # gamma:
    #  'gamma': ['scale',
    param_grid = [{'C': [0.01, 0.1, 1, 10, 100, 1000],
                   'gamma': ['scale', 100, 10, 1, 0.1, 0.01, 0.001, 0.0001],
                  'kernel': ['linear','poly','rbf']}, ] # 'gamma': ['scale', 100, 10, 1, 0.1, 0.01, 0.001, 0.0001],  C': [0.01, 0.1, 1, 10, 100, 1000]


    grid = GridSearchCV(SVC(), param_grid,
                                cv = 5,
                                scoring= 'accuracy',
                                verbose = 3)
    grid.fit(X_train_scaled, Y_train)
    print(grid.best_params_)
    print(grid.best_estimator_)
    grid_predictions = grid.predict(X_test)
    print(classification_report(Y_test, grid_predictions))
    '''

    '''
    # rf hyperparameter optimization 
 # estimators 200,500
    param_grid = {
        'n_estimators': [10, 50],
        'max_features': ['auto', 'sqrt', 'log2'],
        'max_depth': [4, 5, 6, 7, 8],
        'criterion': ['gini', 'entropy']
    }

    CV_rfc = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5)
    CV_rfc.fit(X_train_scaled, Y_train)
    best_params = CV_rfc.best_params_
    '''
    '''
    # optimize parameters
    # C: regularization parameter
    # gamma:
    #  'gamma': ['scale',
    param_grid = [{'C': [0.01, 0.1, 1, 10, 100, 1000],
                   'gamma': ['scale', 100, 10, 1, 0.1, 0.01, 0.001, 0.0001],
                  'kernel': ['linear','poly','rbf']}, ] # 'gamma': ['scale', 100, 10, 1, 0.1, 0.01, 0.001, 0.0001],  C': [0.01, 0.1, 1, 10, 100, 1000]

    grid = GridSearchCV(SVC(), param_grid,
                                cv = 5,
                                scoring= 'accuracy',
                                verbose = 3)
    grid.fit(X_train_scaled, Y_train)
    print(grid.best_params_)
    print(grid.best_estimator_)
    grid_predictions = grid.predict(X_test)
    print(classification_report(Y_test, grid_predictions))
    '''

    '''
    # best: metric= manhattan, n_neighbors= 3, weights= uniform, 0.62
    #knn hyperparameter optimization
    KNN = sklearn.neighbors.KNeighborsClassifier()
    param_grid = {
        'n_neighbors': [3, 11, 15, 21, 25],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    }

    CV_knnc = GridSearchCV(estimator = KNN, param_grid = param_grid, cv = 5, verbose = 1)
    gs_results = CV_knnc.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    best_params = CV_knnc.best_params_
    gs_results.best_estimators_
    gs_results.best_score_
    '''
    '''
    df_foci = df_foci.loc[(df_foci['status'] == 1) | (df_foci['status'] == 3)]
    props = ('var_lap', 'var_lap_gb3', 'var_lap_gb5', 'var_lap_gb7', 'var_lap_gb9')
    # for comparison
    data_0_1 = df_foci.loc[(df_foci['status'] == 1)]['var_lap_gb3']
    data_0_3 = df_foci.loc[(df_foci['status'] == 3)]['var_lap_gb3']
    data_w_0_1 = np.empty(data_0_1.shape)
    data_w_0_1.fill(1 / data_0_1.shape[0])
    data_w_0_3 = np.empty(data_0_3.shape)
    data_w_0_3.fill(1 / data_0_3.shape[0])
    n, bins, patches = plt.hist([data_0_1, data_0_3], weights=[data_w_0_1, data_w_0_3], bins=bins, alpha=0.5, label=[str(1), str(3)])
    plt.savefig(base_dir + '/foci_props_plt_gb3.pdf')
    plt.close()

    bins = 20
    data_1_1 = df_foci.loc[(df_foci['status'] == 1)]['var_lap_gb5']
    data_1_3 = df_foci.loc[(df_foci['status'] == 3)]['var_lap_gb5']
    data_w_1_1 = np.empty(data_1_1.shape)
    data_w_1_1.fill(1 / data_1_1.shape[0])
    data_w_1_3 = np.empty(data_1_3.shape)
    data_w_1_3.fill(1 / data_1_3.shape[0])
    n, bins, patches = plt.hist([data_1_1, data_1_3], weights=[data_w_1_1, data_w_1_3], bins=bins, alpha=0.5, label=[str(1), str(3)])
    plt.savefig(base_dir + '/foci_props_plt_gb5.pdf')
    plt.close()

    bins = 20
    data_2_1 = df_foci.loc[(df_foci['status'] == 1)]['var_lap_gb7']
    data_2_3 = df_foci.loc[(df_foci['status'] == 3)]['var_lap_gb7']
    data_w_2_1 = np.empty(data_2_1.shape)
    data_w_2_1.fill(1 / data_2_1.shape[0])
    data_w_2_3 = np.empty(data_2_3.shape)
    data_w_2_3.fill(1 / data_2_3.shape[0])
    n, bins, patches = plt.hist([data_2_1, data_2_3], weights=[data_w_2_1, data_w_2_3], bins=bins, alpha=0.5, label=[str(1), str(3)])
    plt.savefig(base_dir + '/foci_props_plt_gb7.pdf')
    plt.close()

    bins = 20
    data_3_1 = df_foci.loc[(df_foci['status'] == 1)]['var_lap_gb9']
    data_3_3 = df_foci.loc[(df_foci['status'] == 3)]['var_lap_gb9']
    data_w_3_1 = np.empty(data_3_1.shape)
    data_w_3_1.fill(1 / data_3_1.shape[0])
    data_w_3_3 = np.empty(data_3_3.shape)
    data_w_3_3.fill(1 / data_3_3.shape[0])
    n, bins, patches = plt.hist([data_3_1, data_3_3], weights=[data_w_3_1, data_w_3_3], bins=bins, alpha=0.5, label=[str(1), str(3)])
    plt.savefig(base_dir + '/foci_props_plt_gb9.pdf')
    plt.close()
    '''

def focus_tables(summary_dir, dir_list):
    # load properties
    #pd.read_csv(base_dir + '/foci_info_20220128.csv')
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    df_foci_df = pd.read_csv(base_dir + '/foci_info_df.csv')

    # create table of total number classified foci for BP1-2 and 53BP1 datasets
    fig, ax = plt.subplots(figsize=(13, 3))
    fig.patch.set_visible(False)
    ax.axis('off')
    ax.axis('tight')
    classes = [3,4,5,1]
    foci_num_BP12 = []
    foci_num_53BP1 = []
    foci_num_H2AX = []
    for i_class in classes:
        foci_num_BP12.append(len(df_foci.loc[(df_foci['status'] == i_class)]))
        foci_num_53BP1.append(len(df_foci_idr.loc[(df_foci_idr['status'] == i_class)]))
        foci_num_H2AX.append(len(df_foci_df.loc[(df_foci_df['status'] == i_class)]))

    foci_num_BP12.append(sum(foci_num_BP12))
    foci_num_53BP1.append(sum(foci_num_53BP1))
    foci_num_H2AX.append(sum(foci_num_H2AX))
    df = pd.DataFrame(foci_num_BP12, columns= ['BP1-2'])
    df['53BP1'] = foci_num_53BP1
    #df['H2AX'] = foci_num_H2AX
    df = df.T
    df.columns = ['Symmetric','Asymmetric','Midway','Blur', 'Total']
    table = ax.table(cellText=df.values, colLabels=df.columns, rowLabels = ['BP1-2', '53BP1'], loc='center', cellLoc='center') # ['BP1-2', '53BP1', 'H2AX']
    table.set_fontsize(20)
    table.scale(1.5, 1.5)  # may help
    #plt.show()
    # save table to pdf
    pp = PdfPages("Table1_TotalFocusNumbers_20_2.pdf")
    pp.savefig(fig, bbox_inches='tight')
    pp.close()
    print('Done.')

def tables(df_foci, df_foci_idr):
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/Results/' + time_dir )):
        os.mkdir(base_dir + '/Results/' + time_dir )
    save_dir = base_dir + '/Results/' + time_dir + '/Tables'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)

    # save inputs
    df_foci.to_csv(save_dir + '/tables_train_input_'+ time_dir +'.csv')
    df_foci_idr.to_csv(save_dir + '/tables_test_input_'+ time_dir +'.csv')

    # calculate total number of nuclei
    nucleus_num_train = df_foci['roi_id'].unique()
    print('Total number of nuclei in train set: ' + str(len(nucleus_num_train)))
    nucleus_num_test = df_foci_idr['roi_id'].unique()
    print('Total number of nuclei in test set: ' + str(len(nucleus_num_test)))

    df_nuclei = pd.DataFrame([len(nucleus_num_train)], columns= ['train'])
    df_nuclei['test'] = [len(nucleus_num_test)]
    df_nuclei.to_csv(save_dir + '/nuclei_numbers_'+ time_dir +'.csv')

    # create table of total number classified foci for train and test datasets
    fig, ax = plt.subplots(figsize=(13, 3))
    fig.patch.set_visible(False)
    ax.axis('off')
    ax.axis('tight')
    classes = [3,4,5,1]
    foci_num_BP12 = []
    foci_num_53BP1 = []
    for i_class in classes:
        foci_num_BP12.append(len(df_foci.loc[(df_foci['status'] == i_class)]))
        foci_num_53BP1.append(len(df_foci_idr.loc[(df_foci_idr['status'] == i_class)]))

    foci_num_BP12.append(sum(foci_num_BP12))
    foci_num_53BP1.append(sum(foci_num_53BP1))
    df = pd.DataFrame(foci_num_BP12, columns= ['BP1-2'])
    df['53BP1'] = foci_num_53BP1
    #df['H2AX'] = foci_num_H2AX
    df = df.T
    df.columns = ['Symmetric','Asymmetric','Midway','Blur', 'Total']
    table = ax.table(cellText=df.values, colLabels=df.columns, rowLabels = ['train', 'test'], loc='center', cellLoc='center') # ['BP1-2', '53BP1', 'H2AX']
    table.set_fontsize(20)
    table.scale(1.5, 1.5)  # may help
    #plt.show()
    # save table to pdf
    pp = PdfPages(save_dir + '/Table1_TotalFocusNumbers_' + time_dir +'.pdf')
    pp.savefig(fig, bbox_inches='tight')
    pp.close()

    print('Tables completed.')

def U_MAP_train_test(summary_dir):

    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/Plots/UMAP/' + time_dir)):
        os.mkdir(base_dir + '/Plots/UMAP/' + time_dir)

    #******************************************************************************
    # START : LOAD PROPERTIES
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    props = datasets[4]

    #pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci_og = df_foci

    iloc_rows = []

    df_foci_simple = df_foci_og.copy()
    df_foci_simple_idr = df_foci_idr.copy()

    # END : LOAD PROPERTIES
    #******************************************************************************

    #******************************************************************************

    # START: ORGANIZE DATA
    # initialize evaliation metric lists
    classifr_auc = []
    classifr_accuracy = []
    classifr_auc_idr = []
    classifr_accuracy_idr = []

    df_foci_simple = df_foci_simple.drop('focus_min', axis=1).copy()
    df_foci_simple_idr = df_foci_simple_idr.drop('focus_min', axis=1).copy()

    # simplify datasets, select focus classes of interest
    # train
    df_foci_simple = df_foci_simple.dropna()
    df_foci_simple_clsfrs = df_foci_simple.loc[ (df_foci_simple['status'] == 1) | (
                    df_foci_simple['status'] == 3) | (df_foci_simple['status'] == 4) | (df_foci_simple['status'] == 5)]

    # test
    df_foci_simple_idr = df_foci_simple_idr.dropna()
    df_foci_simple_idr = df_foci_simple_idr.loc[ (df_foci_simple_idr['status'] == 1) | (
                    df_foci_simple_idr['status'] == 3) | (df_foci_simple_idr['status'] == 4) | (df_foci_simple_idr['status'] == 5)]
    df_foci_simple_idr = df_foci_simple_idr.reset_index(drop = True)

    # define dropped properties
    dropped_props = df_foci_simple_clsfrs.columns.difference(props)
    dropped_props_idr = df_foci_simple_idr.columns.difference(props)

    # drop dropped properties
    # X raw
    X_raw_clsfrs = df_foci_simple_clsfrs.drop(dropped_props, axis=1).copy()
    X_raw_clsfrs_idr = df_foci_simple_idr.drop(dropped_props_idr, axis=1).copy()

    # Y raw
    Y_raw_clsfrs = df_foci_simple_clsfrs['status'].copy()
    Y_raw_clsfrs_idr = df_foci_simple_idr['status'].copy()

    classes = [1, 3, 4, 5]
    # END: ORGANIZE DATA
    #******************************************************************************

    #******************************************************************************
    # START: UMAP
    dataset = 'train_v1'
    classes = '1345'

    umap_2d = UMAP(n_components=2, init='random', random_state=0)
    proj_2d = umap_2d.fit_transform(X_raw_clsfrs)
    fig_2d = px.scatter(proj_2d, x=0, y=1, color=Y_raw_clsfrs.astype(str), labels=Y_raw_clsfrs_idr.astype(str))
    fig_2d.show()
    plt.close()

    # train v1
    mapper = umap.UMAP().fit(X_raw_clsfrs)
    umap.plot.points(mapper, labels=Y_raw_clsfrs.astype(str))
    plt.show()
    plt.savefig(base_dir + '/Plots/UMAP/' + time_dir + '/UMAP_' + 'train_v1' + '_classes_'+ classes + '_' + time_dir + '.pdf',
        dpi=300, bbox_inches='tight')
    plt.close()

    # test v1
    mapper = umap.UMAP().fit(X_raw_clsfrs_idr)
    umap.plot.points(mapper, labels=Y_raw_clsfrs_idr.astype(str))
    plt.savefig(
        base_dir + '/Plots/UMAP/' + time_dir + '/UMAP_' + 'test_v1' + '_classes_' + classes + '_' + time_dir + '.pdf',
        dpi=300, bbox_inches='tight')
    plt.close()

    umap_3d = UMAP(n_components=3, init='random', random_state=0)
    proj_3d = umap_3d.fit_transform(X_raw_clsfrs)
    fig_3d = px.scatter_3d(proj_3d, x=0, y=1, z=2,
        color=Y_raw_clsfrs.astype(str), labels=Y_raw_clsfrs.astype(str))
    fig_3d.update_traces(marker_size=5)
    fig_3d.show()
    plt.close()

    # optimization
    dataset = 'train'
    n_neighbors = (5, 10, 20, 50, 100, 200) # 5
    min_dist = (0.25, 0.5, 0.8, 0.99) #0.1
    n_components = 2
    metric = ('braycurtis', 'correlation','minkowski','canberra') #'euclidean', 'cosine', ,'chebyshev'
    for n_neighbors_i in n_neighbors:
        for min_dist_i in min_dist:
            for metric_i in metric:
                mapper = umap.UMAP(
                        n_neighbors = n_neighbors_i,
                        min_dist = min_dist_i,
                        n_components = n_components,
                        metric=metric_i).fit(X_raw_clsfrs)
                umap.plot.points(mapper, labels= Y_raw_clsfrs.astype(str))
                plt.savefig(base_dir + '/Plots/UMAP/' + time_dir + '/UMAP_nnghbrs-' + str(n_neighbors_i) +'_mindist-'+ str(min_dist_i)
                            +'_ncmpnts-' + str(n_components) + '_metric-'+ str(metric_i) + '_' + dataset + '_' + time_dir + '.pdf', dpi=300, bbox_inches='tight')
                plt.close()
                del mapper

    stop = 1
    print('Done.')
    # END: UMAP
    #******************************************************************************

def load_properties():

    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)

    save_dir_ = base_dir + '/Results/' + time_dir
    if (not os.path.isdir(save_dir_)):
        os.mkdir(save_dir_)

    save_dir = base_dir + '/Results/' + time_dir + '/CSV_files'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)

    df_foci_name = 'foci_info_v3.csv'
    df_foci_name_idr = 'foci_info_idr_full_v3.csv'
    df_foci = pd.read_csv(base_dir + '/Results/CSV_files/' + df_foci_name)
    df_foci_idr = pd.read_csv(base_dir + '/Results/CSV_files/' + df_foci_name_idr)

    df_train_full = df_foci.copy()
    df_test_full = df_foci_idr.copy()

    # define properties
    props = ['orientation', 'area',
       'perimeter', 'major_axis_length', 'minor_axis_length', 'eccentricity',
       'compare_val_correl', 'compare_val_chisq',
       'compare_val_intrsct', 'compare_val_bc', 'compare_val_chisqalt',
       'compare_val_hellngr', 'compare_val_kl', 'euler_number', 'solidity',
       'fft_orientation', 'fft_area', 'fft_perimeter', 'fft_major_axis_length',
       'fft_minor_axis_length', 'fft_eccentricity', 'fft_euler_number',
       'fft_solidity', 'fft_compactness_pp', 'fft_compactness_s',
       'fft_var_laplacian', 'fft_blur_2', 'fft_shannon_entropy', 'fft_mean',
       'fft_med', 'fft_sum', 'fft_std', 'fft_var', 'fft_max', 'fft_min',
       'var_lap', 'var_lap_gb3', 'var_lap_gb5', 'var_lap_gb7', 'var_lap_gb9',
       'blur_2', 'shannon_entropy', 'focus_mean', 'focus_med', 'focus_sum',
       'focus_std', 'focus_var', 'focus_max', 'focus_min', 'compactness_pp',
       'compactness_s', 'tot_pix_no']

    props.remove('focus_min') # nans
    props.remove('focus_max')

    props_ids = props.copy()
    props_ids.append('exp_folder')
    props_ids.append('roi_id')
    props_ids.append('status')

    # define Y
    Y = df_foci['status']
    Y_idr = df_foci_idr['status']

    # drop properties
    X = df_foci.drop(columns = df_foci.columns.difference(props), axis=1).copy()
    X_idr = df_foci_idr.drop(columns = df_foci_idr.columns.difference(props), axis=1).copy()

    df_train_full = df_train_full.drop(columns = df_train_full.columns.difference(props_ids), axis=1)
    df_test_full = df_test_full.drop(columns = df_test_full.columns.difference(props_ids), axis=1)
    df_test_full.to_csv(save_dir + '/0_full_' + df_foci_name_idr)

    # create new df with properties of interest
    df_train = X
    df_train['status'] = Y

    df_test = X_idr
    df_test['status'] = Y_idr

    # drop nas and reset index
    df_train = df_train.dropna().reset_index(drop = True)
    df_test = df_test.dropna().reset_index(drop = True)

    df_train_full = df_train_full.dropna().reset_index(drop = True)
    df_test_full = df_test_full.dropna().reset_index(drop = True)

    # select focus classes
    df_train = df_train.loc[ (df_train['status'] == 1) | (df_train['status'] == 3) | (df_train['status'] == 4) | (df_train['status'] == 5)]
    df_test = df_test.loc[ (df_test['status'] == 1) | (df_test['status'] == 3) | (df_test['status'] == 4) | (df_test['status'] == 5)]

    df_train_full = df_train_full.loc[ (df_train_full['status'] == 1) | (df_train_full['status'] == 3) | (df_train_full['status'] == 4) | (df_train_full['status'] == 5)]
    df_test_full = df_test_full.loc[ (df_test_full['status'] == 1) | (df_test_full['status'] == 3) | (df_test_full['status'] == 4) | (df_test_full['status'] == 5)]

    # reset index
    df_train = df_train.reset_index(drop = True)
    df_test = df_test.reset_index(drop = True)

    df_train_full = df_train_full.reset_index(drop = True)
    df_test_full = df_test_full.reset_index(drop = True)

    # metrics for metric dependancies
    blur_props = ['area', 'perimeter', 'major_axis_length', 'minor_axis_length', 'fft_shannon_entropy',
    'fft_sum']
    symmetry_props = ['area', 'perimeter', 'major_axis_length', 'minor_axis_length', 'solidity', 'shannon_entropy',
     'fft_shannon_entropy']

    # save
    df_train_full.to_csv(save_dir + '/full_' + df_foci_name)
    df_test_full.to_csv(save_dir + '/full_' + df_foci_name_idr)

    df_train.to_csv(save_dir + '/' + df_foci_name)
    df_test.to_csv(save_dir + '/' + df_foci_name_idr)

    df_props = pd.DataFrame(props)
    df_props.to_csv(save_dir + '/' + 'properties_' + time_dir + '.csv')

    df_blur_props = pd.DataFrame(blur_props)
    df_blur_props.to_csv(save_dir + '/' + 'blur_properties_' + time_dir + '.csv')

    df_sym_props = pd.DataFrame(symmetry_props)
    df_sym_props.to_csv(save_dir + '/' + 'symmetry_properties_' + time_dir + '.csv')

    return df_train, df_test, df_foci_name, df_foci_name_idr, props, blur_props, symmetry_props, df_train_full, df_test_full

def binary_problems(df_foci, df_foci_idr, df_foci_name, df_foci_idr_name, props):
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    save_dir = base_dir + '/Results/' + time_dir + '/BinaryProblems'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)
    #******************************************************************************
    # START : LOAD PROPERTIES
    df_foci_og = df_foci
    iloc_rows = []
    df_foci_simple = df_foci_og.copy()
    df_foci_simple_idr = df_foci_idr.copy()

    # save inputs
    df_foci.to_csv(save_dir + '/binary-problems_train_input_'+ time_dir +'.csv')
    df_foci_idr.to_csv(save_dir + '/binary-problems_test_input_'+ time_dir +'.csv')
    df_props = pd.DataFrame(props)
    df_props.to_csv(save_dir + '/binary-problems_props_input_'+ time_dir +'.csv')

    # END : LOAD PROPERTIES
    #******************************************************************************

    # ******************************************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST
    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA
    # initialize evaliation metric lists
    classifr_auc = []
    classifr_accuracy = []
    classifr_auc_idr = []
    classifr_accuracy_idr = []

    # organize data for classification
    # Y raw
    Y_raw_clsfrs = df_foci['status'].copy()
    Y_raw_clsfrs_idr = df_foci_idr['status'].copy()

    # X raw
    X_raw_clsfrs = df_foci.drop(columns = 'status', axis=1).copy()
    X_raw_clsfrs_idr = df_foci_idr.drop(columns = 'status', axis=1).copy()

    # binarize the output
    classes = [1, 3, 4, 5]
    lb = preprocessing.LabelBinarizer()
    no_classes = len(classes)
    # train
    Y_raw_clsfrs = lb.fit_transform(Y_raw_clsfrs)
    n_classes_clsfrs = Y_raw_clsfrs.shape[1]
    # test
    Y_raw_clsfrs_idr = lb.fit_transform(Y_raw_clsfrs_idr)
    n_classes_clsfrs_idr = Y_raw_clsfrs_idr.shape[1]

    # for train set: split into train and test sets
    X_train_clsfrs, X_test_clsfrs, Y_train_clsfrs, Y_test_clsfrs = train_test_split(X_raw_clsfrs, Y_raw_clsfrs,
                                                                                    test_size=0.3,
                                                                                    random_state=42)  # random_state=0
    # inverse binarization
    Y_train_clsfrs_nobin = lb.inverse_transform(Y_train_clsfrs)
    Y_raw_clsfrs_idr_nobin = lb.inverse_transform(Y_raw_clsfrs_idr)

    # scale data to speed up classification
    X_train_scaled_clsfrs = X_train_clsfrs #scale(X_train_clsfrs)
    X_test_scaled_clsfrs = X_test_clsfrs#scale(X_test_clsfrs)
    X_scaled_clsfrs_idr = X_raw_clsfrs_idr #scale(X_raw_clsfrs_idr)

    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA
    # ******************************************************

    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- DEFINE AND TRAIN CLASSIFIERS
    start_time_ovr_class = time.time()
    # SVM:
    # OneVsRest
    # learn to predict each class against the other
    classifier_svm = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True, class_weight = 'balanced'))
    classifier_svm_fit = classifier_svm.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_score_svm = classifier_svm_fit.decision_function(X_test_scaled_clsfrs)
    y_score_svm_idr = classifier_svm_fit.decision_function(X_scaled_clsfrs_idr)
    Y_pred_svm = classifier_svm.predict(X_test_scaled_clsfrs)
    Y_pred_svm_idr = classifier_svm_fit.predict(X_scaled_clsfrs_idr)

    Y_pred_svm_nobin = lb.inverse_transform(Y_pred_svm)
    Y_pred_svm_idr_nobin = lb.inverse_transform(Y_pred_svm_idr)

    # Compute ROC curve and ROC area for each class
    fpr_svm = dict()
    tpr_svm = dict()
    roc_auc_svm = dict()
    fpr_svm_idr = dict()
    tpr_svm_idr = dict()
    roc_auc_svm_idr = dict()

    for i in range(n_classes_clsfrs):
        fpr_svm[i], tpr_svm[i], _ = roc_curve(Y_test_clsfrs[:, i], y_score_svm[:, i])
        roc_auc_svm[i] = metrics.auc(fpr_svm[i], tpr_svm[i])
    for i_idr in range(n_classes_clsfrs_idr):
        fpr_svm_idr[i_idr], tpr_svm_idr[i_idr], _ = roc_curve(Y_raw_clsfrs_idr[:, i_idr], y_score_svm_idr[:, i_idr])
        roc_auc_svm_idr[i_idr] = metrics.auc(fpr_svm_idr[i_idr], tpr_svm_idr[i_idr])

    classifr_auc.append(roc_auc_svm)
    classifr_auc_idr.append(roc_auc_svm_idr)

    # Multiclass
    f1_multiclass = []
    precision_multiclass = []
    recall_multiclass = []
    model_multiclass = []
    classifier_svm_multiclass = svm.SVC(kernel='rbf', probability=True, class_weight = 'balanced')
    classifier_svm_fit_multiclass = classifier_svm_multiclass.fit(X_train_scaled_clsfrs, Y_train_clsfrs_nobin)
    y_score_svm_multiclass = classifier_svm_fit_multiclass.decision_function(X_test_scaled_clsfrs)
    y_score_svm_idr_multiclass = classifier_svm_fit_multiclass.decision_function(X_scaled_clsfrs_idr)
    Y_pred_svm_multiclass = classifier_svm_multiclass.predict(X_test_scaled_clsfrs)
    Y_pred_svm_idr_multiclass = classifier_svm_fit_multiclass.predict(X_scaled_clsfrs_idr)
    # Multiclass evaluation metrics
    f1_multiclass.append(f1_score(Y_raw_clsfrs_idr_nobin, Y_pred_svm_idr_multiclass, average='weighted'))
    precision_multiclass.append(precision_score(Y_raw_clsfrs_idr_nobin, Y_pred_svm_idr_multiclass, average='weighted'))
    recall_multiclass.append(recall_score(Y_raw_clsfrs_idr_nobin, Y_pred_svm_idr_multiclass, average='weighted'))
    model_multiclass.append('SVM')

    # SVM cross validation
    start_time_ovr_class_svm = time.time()
    scores_svm = cross_val_score(classifier_svm, X_train_scaled_clsfrs, Y_train_clsfrs, cv=10)
    scores_svm_idr = cross_val_score(classifier_svm, X_scaled_clsfrs_idr, Y_raw_clsfrs_idr, cv=10)
    classifr_accuracy.append(scores_svm.mean())
    classifr_accuracy_idr.append(scores_svm_idr.mean())

    # SVM metrics
    #mcc_svm_met = matthews_corrcoef(Y_test_clsfrs, Y_pred_svm)
    # classification report
    report_svm = classification_report(Y_test_clsfrs, Y_pred_svm, output_dict=True)
    report_svm_idr = classification_report(Y_raw_clsfrs_idr, Y_pred_svm_idr, output_dict=True)
    df_classificationreport_svm = pd.DataFrame(report_svm).transpose()
    df_classificationreport_svm_idr = pd.DataFrame(report_svm_idr).transpose()

    # entropy
    ll_svm = log_loss(Y_test_clsfrs, Y_pred_svm)
    ll_svm_idr = log_loss(Y_raw_clsfrs_idr, Y_pred_svm_idr)

    # class-specific metrics
    df_class_metrics_svm = df_classificationreport_svm.loc[['0','1','2','3'], :]
    df_class_metrics_svm['AUC'] = list(roc_auc_svm.values())
    df_class_metrics_svm_idr = df_classificationreport_svm_idr.loc[['0','1','2','3'], :]
    df_class_metrics_svm_idr['AUC'] = list(roc_auc_svm_idr.values())

    end_time_ovr_class_svm = time.time()
    total_time_ovr_class_svm = end_time_ovr_class_svm - start_time_ovr_class_svm

    # KNN:
    start_time_ovr_class_knn = time.time()
    # learn to predict each class against the other
    kNN = sklearn.neighbors.KNeighborsClassifier(metric= 'manhattan', n_neighbors= 15, weights= 'distance')
    classifier_knn = OneVsRestClassifier(kNN)  # probability=True
    classifier_knn_fit = classifier_knn.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_score_knn = classifier_knn_fit.predict_proba(X_test_scaled_clsfrs)
    y_score_knn_idr = classifier_knn_fit.predict_proba(X_scaled_clsfrs_idr)
    Y_pred_knn = classifier_knn.predict(X_test_scaled_clsfrs)
    Y_pred_knn_idr = classifier_knn.predict(X_scaled_clsfrs_idr)

    Y_pred_knn_nobin = lb.inverse_transform(Y_pred_knn)
    Y_pred_knn_idr_nobin = lb.inverse_transform(Y_pred_knn_idr)

    # Multiclass
    classifier_knn_multiclass = sklearn.neighbors.KNeighborsClassifier(metric= 'manhattan', n_neighbors= 15, weights= 'distance')
    classifier_knn_fit_multiclass = classifier_knn_multiclass.fit(X_train_scaled_clsfrs, Y_train_clsfrs_nobin)
    Y_pred_knn_multiclass = classifier_knn_multiclass.predict(X_test_scaled_clsfrs)
    Y_pred_knn_idr_multiclass = classifier_knn_fit_multiclass.predict(X_scaled_clsfrs_idr)
    # Multiclass evaluation metrics
    f1_multiclass.append(f1_score(Y_raw_clsfrs_idr_nobin, Y_pred_knn_idr_multiclass, average='weighted'))
    precision_multiclass.append(precision_score(Y_raw_clsfrs_idr_nobin, Y_pred_knn_idr_multiclass, average='weighted'))
    recall_multiclass.append(recall_score(Y_raw_clsfrs_idr_nobin, Y_pred_knn_idr_multiclass, average='weighted'))
    model_multiclass.append('KNN')

    # Compute ROC curve and ROC area for each class
    fpr_knn = dict()
    tpr_knn = dict()
    roc_auc_knn = dict()
    for i in range(n_classes_clsfrs):
        fpr_knn[i], tpr_knn[i], _ = roc_curve(Y_test_clsfrs[:, i], y_score_knn[:, i])
        roc_auc_knn[i] = metrics.auc(fpr_knn[i], tpr_knn[i])

    fpr_knn_idr = dict()
    tpr_knn_idr = dict()
    roc_auc_knn_idr = dict()
    for i_idr in range(n_classes_clsfrs_idr):
        fpr_knn_idr[i_idr], tpr_knn_idr[i_idr], _ = roc_curve(Y_raw_clsfrs_idr[:, i_idr], y_score_knn_idr[:, i_idr])
        roc_auc_knn_idr[i_idr] = metrics.auc(fpr_knn_idr[i_idr], tpr_knn_idr[i_idr])

    classifr_auc.append(roc_auc_knn)
    classifr_auc_idr.append(roc_auc_knn_idr)
    scores_knn = cross_val_score(classifier_knn, X_train_scaled_clsfrs, Y_train_clsfrs, cv=10)
    scores_knn_idr = cross_val_score(classifier_knn_fit, X_scaled_clsfrs_idr, Y_raw_clsfrs_idr, cv=10)
    classifr_accuracy.append(scores_knn.mean())
    classifr_accuracy_idr.append(scores_knn_idr.mean())

    # KNN Metrics
    # classification report
    report_knn = classification_report(Y_test_clsfrs, Y_pred_knn, output_dict=True)
    report_knn_idr = classification_report(Y_raw_clsfrs_idr, Y_pred_knn_idr, output_dict=True)
    df_classificationreport_knn = pd.DataFrame(report_knn).transpose()
    df_classificationreport_knn_idr = pd.DataFrame(report_knn_idr).transpose()

    # entropy
    ll_knn = log_loss(Y_test_clsfrs, Y_pred_knn)
    ll_knn_idr = log_loss(Y_raw_clsfrs_idr, Y_pred_knn_idr)

    # class-specific metrics
    df_class_metrics_knn = df_classificationreport_knn.loc[['0', '1', '2', '3'], :]
    df_class_metrics_knn['AUC'] = list(roc_auc_knn.values())
    df_class_metrics_knn_idr = df_classificationreport_knn_idr.loc[['0', '1', '2', '3'], :]
    df_class_metrics_knn_idr['AUC'] = list(roc_auc_knn_idr.values())

    end_time_ovr_class_knn = time.time()
    total_time_ovr_class_knn = end_time_ovr_class_knn - start_time_ovr_class_knn

    # RF:
    start_time_ovr_class_rf = time.time()
    # learn to predict each class against the other
    classifier_rf = OneVsRestClassifier(RandomForestClassifier(class_weight = 'balanced'))  # probability=True
    classifier_rf_fit = classifier_rf.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_score_rf = classifier_rf_fit.predict_proba(X_test_scaled_clsfrs) # probability estimate of the positive class, for roc
    y_score_rf_idr = classifier_rf_fit.predict_proba(X_scaled_clsfrs_idr)
    Y_pred_rf = classifier_rf.predict(X_test_scaled_clsfrs)
    Y_pred_rf_idr = classifier_rf_fit.predict(X_scaled_clsfrs_idr)

    # Multiclass
    classifier_rf_multiclass = RandomForestClassifier(class_weight = 'balanced')
    classifier_rf_fit_multiclass = classifier_rf_multiclass.fit(X_train_scaled_clsfrs, Y_train_clsfrs_nobin)
    Y_pred_rf_multiclass = classifier_rf_multiclass.predict(X_test_scaled_clsfrs)
    Y_pred_rf_idr_multiclass = classifier_rf_fit_multiclass.predict(X_scaled_clsfrs_idr)
    # Multiclass evaluation metrics
    f1_multiclass.append(f1_score(Y_raw_clsfrs_idr_nobin, Y_pred_rf_idr_multiclass, average='weighted'))
    precision_multiclass.append(precision_score(Y_raw_clsfrs_idr_nobin, Y_pred_rf_idr_multiclass, average='weighted'))
    recall_multiclass.append(recall_score(Y_raw_clsfrs_idr_nobin, Y_pred_rf_idr_multiclass, average='weighted'))
    model_multiclass.append('RF')

    df_multiclass_metrics = pd.DataFrame(['test']*3, columns = ['dataset'])

    df_multiclass_metrics['model'] = model_multiclass
    df_multiclass_metrics['f1_weighted'] = f1_multiclass
    df_multiclass_metrics['precision_weighted'] = precision_multiclass
    df_multiclass_metrics['recall_weighted'] = recall_multiclass

    df_multiclass_metrics.to_csv(save_dir + '/multiclass_metrics_' + time_dir + '.csv')

    Y_pred_rf_nobin = lb.inverse_transform(Y_pred_rf)
    Y_pred_rf_idr_nobin = lb.inverse_transform(Y_pred_rf_idr)

    end_time_ovr_class_rf = time.time()
    total_time_ovr_class_rf = end_time_ovr_class_rf - start_time_ovr_class_rf

    df_pred_nobins_train = pd.DataFrame(np.transpose(Y_pred_svm_nobin), columns = ['svm_train'])
    df_pred_nobins_test = pd.DataFrame(np.transpose(Y_pred_svm_idr_nobin), columns = ['svm_test'])
    df_pred_nobins_train['knn_train'] = Y_pred_knn_nobin
    df_pred_nobins_test['knn_test'] = Y_pred_knn_idr_nobin
    df_pred_nobins_train['rf_train'] = Y_pred_rf_nobin
    df_pred_nobins_test['rf_test'] = Y_pred_rf_idr_nobin

    # Compute ROC curve and ROC area for each class
    # original
    fpr_rf = dict()
    tpr_rf = dict()
    roc_auc_rf = dict()
    for i in range(n_classes_clsfrs):
        fpr_rf[i], tpr_rf[i], _ = roc_curve(Y_test_clsfrs[:, i], y_score_rf[:, i])
        roc_auc_rf[i] = metrics.auc(fpr_rf[i], tpr_rf[i])
    # idr
    fpr_rf_idr = dict()
    tpr_rf_idr = dict()
    roc_auc_rf_idr = dict()
    for i_idr in range(n_classes_clsfrs):
        fpr_rf_idr[i_idr], tpr_rf_idr[i_idr], _ = roc_curve(Y_raw_clsfrs_idr[:, i_idr], y_score_rf_idr[:, i_idr])
        roc_auc_rf_idr[i_idr] = metrics.auc(fpr_rf_idr[i_idr], tpr_rf_idr[i_idr])


    classifr_auc.append(roc_auc_rf)
    classifr_auc_idr.append(roc_auc_rf_idr)
    scores_rf = cross_val_score(classifier_rf, X_train_scaled_clsfrs, Y_train_clsfrs, cv=10)
    scores_rf_idr = cross_val_score(classifier_rf, X_scaled_clsfrs_idr, Y_raw_clsfrs_idr, cv=10)
    classifr_accuracy.append(scores_rf.mean())
    classifr_accuracy_idr.append(scores_rf_idr.mean())

    # RF Metrics
    # classification report
    report_rf = classification_report(Y_test_clsfrs, Y_pred_rf, output_dict=True)
    report_rf_idr = classification_report(Y_raw_clsfrs_idr, Y_pred_rf_idr, output_dict=True)
    df_classificationreport_rf = pd.DataFrame(report_rf).transpose()
    df_classificationreport_rf_idr = pd.DataFrame(report_rf_idr).transpose()

    # entropy
    ll_rf = log_loss(Y_test_clsfrs, Y_pred_rf)
    ll_rf_idr = log_loss(Y_raw_clsfrs_idr, Y_pred_rf_idr)

    # class-specific metrics
    df_class_metrics_rf = df_classificationreport_rf.loc[['0', '1', '2', '3'], :]
    df_class_metrics_rf['AUC'] = list(roc_auc_rf.values())
    df_class_metrics_rf_idr = df_classificationreport_rf_idr.loc[['0', '1', '2', '3'], :]
    df_class_metrics_rf_idr['AUC'] = list(roc_auc_rf_idr.values())

    end_time_ovr_class = time.time()
    total_time_ovr_class = end_time_ovr_class - start_time_ovr_class
    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- DEFINE AND TRAIN CLASSIFIERS
    # ******************************************************

    # ******************************************************
    start_time_ovr = time.time()
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- CLASSIFICATION EVALUATION

    # plot metrics and ROC curves
    # class 0: NOISE
    # ROC FOR LOOP :START
    for i_roc in range(no_classes):
        # original
        fpr_all = [fpr_svm[i_roc], fpr_knn[i_roc], fpr_rf[i_roc]]
        tpr_all = [tpr_svm[i_roc], tpr_knn[i_roc], tpr_rf[i_roc]]
        roc_auc_all = [roc_auc_svm[i_roc], roc_auc_knn[i_roc], roc_auc_rf[i_roc]]
        # idr
        fpr_all_idr = [fpr_svm_idr[i_roc], fpr_knn_idr[i_roc], fpr_rf_idr[i_roc]]
        tpr_all_idr = [tpr_svm_idr[i_roc], tpr_knn_idr[i_roc], tpr_rf_idr[i_roc]]
        roc_auc_all_idr = [roc_auc_svm_idr[i_roc], roc_auc_knn_idr[i_roc], roc_auc_rf_idr[i_roc]]

        fpr_all_flat = [item for sublist in fpr_all for item in sublist]
        tpr_all_flat = [item for sublist in tpr_all for item in sublist]

        fig, ax = plt.subplots()
        lw = 2
        methods = ['svm', 'knn', 'rf']
        colors = cycle(['royalblue','darkcyan','goldenrod'])
        for i, color in zip(range(len(methods)), colors):
            plt.plot(fpr_all[i], tpr_all[i], color=color, lw=lw,
                     label='{0} (AUC = {1:0.2f})'
                           ''.format(methods[i], roc_auc_all[i]))


        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Positive:' + str(classes[i_roc]))
        plt.legend(loc="lower right")
        #plt.show()
        plt.savefig(save_dir + '/BinaryProblems_roc_train_normalized_class_' + str(classes[i_roc]) + '_' + time_dir + '.pdf', dpi=300, bbox_inches='tight')
        plt.close('all')

        # idr
        for i_idr, color in zip(range(len(methods)), colors):
            plt.plot(fpr_all_idr[i_idr], tpr_all_idr[i_idr], color=color, lw=lw,
                     label='{0} (AUC = {1:0.2f})'
                           ''.format(methods[i_idr], roc_auc_all_idr[i_idr]))

        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Positive:' + str(classes[i_roc]) + ', idr')
        plt.legend(loc="lower right")
        plt.savefig(save_dir + '/BinaryProblems_roc_test_normalized_class_' + str(classes[i_roc]) + '_'  + time_dir + '.pdf', dpi=300, bbox_inches='tight')
        #plt.show()
        plt.close('all')

    end_time_ovr = time.time()
    total_time_ovr = end_time_ovr - start_time_ovr
    return df_multiclass_metrics
    print('Binary problems completed.')
    # ROC FOR LOOP :END



def classify_foci(summary_dir, dir_list):
    #******************************************************************************
    # START : LOAD PROPERTIES
    df_foci = pd.read_csv(base_dir + '/foci_info.csv')#pd.read_csv(base_dir + '/foci_info_20220128.csv')
    df_foci_og = df_foci
    df_foci_idr = pd.read_csv(base_dir + '/foci_info_idr.csv')
    df_foci_df = pd.read_csv(base_dir + '/foci_info_df.csv')

    iloc_rows = []

    df_foci_simple = df_foci_og.copy()
    df_foci_simple_idr = df_foci_idr.copy()
    df_foci_simple_df = df_foci_df.copy()
    # END : LOAD PROPERTIES
    #******************************************************************************

    # ******************************************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST
    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA
    # initialize evaliation metric lists
    classifr_auc = []
    classifr_accuracy = []
    classifr_auc_idr = []
    classifr_accuracy_idr = []
    classifr_auc_df = []
    classifr_accuracy_df = []

    # simplify datasets
    # original
    df_foci_simple = df_foci_simple.dropna()
    df_foci_simple = df_foci_simple.reset_index(drop = True)
    df_foci_simple_clsfrs = df_foci_simple.loc[ (df_foci_simple['status'] == 1) | (
                    df_foci_simple['status'] == 3) | (df_foci_simple['status'] == 4) | (df_foci_simple['status'] == 5)]
    # idr
    df_foci_simple_idr = df_foci_simple_idr.dropna()
    df_foci_simple_idr = df_foci_simple_idr.reset_index(drop = True)
    df_foci_simple_idr = df_foci_simple_idr.loc[ (df_foci_simple_idr['status'] == 1) | (
                    df_foci_simple_idr['status'] == 3) | (df_foci_simple_idr['status'] == 4) | (df_foci_simple_idr['status'] == 5)]
    # df
    df_foci_simple_df = df_foci_simple_df.dropna()
    df_foci_simple_df = df_foci_simple_df.reset_index(drop = True)
    df_foci_simple_df = df_foci_simple_df.loc[ (df_foci_simple_df['status'] == 1) | (
                    df_foci_simple_df['status'] == 3) | (df_foci_simple_df['status'] == 4) | (df_foci_simple_df['status'] == 5)]

    # organize data for classification
    dropped_props = ['Unnamed: 0', 'protein', 'exp_folder', 'roi_id', 'focus_label',
        'status', 'centroid-0', 'centroid-1', 'coords', 'orientation' ,'bbox', 'focus_max', 'focus_min','fft_eccentricity',
        'fft_major_axis_length', 'fft_minor_axis_length','fft_area','fft_orientation','fft_eccentricity','orientation',
        'compare_val_correl','focus_med', 'focus_mean', 'fft_min', 'fft_std', 'fft_var_laplacian',
        'var_lap_gb9', 'focus_var', 'focus_std', 'fft_euler_number', 'euler_number', 'fft_orientation','fft_blur_2',
        'fft_major_axis_length', 'fft_minor_axis_length','solidity','fft_mean', 'fft_med']

    dropped_props_df = ['Unnamed: 0', 'exp_folder', 'roi_id', 'focus_label',
        'status', 'centroid-0', 'centroid-1', 'coords', 'orientation' ,'bbox', 'focus_max', 'focus_min','fft_eccentricity',
        'fft_major_axis_length', 'fft_minor_axis_length','fft_area','fft_orientation','fft_eccentricity','orientation',
        'compare_val_correl','focus_med', 'focus_mean', 'fft_min', 'fft_std', 'fft_var_laplacian',
        'var_lap_gb9', 'focus_var', 'focus_std', 'fft_euler_number', 'euler_number', 'fft_orientation','fft_blur_2',
        'fft_major_axis_length', 'fft_minor_axis_length','solidity','fft_mean', 'fft_med']

    # X raw
    X_raw_clsfrs = df_foci_simple_clsfrs.drop(dropped_props, axis=1).copy()
    X_raw_clsfrs_idr = df_foci_simple_idr.drop(dropped_props, axis=1).copy()
    X_raw_clsfrs_df = df_foci_simple_df.drop(dropped_props_df, axis=1).copy()
    # Y raw
    Y_raw_clsfrs = df_foci_simple_clsfrs['status'].copy()
    Y_raw_clsfrs_idr = df_foci_simple_idr['status'].copy()
    Y_raw_clsfrs_df = df_foci_simple_df['status'].copy()

    # weights
    from sklearn.utils import class_weight
    class_weight = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(Y_raw_clsfrs), y = Y_raw_clsfrs)

    # binarize the output
    classes = [1, 3, 4, 5]
    lb = preprocessing.LabelBinarizer()
    no_classes = len(classes)
    # original
    Y_raw_clsfrs = lb.fit_transform(Y_raw_clsfrs)
    n_classes_clsfrs = Y_raw_clsfrs.shape[1]
    # idr
    Y_raw_clsfrs_idr = lb.fit_transform(Y_raw_clsfrs_idr)
    n_classes_clsfrs_idr = Y_raw_clsfrs_idr.shape[1]
    # df
    Y_raw_clsfrs_df = lb.fit_transform(Y_raw_clsfrs_df)
    n_classes_clsfrs_df = Y_raw_clsfrs_df.shape[1]
    # split into train and test sets
    X_train_clsfrs, X_test_clsfrs, Y_train_clsfrs, Y_test_clsfrs = train_test_split(X_raw_clsfrs, Y_raw_clsfrs,
                                                                                    test_size=0.3,
                                                                                    random_state=42)  # random_state=0
    # inverse binarization
    Y_train_clsfrs_nobin = lb.inverse_transform(Y_train_clsfrs)
    Y_raw_clsfrs_idr_nobin = lb.inverse_transform(Y_raw_clsfrs_idr)
    Y_raw_clsfrs_df_nobin = lb.inverse_transform(Y_raw_clsfrs_df)
    # scale data to speed up classification
    X_train_scaled_clsfrs = scale(X_train_clsfrs)
    X_test_scaled_clsfrs = scale(X_test_clsfrs)
    X_scaled_clsfrs_idr = scale(X_raw_clsfrs_idr)
    X_scaled_clsfrs_df = scale(X_raw_clsfrs_df)

    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA
    # ******************************************************
    '''
    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- NO BINARY LABELS -- CROSS VALIDATED METRICS
    start_time = time.time()
    score_options = sorted(sklearn.metrics.SCORERS.keys())
    # Cross validated metrics
    model_svm = svm.SVC(kernel='rbf', probability=True, class_weight = 'balanced')
    model_knn = sklearn.neighbors.KNeighborsClassifier(metric= 'manhattan', n_neighbors= 15, weights= 'distance')
    model_rf = RandomForestClassifier(class_weight = 'balanced')
    models = [model_svm, model_knn, model_rf]
    X_Y_data = [[X_train_scaled_clsfrs, Y_train_clsfrs_nobin], [X_scaled_clsfrs_idr, Y_raw_clsfrs_idr_nobin], [X_scaled_clsfrs_df, Y_raw_clsfrs_df_nobin]]
    dataset_labels = ['BP1-2','53BP1','H2AX']
    model_labels = ['SVM', 'KNN','RF']
    all_scores = ('fowlkes_mallows_score','jaccard_weighted','accuracy', 'f1_weighted','precision_weighted', 'recall_weighted','roc_auc_ovo_weighted', 'roc_auc_ovr_weighted')
    cv_scores_full = []
    dataset_labels_ = []
    model_labels_ = []
    # compute cv scores, label lists
    for i_model in range(len(models)): #len(models)
        for i_data in range(len(X_Y_data)): # len(X_Y_data)
            cv_scores = cross_validate(models[i_model], X_Y_data[i_data][0],  X_Y_data[i_data][1], cv=10, scoring = all_scores)
            cv_scores_full.append(cv_scores)
            # create label lists
            dataset_labels_.append([dataset_labels[i_data]]*10)
            model_labels_.append([model_labels[i_model]]*10)
            del cv_scores

    # organize labels into dataframe
    dataset_labels_l = list(itertools.chain.from_iterable(dataset_labels_))
    model_labels_l = list(itertools.chain.from_iterable(model_labels_))
    df_scores_full = pd.DataFrame(dataset_labels_l, columns= ['dataset'])
    df_scores_full['model'] = model_labels_l

    # organize scores into df columns
    for i_score in all_scores:
        score_label = 'test_' + i_score
        scores_full = []
        for i_cv_scores in cv_scores_full:
            cv_score = i_cv_scores[score_label]
            for i_cv_score in cv_score:
                scores_full.append(i_cv_score)
        df_scores_full[i_score] = scores_full # add scores to df

    # plot and save boxplots
    for i_plot in all_scores:
        plt.close()
        score_plt = sns.catplot(x="model", y=i_plot, hue="dataset", kind="box", data=df_scores_full)
        score_plt.figure.savefig(base_dir + '/'+ i_plot + '_boxplot_test.pdf')
    plt.close()
    end_time = time.time()
    total_time = end_time - start_time
    # END: CLASSIFICATION -- IMPLEMENTATION -- NO BINARY LABELS -- CROSS VALIDATED METRICS
    # ******************************************************
    '''
    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- DEFINE AND TRAIN CLASSIFIERS
    start_time_ovr_class = time.time()
    # SVM:
    # OneVsRest
    # learn to predict each class against the other
    classifier_svm = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True, class_weight = 'balanced'))
    classifier_svm_fit = classifier_svm.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_score_svm = classifier_svm_fit.decision_function(X_test_scaled_clsfrs)
    y_score_svm_idr = classifier_svm_fit.decision_function(X_scaled_clsfrs_idr)
    y_score_svm_df = classifier_svm_fit.decision_function(X_scaled_clsfrs_df)
    Y_pred_svm = classifier_svm.predict(X_test_scaled_clsfrs)
    Y_pred_svm_idr = classifier_svm_fit.predict(X_scaled_clsfrs_idr)
    Y_pred_svm_df = classifier_svm_fit.predict(X_scaled_clsfrs_df)

    # Compute ROC curve and ROC area for each class
    fpr_svm = dict()
    tpr_svm = dict()
    roc_auc_svm = dict()
    fpr_svm_idr = dict()
    tpr_svm_idr = dict()
    roc_auc_svm_idr = dict()
    fpr_svm_df = dict()
    tpr_svm_df = dict()
    roc_auc_svm_df = dict()
    for i in range(n_classes_clsfrs):
        fpr_svm[i], tpr_svm[i], _ = roc_curve(Y_test_clsfrs[:, i], y_score_svm[:, i])
        roc_auc_svm[i] = metrics.auc(fpr_svm[i], tpr_svm[i])
    for i_idr in range(n_classes_clsfrs_idr):
        fpr_svm_idr[i_idr], tpr_svm_idr[i_idr], _ = roc_curve(Y_raw_clsfrs_idr[:, i_idr], y_score_svm_idr[:, i_idr])
        roc_auc_svm_idr[i_idr] = metrics.auc(fpr_svm_idr[i_idr], tpr_svm_idr[i_idr])
    for i_df in range(n_classes_clsfrs_df):
        fpr_svm_df[i_df], tpr_svm_df[i_df], _ = roc_curve(Y_raw_clsfrs_df[:, i_df], y_score_svm_df[:, i_df])
        roc_auc_svm_df[i_df] = metrics.auc(fpr_svm_df[i_df], tpr_svm_df[i_df])

    classifr_auc.append(roc_auc_svm)
    classifr_auc_idr.append(roc_auc_svm_idr)
    classifr_auc_df.append(roc_auc_svm_df)

    # SVM cross validation
    start_time_ovr_class_svm = time.time()
    scores_svm = cross_val_score(classifier_svm, X_train_scaled_clsfrs, Y_train_clsfrs, cv=10)
    scores_svm_idr = cross_val_score(classifier_svm, X_scaled_clsfrs_idr, Y_raw_clsfrs_idr, cv=10)
    scores_svm_df = cross_val_score(classifier_svm, X_scaled_clsfrs_df, Y_raw_clsfrs_df, cv=10)
    classifr_accuracy.append(scores_svm.mean())
    classifr_accuracy_idr.append(scores_svm_idr.mean())
    classifr_accuracy_df.append(scores_svm_df.mean())

    # SVM metrics
    #mcc_svm_met = matthews_corrcoef(Y_test_clsfrs, Y_pred_svm)
    # classification report
    report_svm = classification_report(Y_test_clsfrs, Y_pred_svm, output_dict=True)
    report_svm_idr = classification_report(Y_raw_clsfrs_idr, Y_pred_svm_idr, output_dict=True)
    report_svm_df = classification_report(Y_raw_clsfrs_df, Y_pred_svm_df, output_dict=True)
    df_classificationreport_svm = pd.DataFrame(report_svm).transpose()
    df_classificationreport_svm_idr = pd.DataFrame(report_svm_idr).transpose()
    df_classificationreport_svm_df = pd.DataFrame(report_svm_df).transpose()

    # entropy
    ll_svm = log_loss(Y_test_clsfrs, Y_pred_svm)
    ll_svm_idr = log_loss(Y_raw_clsfrs_idr, Y_pred_svm_idr)

    # class-specific metrics
    df_class_metrics_svm = df_classificationreport_svm.loc[['0','1','2','3'], :]
    df_class_metrics_svm['AUC'] = list(roc_auc_svm.values())
    df_class_metrics_svm_idr = df_classificationreport_svm_idr.loc[['0','1','2','3'], :]
    df_class_metrics_svm_idr['AUC'] = list(roc_auc_svm_idr.values())
    df_class_metrics_svm_df = df_classificationreport_svm_df.loc[['0','1','2','3'], :]
    df_class_metrics_svm_df['AUC'] = list(roc_auc_svm_df.values())
    df_class_metrics_svm.to_csv(base_dir + '/class_metrics_svm.csv')
    df_class_metrics_svm_idr.to_csv(base_dir + '/class_metrics_svm_idr.csv')
    df_class_metrics_svm_df.to_csv(base_dir + '/class_metrics_svm_df.csv')

    end_time_ovr_class_svm = time.time()
    total_time_ovr_class_svm = end_time_ovr_class_svm - start_time_ovr_class_svm

    # KNN:
    start_time_ovr_class_knn = time.time()
    # learn to predict each class against the other
    kNN = sklearn.neighbors.KNeighborsClassifier(metric= 'manhattan', n_neighbors= 15, weights= 'distance')
    classifier_knn = OneVsRestClassifier(kNN)  # probability=True
    classifier_knn_fit = classifier_knn.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_score_knn = classifier_knn_fit.predict_proba(X_test_scaled_clsfrs)
    y_score_knn_idr = classifier_knn_fit.predict_proba(X_scaled_clsfrs_idr)
    y_score_knn_df = classifier_knn_fit.predict_proba(X_scaled_clsfrs_df)
    Y_pred_knn = classifier_knn.predict(X_test_scaled_clsfrs)
    Y_pred_knn_idr = classifier_knn.predict(X_scaled_clsfrs_idr)
    Y_pred_knn_df = classifier_knn.predict(X_scaled_clsfrs_df)

    # Compute ROC curve and ROC area for each class
    fpr_knn = dict()
    tpr_knn = dict()
    roc_auc_knn = dict()
    for i in range(n_classes_clsfrs):
        fpr_knn[i], tpr_knn[i], _ = roc_curve(Y_test_clsfrs[:, i], y_score_knn[:, i])
        roc_auc_knn[i] = metrics.auc(fpr_knn[i], tpr_knn[i])

    fpr_knn_idr = dict()
    tpr_knn_idr = dict()
    roc_auc_knn_idr = dict()
    for i_idr in range(n_classes_clsfrs_idr):
        fpr_knn_idr[i_idr], tpr_knn_idr[i_idr], _ = roc_curve(Y_raw_clsfrs_idr[:, i_idr], y_score_knn_idr[:, i_idr])
        roc_auc_knn_idr[i_idr] = metrics.auc(fpr_knn_idr[i_idr], tpr_knn_idr[i_idr])

    fpr_knn_df = dict()
    tpr_knn_df = dict()
    roc_auc_knn_df = dict()
    for i_df in range(n_classes_clsfrs_df):
        fpr_knn_df[i_df], tpr_knn_df[i_df], _ = roc_curve(Y_raw_clsfrs_df[:, i_df], y_score_knn_df[:, i_df])
        roc_auc_knn_df[i_df] = metrics.auc(fpr_knn_df[i_df], tpr_knn_df[i_df])

    classifr_auc.append(roc_auc_knn)
    classifr_auc_idr.append(roc_auc_knn_idr)
    classifr_auc_df.append(roc_auc_knn_df)
    scores_knn = cross_val_score(classifier_knn, X_train_scaled_clsfrs, Y_train_clsfrs, cv=10)
    scores_knn_idr = cross_val_score(classifier_knn_fit, X_scaled_clsfrs_idr, Y_raw_clsfrs_idr, cv=10)
    scores_knn_df = cross_val_score(classifier_knn_fit, X_scaled_clsfrs_df, Y_raw_clsfrs_df, cv=10)
    classifr_accuracy.append(scores_knn.mean())
    classifr_accuracy_idr.append(scores_knn_idr.mean())
    classifr_accuracy_df.append(scores_knn_df.mean())

    # KNN Metrics
    # classification report
    report_knn = classification_report(Y_test_clsfrs, Y_pred_knn, output_dict=True)
    report_knn_idr = classification_report(Y_raw_clsfrs_idr, Y_pred_knn_idr, output_dict=True)
    report_knn_df = classification_report(Y_raw_clsfrs_df, Y_pred_knn_df, output_dict=True)
    df_classificationreport_knn = pd.DataFrame(report_knn).transpose()
    df_classificationreport_knn_idr = pd.DataFrame(report_knn_idr).transpose()
    df_classificationreport_knn_df = pd.DataFrame(report_knn_df).transpose()

    # entropy
    ll_knn = log_loss(Y_test_clsfrs, Y_pred_knn)
    ll_knn_idr = log_loss(Y_raw_clsfrs_idr, Y_pred_knn_idr)
    ll_knn_df = log_loss(Y_raw_clsfrs_df, Y_pred_knn_df)

    # class-specific metrics
    df_class_metrics_knn = df_classificationreport_knn.loc[['0', '1', '2', '3'], :]
    df_class_metrics_knn['AUC'] = list(roc_auc_knn.values())
    df_class_metrics_knn_idr = df_classificationreport_knn_idr.loc[['0', '1', '2', '3'], :]
    df_class_metrics_knn_idr['AUC'] = list(roc_auc_knn_idr.values())
    df_class_metrics_knn_df = df_classificationreport_knn_df.loc[['0', '1', '2', '3'], :]
    df_class_metrics_knn_df['AUC'] = list(roc_auc_knn_df.values())
    df_class_metrics_knn.to_csv(base_dir + '/class_metrics_knn.csv')
    df_class_metrics_knn_idr.to_csv(base_dir + '/class_metrics_knn_idr.csv')
    df_class_metrics_knn_df.to_csv(base_dir + '/class_metrics_knn_df.csv')

    end_time_ovr_class_knn = time.time()
    total_time_ovr_class_knn = end_time_ovr_class_knn - start_time_ovr_class_knn

    # RF:
    start_time_ovr_class_rf = time.time()
    # learn to predict each class against the other
    classifier_rf = OneVsRestClassifier(RandomForestClassifier(class_weight = 'balanced'))  # probability=True
    classifier_rf_fit = classifier_rf.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_score_rf = classifier_rf_fit.predict_proba(X_test_scaled_clsfrs) # probability estimate of the positive class, for roc
    y_score_rf_idr = classifier_rf_fit.predict_proba(X_scaled_clsfrs_idr)
    y_score_rf_df = classifier_rf_fit.predict_proba(X_scaled_clsfrs_df)
    Y_pred_rf = classifier_rf.predict(X_test_scaled_clsfrs)
    Y_pred_rf_idr = classifier_rf_fit.predict(X_scaled_clsfrs_idr)
    Y_pred_rf_df = classifier_rf_fit.predict(X_scaled_clsfrs_df)

    end_time_ovr_class_rf = time.time()
    total_time_ovr_class_rf = end_time_ovr_class_rf - start_time_ovr_class_rf

    '''
    # VOTING CLASSIFIER:
    graph_builder = LabelCooccurrenceGraphBuilder(weighted=True,
                                                  include_self_edges=False)
    classifier = LabelSpacePartitioningClassifier(
        classifier=BinaryRelevance(
            classifier=RandomForestClassifier(class_weight = 'balanced'),
            require_dense=[False, True]
        ),
        clusterer=NetworkXLabelGraphClusterer(graph_builder, method='louvain')
    )

    start = time.time()
    classifier.fit(X_train_scaled_clsfrs, Y_train_clsfrs)

    print('training time taken: ', round(time.time() - start, 0), 'seconds')


    # group / ensemble of models
    estimator = []
    estimator.append(('RF',
                      RandomForestClassifier(class_weight = 'balanced')))
    estimator.append(('KNN', kNN))
    estimator.append(('SVC', svm.SVC(kernel='rbf', probability=True, class_weight = 'balanced')))

    # Voting Classifier with hard voting
    vot_hard = VotingClassifier(estimators=estimator, voting='hard')
    vot_hard.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_pred = vot_hard.predict(X_test_scaled_clsfrs)

    # using accuracy_score metric to predict accuracy
    score = accuracy_score(Y_test_clsfrs, y_pred)

    # Voting Classifier with soft voting
    vot_soft = VotingClassifier(estimators=estimator, voting='soft')
    vot_soft.fit(X_train_scaled_clsfrs, Y_train_clsfrs)
    y_pred = vot_soft.predict(X_test_scaled_clsfrs)
    score = accuracy_score(Y_test_clsfrs, y_pred)
    '''

    # Compute ROC curve and ROC area for each class
    # original
    fpr_rf = dict()
    tpr_rf = dict()
    roc_auc_rf = dict()
    for i in range(n_classes_clsfrs):
        fpr_rf[i], tpr_rf[i], _ = roc_curve(Y_test_clsfrs[:, i], y_score_rf[:, i])
        roc_auc_rf[i] = metrics.auc(fpr_rf[i], tpr_rf[i])
    # idr
    fpr_rf_idr = dict()
    tpr_rf_idr = dict()
    roc_auc_rf_idr = dict()
    for i_idr in range(n_classes_clsfrs):
        fpr_rf_idr[i_idr], tpr_rf_idr[i_idr], _ = roc_curve(Y_raw_clsfrs_idr[:, i_idr], y_score_rf_idr[:, i_idr])
        roc_auc_rf_idr[i_idr] = metrics.auc(fpr_rf_idr[i_idr], tpr_rf_idr[i_idr])
    # df
    fpr_rf_df = dict()
    tpr_rf_df = dict()
    roc_auc_rf_df = dict()
    for i_df in range(n_classes_clsfrs):
        fpr_rf_df[i_df], tpr_rf_df[i_df], _ = roc_curve(Y_raw_clsfrs_df[:, i_df], y_score_rf_df[:, i_df])
        roc_auc_rf_df[i_df] = metrics.auc(fpr_rf_df[i_df], tpr_rf_df[i_df])

    classifr_auc.append(roc_auc_rf)
    classifr_auc_idr.append(roc_auc_rf_idr)
    classifr_auc_df.append(roc_auc_rf_df)
    scores_rf = cross_val_score(classifier_rf, X_train_scaled_clsfrs, Y_train_clsfrs, cv=10)
    scores_rf_idr = cross_val_score(classifier_rf, X_scaled_clsfrs_idr, Y_raw_clsfrs_idr, cv=10)
    scores_rf_df = cross_val_score(classifier_rf, X_scaled_clsfrs_df, Y_raw_clsfrs_df, cv=10)
    classifr_accuracy.append(scores_rf.mean())
    classifr_accuracy_idr.append(scores_rf_idr.mean())
    classifr_accuracy_df.append(scores_rf_df.mean())

    # RF Metrics
    # classification report
    report_rf = classification_report(Y_test_clsfrs, Y_pred_rf, output_dict=True)
    report_rf_idr = classification_report(Y_raw_clsfrs_idr, Y_pred_rf_idr, output_dict=True)
    report_rf_df = classification_report(Y_raw_clsfrs_df, Y_pred_rf_df, output_dict=True)
    df_classificationreport_rf = pd.DataFrame(report_rf).transpose()
    df_classificationreport_rf_idr = pd.DataFrame(report_rf_idr).transpose()
    df_classificationreport_rf_df = pd.DataFrame(report_rf_df).transpose()

    # entropy
    ll_rf = log_loss(Y_test_clsfrs, Y_pred_rf)
    ll_rf_idr = log_loss(Y_raw_clsfrs_idr, Y_pred_rf_idr)
    ll_rf_df = log_loss(Y_raw_clsfrs_df, Y_pred_rf_df)

    # class-specific metrics
    df_class_metrics_rf = df_classificationreport_rf.loc[['0', '1', '2', '3'], :]
    df_class_metrics_rf['AUC'] = list(roc_auc_rf.values())
    df_class_metrics_rf_idr = df_classificationreport_rf_idr.loc[['0', '1', '2', '3'], :]
    df_class_metrics_rf_idr['AUC'] = list(roc_auc_rf_idr.values())
    df_class_metrics_rf_df = df_classificationreport_rf_df.loc[['0', '1', '2', '3'], :]
    df_class_metrics_rf_df['AUC'] = list(roc_auc_rf_df.values())
    df_class_metrics_rf.to_csv(base_dir + '/class_metrics_rf.csv')
    df_class_metrics_rf_idr.to_csv(base_dir + '/class_metrics_rf_idr.csv')
    df_class_metrics_rf_df.to_csv(base_dir + '/class_metrics_rf_df.csv')

    end_time_ovr_class = time.time()
    total_time_ovr_class = end_time_ovr_class - start_time_ovr_class
    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- DEFINE AND TRAIN CLASSIFIERS
    # ******************************************************

    # ******************************************************
    start_time_ovr = time.time()
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- CLASSIFICATION EVALUATION

    # plot metrics and ROC curves
    # class 0: NOISE

    '''
    merged_0_df = merged_df.loc[[0, 6, 12], :]
    merged_0_df['accuracy'] = classifr_accuracy
    merged_0_df['auc'] = [classifr_auc[0][0], classifr_auc[1][0], classifr_auc[2][0]]

    merged_0_df_idr = merged_df_idr.loc[[0, 6, 12], :]
    merged_0_df_idr['accuracy'] = classifr_accuracy_idr
    merged_0_df_idr['auc'] = [classifr_auc_idr[0][0], classifr_auc_idr[1][0], classifr_auc_idr[2][0]]

    # Metrics bar plot
    svm_0 = merged_0_df.loc[0, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_0 = merged_0_df.loc[6, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_0 = merged_0_df.loc[12, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()

    # Metrics bar plot, idr
    svm_0_idr = merged_0_df_idr.loc[0, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_0_idr = merged_0_df_idr.loc[6, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_0_idr = merged_0_df_idr.loc[12, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()

    index = ['precision', 'recall', 'F1 score', 'accuracy', 'auc']
    plotdata = pd.DataFrame({'svm_0': svm_0, 'knn_0': knn_0, 'rf_0': rf_0}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers", fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 0 (noise)')
    plt.show()
    plt.close('all')

    # idr
    plotdata_idr = pd.DataFrame({'svm_0': svm_0_idr, 'knn_0': knn_0_idr, 'rf_0': rf_0_idr}, index=index)
    plotdata_idr.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers, idr", fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 0 (noise), idr')
    plt.show()
    plt.close('all')
    '''
    # ROC FOR LOOP :START
    for i_roc in range(no_classes):
        # original
        fpr_all = [fpr_svm[i_roc], fpr_knn[i_roc], fpr_rf[i_roc]]
        tpr_all = [tpr_svm[i_roc], tpr_knn[i_roc], tpr_rf[i_roc]]
        roc_auc_all = [roc_auc_svm[i_roc], roc_auc_knn[i_roc], roc_auc_rf[i_roc]]
        # idr
        fpr_all_idr = [fpr_svm_idr[i_roc], fpr_knn_idr[i_roc], fpr_rf_idr[i_roc]]
        tpr_all_idr = [tpr_svm_idr[i_roc], tpr_knn_idr[i_roc], tpr_rf_idr[i_roc]]
        roc_auc_all_idr = [roc_auc_svm_idr[i_roc], roc_auc_knn_idr[i_roc], roc_auc_rf_idr[i_roc]]
        # df
        fpr_all_df = [fpr_svm_df[i_roc], fpr_knn_df[i_roc], fpr_rf_df[i_roc]]
        tpr_all_df = [tpr_svm_df[i_roc], tpr_knn_df[i_roc], tpr_rf_df[i_roc]]
        roc_auc_all_df = [roc_auc_svm_df[i_roc], roc_auc_knn_df[i_roc], roc_auc_rf_df[i_roc]]

        lw = 2
        methods = ['svm', 'knn', 'rf']
        colors = cycle(['royalblue','darkcyan','goldenrod'])
        for i, color in zip(range(len(methods)), colors):
            plt.plot(fpr_all[i], tpr_all[i], color=color, lw=lw,
                     label='{0} (AUC = {1:0.2f})'
                           ''.format(methods[i], roc_auc_all[i]))

        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Positive:' + str(classes[i_roc]))
        plt.legend(loc="lower right")
        #plt.show()
        plt.close('all')

        # idr
        for i_idr, color in zip(range(len(methods)), colors):
            plt.plot(fpr_all_idr[i_idr], tpr_all_idr[i_idr], color=color, lw=lw,
                     label='{0} (AUC = {1:0.2f})'
                           ''.format(methods[i_idr], roc_auc_all_idr[i_idr]))

        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Positive:' + str(classes[i_roc]) + ', idr')
        plt.legend(loc="lower right")
        #plt.show()
        plt.close('all')

        # df
        for i_df, color in zip(range(len(methods)), colors):
            plt.plot(fpr_all_df[i_df], tpr_all_df[i_df], color=color, lw=lw,
                     label='{0} (AUC = {1:0.2f})'
                           ''.format(methods[i_df], roc_auc_all_df[i_df]))

        plt.plot([0, 1], [0, 1], 'k--', lw=lw)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Positive:' + str(classes[i_roc]) + ', df')
        plt.legend(loc="lower right")
        plt.show()
        plt.close('all')
    end_time_ovr = time.time()
    total_time_ovr = end_time_ovr - start_time_ovr

    # ROC FOR LOOP :END


    '''
    # ROC curves
    fpr_all_0 = [fpr_svm[0], fpr_knn[0], fpr_rf[0]]
    tpr_all_0 = [tpr_svm[0], tpr_knn[0], tpr_rf[0]]
    roc_auc_all_0 = [roc_auc_svm[0], roc_auc_knn[0], roc_auc_rf[0]]

    fpr_all_0_idr = [fpr_svm_idr[0], fpr_knn_idr[0], fpr_rf_idr[0]]
    tpr_all_0_idr = [tpr_svm_idr[0], tpr_knn_idr[0], tpr_rf_idr[0]]
    roc_auc_all_0_idr = [roc_auc_svm_idr[0], roc_auc_knn_idr[0], roc_auc_rf_idr[0]]

    lw = 2
    methods = ['svm', 'knn', 'rf']
    colors = cycle(['royalblue','darkcyan','goldenrod'])
    for i, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_0[i], tpr_all_0[i], color=color, lw=lw,
                 label='{0} (AUC = {1:0.2f})'
                       ''.format(methods[i], roc_auc_all_0[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 0 (noise), Negative: 1-5')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    # idr
    for i_idr, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_0_idr[i_idr], tpr_all_0_idr[i_idr], color=color, lw=lw,
                 label='{0} (AUC = {1:0.2f})'
                       ''.format(methods[i_idr], roc_auc_all_0_idr[i_idr]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 0 (noise), Negative: 1-5, idr')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')
    '''
    '''
    # class 1: BLURRED
    merged_1_df = merged_df.loc[[1, 7, 13], :]
    merged_1_df['accuracy'] = classifr_accuracy
    merged_1_df['auc'] = [classifr_auc[0][1], classifr_auc[1][1], classifr_auc[2][1]]

    merged_1_df_idr = merged_df_idr.loc[[1, 7, 13], :]
    merged_1_df_idr['accuracy'] = classifr_accuracy_idr
    merged_1_df_idr['auc'] = [classifr_auc_idr[0][1], classifr_auc_idr[1][1], classifr_auc_idr[2][1]]

    # Metrics bar plot
    svm_1 = merged_1_df.loc[1, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_1 = merged_1_df.loc[7, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_1 = merged_1_df.loc[13, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    index = ['precision', 'recall', 'F1 score', 'accuracy', 'auc']
    plotdata = pd.DataFrame({'svm_1': svm_1, 'knn_1': knn_1,'rf_1': rf_1}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 1 (blurred)')
    plt.show()
    plt.close('all')

    # Metrics bar plot, idr
    svm_1_idr = merged_1_df_idr.loc[1, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_1_idr = merged_1_df_idr.loc[7, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_1_idr = merged_1_df_idr.loc[13, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    plotdata_idr = pd.DataFrame({'svm_1': svm_1_idr, 'knn_1': knn_1_idr,'rf_1': rf_1_idr}, index=index)
    plotdata_idr.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 1 (blurred), idr')
    plt.show()
    plt.close('all')
    '''
    # ROC curves
    fpr_all_1 = [fpr_svm[0], fpr_knn[0], fpr_rf[0]]
    tpr_all_1 = [tpr_svm[0], tpr_knn[0], tpr_rf[0]]
    roc_auc_all_1 = [roc_auc_svm[0], roc_auc_knn[0], roc_auc_rf[0]]
    lw = 2
    methods = ['svm', 'knn', 'rf']
    colors = cycle(['royalblue','darkcyan','goldenrod'])
    for i, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_1[i], tpr_all_1[i], color=color, lw=lw,
                 label='{0} (AUC = {1:0.2f})'
                       ''.format(methods[i], roc_auc_all_1[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 1 (blurred), Negative: 0, 2-5')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    # ROC curves, idr
    fpr_all_1_idr = [fpr_svm_idr[0], fpr_knn_idr[0], fpr_rf_idr[0]]
    tpr_all_1_idr = [tpr_svm_idr[0], tpr_knn_idr[0], tpr_rf_idr[0]]
    roc_auc_all_1_idr = [roc_auc_svm_idr[0], roc_auc_knn_idr[0], roc_auc_rf_idr[0]]
    for i_idr, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_1_idr[i_idr], tpr_all_1_idr[i_idr], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i_idr], roc_auc_all_1_idr[i_idr]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 1 (blurred), Negative: 0, 2-5, idr')
    plt.legend(loc="lower right")
    plt.show()
    plt.close()

    '''
    # class 2: MISDETECTION
    plt.close('all')
    merged_2_df = merged_df.loc[[2, 8, 14], :]
    merged_2_df['accuracy'] = classifr_accuracy
    merged_2_df['auc'] = [classifr_auc[0][2], classifr_auc[1][2], classifr_auc[2][2]]

    merged_2_df_idr = merged_df_idr.loc[[2, 8, 14], :]
    merged_2_df_idr['accuracy'] = classifr_accuracy_idr
    merged_2_df_idr['auc'] = [classifr_auc_idr[0][2], classifr_auc_idr[1][2], classifr_auc_idr[2][2]]

    # Metrics bar plot
    svm_2 = merged_2_df.loc[2, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_2 = merged_2_df.loc[8, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_2 = merged_2_df.loc[14, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    index = ['precision', 'recall', 'F1 score', 'accuracy', 'auc']
    svm_2_idr = merged_2_df_idr.loc[2, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_2_idr = merged_2_df_idr.loc[8, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_2_idr = merged_2_df_idr.loc[14, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()

    plotdata = pd.DataFrame({'svm_2': svm_2, 'knn_2': knn_2,'rf_2': rf_2}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 2 (misdetection)')
    plt.show()
    plt.close('all')

    # idr
    plotdata_idr = pd.DataFrame({'svm_2': svm_2_idr, 'knn_2': knn_2_idr,'rf_2': rf_2_idr}, index=index)
    plotdata_idr.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 2 (misdetection), idr')
    plt.show()
    plt.close('all')

    # ROC curves
    fpr_all_2 = [fpr_svm[2], fpr_knn[2], fpr_rf[2]]
    tpr_all_2 = [tpr_svm[2], tpr_knn[2], tpr_rf[2]]
    roc_auc_all_2 = [roc_auc_svm[2], roc_auc_knn[2], roc_auc_rf[2]]
    lw = 2
    methods = ['svm', 'knn', 'rf']
    colors = cycle(['royalblue','darkcyan','goldenrod'])
    for i, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_2[i], tpr_all_2[i], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i], roc_auc_all_2[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 2 (misdetection), Negative: 0, 1, 3-5')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    # ROC curves, idr
    fpr_all_2_idr = [fpr_svm_idr[2], fpr_knn_idr[2], fpr_rf_idr[2]]
    tpr_all_2_idr = [tpr_svm_idr[2], tpr_knn_idr[2], tpr_rf_idr[2]]
    roc_auc_all_2_idr = [roc_auc_svm_idr[2], roc_auc_knn_idr[2], roc_auc_rf_idr[2]]
    for i_idr, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_2_idr[i_idr], tpr_all_2_idr[i_idr], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i_idr], roc_auc_all_2_idr[i_idr]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 2 (misdetection), Negative: 0, 1, 3-5, idr')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')
    '''
    # class 3: SYMMETRIC
    '''
    merged_3_df = merged_df.loc[[3, 9, 15], :]
    merged_3_df['accuracy'] = classifr_accuracy
    merged_3_df['auc'] = [classifr_auc[0][3], classifr_auc[1][3], classifr_auc[1][3]]

    # idr
    merged_3_df_idr = merged_df_idr.loc[[3, 9, 15], :]
    merged_3_df_idr['accuracy'] = classifr_accuracy_idr
    merged_3_df_idr['auc'] = [classifr_auc_idr[0][3], classifr_auc_idr[1][3], classifr_auc_idr[1][3]]

    # Metrics bar plot
    svm_3 = merged_3_df.loc[3, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_3 = merged_3_df.loc[9, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_3 = merged_3_df.loc[15, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    index = ['precision', 'recall', 'F1 score', 'accuracy', 'auc']

    # Metrics bar plot, idr
    svm_3_idr = merged_3_df_idr.loc[3, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_3_idr = merged_3_df_idr.loc[9, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_3_idr = merged_3_df_idr.loc[15, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()

    plotdata = pd.DataFrame({'svm_3': svm_3, 'knn_3': knn_3,'rf_3': rf_3}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 3 (symmetric)')
    plt.show()
    plt.close('all')

    # idr
    plotdata_idr = pd.DataFrame({'svm_3': svm_3_idr, 'knn_3': knn_3_idr,'rf_3': rf_3_idr}, index=index)
    plotdata_idr.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 3 (symmetric), idr')
    plt.show()
    plt.close('all')
    '''
    # ROC curves
    fpr_all_3 = [fpr_svm[1], fpr_knn[1], fpr_rf[1]]
    tpr_all_3 = [tpr_svm[1], tpr_knn[1], tpr_rf[1]]
    roc_auc_all_3 = [roc_auc_svm[1], roc_auc_knn[1], roc_auc_rf[1]]

    # ROC curves, idr
    fpr_all_3_idr = [fpr_svm_idr[1], fpr_knn_idr[1], fpr_rf_idr[1]]
    tpr_all_3_idr = [tpr_svm_idr[1], tpr_knn_idr[1], tpr_rf_idr[1]]
    roc_auc_all_3_idr = [roc_auc_svm_idr[1], roc_auc_knn_idr[1], roc_auc_rf_idr[1]]

    lw = 2
    methods = ['svm', 'knn', 'rf']
    colors = cycle(['royalblue','darkcyan','goldenrod'])
    for i, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_3[i], tpr_all_3[i], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i], roc_auc_all_3[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 3 (symmetric), Negative: 0-2, 4, 5')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    # idr
    for i_idr, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_3_idr[i_idr], tpr_all_3_idr[i_idr], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i_idr], roc_auc_all_3_idr[i_idr]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 3 (symmetric), Negative: 0-2, 4, 5, idr')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    # class 4: ASYMMETRIC
    '''
    merged_4_df = merged_df.loc[[4, 10, 16], :]
    merged_4_df['accuracy'] = classifr_accuracy
    merged_4_df['auc'] = [classifr_auc[0][4], classifr_auc[1][4], classifr_auc[2][4]]

    #idr
    merged_4_df_idr = merged_df_idr.loc[[4, 10, 16], :]
    merged_4_df_idr['accuracy'] = classifr_accuracy_idr
    merged_4_df_idr['auc'] = [classifr_auc_idr[0][4], classifr_auc_idr[1][4], classifr_auc_idr[2][4]]

    # Metrics bar plot
    svm_4 = merged_4_df.loc[4, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_4 = merged_4_df.loc[10, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_4 = merged_4_df.loc[16, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    index = ['precision', 'recall', 'F1 score', 'accuracy', 'auc']

    # idr
    # Metrics bar plot
    svm_4_idr = merged_4_df_idr.loc[4, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_4_idr = merged_4_df_idr.loc[10, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_4_idr = merged_4_df_idr.loc[16, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()

    plotdata = pd.DataFrame({'svm_4': svm_4, 'knn_4': knn_4,'rf_4': rf_4}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 4 (asymmetric)')
    plt.show()
    plt.close('all')

    # idr
    plotdata_idr = pd.DataFrame({'svm_4': svm_4_idr, 'knn_4': knn_4_idr,'rf_4': rf_4_idr}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 4 (asymmetric), idr')
    plt.show()
    plt.close('all')
    '''
    # ROC curves
    fpr_all_4 = [fpr_svm[2], fpr_knn[2], fpr_rf[2]]
    tpr_all_4 = [tpr_svm[2], tpr_knn[2], tpr_rf[2]]
    roc_auc_all_4 = [roc_auc_svm[2], roc_auc_knn[2], roc_auc_rf[2]]
    lw = 2
    methods = ['svm', 'knn', 'rf']
    colors = cycle(['royalblue','darkcyan','goldenrod'])
    for i, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_4[i], tpr_all_4[i], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i], roc_auc_all_4[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 4 (asymmetric), Negative: 0-3, 5')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    # idr
    # ROC curves
    fpr_all_4_idr = [fpr_svm_idr[2], fpr_knn_idr[2], fpr_rf_idr[2]]
    tpr_all_4_idr = [tpr_svm_idr[2], tpr_knn_idr[2], tpr_rf_idr[2]]
    roc_auc_all_4_idr = [roc_auc_svm_idr[2], roc_auc_knn_idr[2], roc_auc_rf_idr[2]]

    for i_idr, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_4_idr[i_idr], tpr_all_4_idr[i_idr], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i_idr], roc_auc_all_4_idr[i_idr]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 4 (asymmetric), Negative: 0-3, 5, idr')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')


    # class 5: UNCLEAR
    '''
    merged_5_df = merged_df.loc[[5, 11, 17], :]
    merged_5_df['accuracy'] = classifr_accuracy
    merged_5_df['auc'] = [classifr_auc[0][5], classifr_auc[1][5], classifr_auc[2][5]]

    merged_5_df_idr = merged_df_idr.loc[[5, 11, 17], :]
    merged_5_df_idr['accuracy'] = classifr_accuracy
    merged_5_df_idr['auc'] = [classifr_auc_idr[0][5], classifr_auc_idr[1][5], classifr_auc_idr[2][5]]


    # Metrics bar plot
    svm_5 = merged_5_df.loc[5, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_5 = merged_5_df.loc[11, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_5 = merged_5_df.loc[17, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    index = ['precision', 'recall', 'F1 score', 'accuracy', 'auc']

    plotdata = pd.DataFrame({'svm_5': svm_5, 'knn_5': knn_5, 'rf_5': rf_5}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 5 (unclear)')
    plt.show()
    plt.close('all')

    # idr
    # Metrics bar plot
    svm_5_idr = merged_5_df_idr.loc[5, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    knn_5_idr = merged_5_df_idr.loc[11, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()
    rf_5_idr = merged_5_df_idr.loc[17, ['precision', 'recall', 'f1_score', 'accuracy', 'auc']].tolist()

    plotdata_idr = pd.DataFrame({'svm_5': svm_5_idr, 'knn_5': knn_5_idr, 'rf_5': rf_5_idr}, index=index)
    plotdata.plot(kind="bar", rot=0, alpha=0.75).legend(title="Classifiers",
                                                        fancybox=True)  # bbox_to_anchor=(0.91,0.827)
    plt.xlabel(" Metrics ")
    plt.ylabel(" Score ")
    plt.title('Metrics for class 5 (unclear), idr')
    plt.show()
    plt.close('all')
    '''
    # ROC curves
    fpr_all_5 = [fpr_svm[3], fpr_knn[3], fpr_rf[3]]
    tpr_all_5 = [tpr_svm[3], tpr_knn[3], tpr_rf[3]]
    roc_auc_all_5 = [roc_auc_svm[3], roc_auc_knn[3], roc_auc_rf[3]]
    for i, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_5[i], tpr_all_5[i], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i], roc_auc_all_5[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 5 (midway), Negative: 0-4 ')
    plt.legend(loc="lower right")
    plt.show()
    plt.close('all')

    #idr
    fpr_all_5_idr = [fpr_svm_idr[3], fpr_knn_idr[3], fpr_rf_idr[3]]
    tpr_all_5_idr = [tpr_svm_idr[3], tpr_knn_idr[3], tpr_rf_idr[3]]
    roc_auc_all_5_idr = [roc_auc_svm_idr[3], roc_auc_knn_idr[3], roc_auc_rf_idr[3]]
    for i_idr, color in zip(range(len(methods)), colors):
        plt.plot(fpr_all_5_idr[i_idr], tpr_all_5_idr[i_idr], color=color, lw=lw,
                 label=' {0} (AUC = {1:0.2f})'
                       ''.format(methods[i_idr], roc_auc_all_5_idr[i_idr]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Positive: 5 (midway), Negative: 0-4, idr')
    plt.legend(loc="lower right")
    plt.show()

    stop = 1

    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- CLASSIFICATION EVALUATION
    # ******************************************************

    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST
    # ******************************************************************************


    #******************************************************************************

    #******************************************************************************

    # -------- BINARIZING CLASSES
    # roc curve
    statuses = df_foci_simple['status'].unique()
    statuses.sort()

    X_raw_ = df_foci_simple.drop(
        ['Unnamed: 0', 'protein', 'exp_folder', 'roi_id', 'focus_label', 'major_axis_length', 'orientation',
         'compare_val_correl', 'compare_val_kl', 'status', 'centroid-0', 'centroid-1', 'coords', 'bbox'], axis=1).copy()
    Y_raw_ = df_foci_simple['status'].copy()
    # keep one class as true
    # Y_raw_[Y_raw_ != 3] = 0

    Y_raw_bin = label_binarize(Y_raw_, statuses)

    # split into train and test sets
    X_train_, X_test_, Y_train_, Y_test_ = train_test_split(X_raw, Y_raw_bin, test_size=0.3,
                                                            random_state=42)  # random_state=0
    statuses_bin = np.unique(Y_raw_bin, axis=0)
    n_classes = len(statuses_bin)

    # Learn to predict each class against the other
    classifier = OneVsRestClassifier(svm.SVC(kernel='rbf'))  # probability=True, OneVsRestClassifier
    # y_score = classifier.fit(X_train_, Y_train_).score(X_train_, Y_train_)

    y_score = classifier.fit(X_train_, Y_train_).decision_function(X_test_)
    '''

    classifier.predict([X_train_])
    # optimize parameters
    # C: regularization parameter
    # gamma:
    param_grid = {'estimator__C': [0.01, 0.1, 1, 10, 100, 1000],
                  'estimator__gamma': ['scale', 100, 10, 1, 0.1, 0.01, 0.001, 0.0001],
                  'estimator__kernel': ['rbf']}

    grid = GridSearchCV(OneVsRestClassifier(svm.SVC()), param_grid,
                                cv = 5,
                                scoring= 'accuracy',
                                verbose = 3)
    grid.fit(X_train_, Y_train_)
    print(grid.best_params_)
    print(grid.best_estimator_)
    grid_predictions = grid.predict(X_test_)
    print(classification_report(Y_test_, grid_predictions))
    '''

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    y_test = Y_test_
    for i in range(n_classes):
        TEST = y_test[:, i]
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
        roc_auc[i] = metrics.auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
    roc_auc["micro"] = metrics.auc(fpr["micro"], tpr["micro"])

    plt.figure()
    lw = 2
    '''
    plt.plot(fpr[2], tpr[2], color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    #plt.xlim([0.0, 1.0])
    #plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()
    '''

    # First aggregate all false positive rates
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

    # Then interpolate all ROC curves at this points
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(n_classes):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

    # Finally average it and compute AUC
    mean_tpr /= n_classes

    fpr["macro"] = all_fpr
    tpr["macro"] = mean_tpr
    roc_auc["macro"] = metrics.auc(fpr["macro"], tpr["macro"])

    # Plot all ROC curves
    plt.figure()
    plt.plot(fpr["micro"], tpr["micro"],
             label='micro-average ROC curve (area = {0:0.2f})'
                   ''.format(roc_auc["micro"]),
             color='deeppink', linestyle=':', linewidth=4)

    plt.plot(fpr["macro"], tpr["macro"],
             label='macro-average ROC curve (area = {0:0.2f})'
                   ''.format(roc_auc["macro"]),
             color='navy', linestyle=':', linewidth=4)

    colors = itertools.cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])
    for i, color in zip(range(n_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=lw,
                 label='ROC curve of class' + str(i + 1) + '(area = {1:0.2f})'
                                                           ''.format(i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Some extension of Receiver operating characteristic to multi-class')
    plt.legend(loc="lower right")
    plt.show()

    # scale data to speed up svm
    X_train_scaled_ = scale(X_train_)
    X_test_scaled_ = scale(X_test_)

    # ----
    # define classifier
    clf_svm = SVC(random_state=42)
    multilabel_classifier = MultiOutputClassifier(clf_svm, n_jobs=-1)
    multilabel_classifier = multilabel_classifier.fit(X_train_scaled, Y_train)
    # train classifier
    clf_svm.fit(X_train_scaled, Y_train)
    svm_scores = cross_val_score(clf_svm, X_train_scaled, Y_train, cv=10)
    accrcy = []
    accrcy_std = []
    accrcy.append(svm_scores.mean())
    accrcy_std.append(svm_scores.std() * 2)

    # plt.show()
    plt.close()

    # implement optimization
    # define classifier
    clf_svm = SVC(random_state=42, C=100, gamma=0.01, kernel='rbf')

    # train classifier
    clf_svm.fit(X_train_scaled, Y_train)
    svm_scores = cross_val_score(clf_svm, X_train_scaled, Y_train, cv=10)
    accrcy.append(svm_scores.mean())
    accrcy_std.append(svm_scores.std() * 2)

    # predict
    Y_pred = clf_svm.predict(X_test_scaled)

    # evaluate
    accrcy.append(metrics.accuracy_score(Y_test, Y_pred))
    print(classification_report(Y_test, Y_pred))  # target_names= class_names
    # metrics.plot_roc_curve(clf_svm, X_test_scaled, Y_test)
    # plt.show()
    # random forest -------------------------------
    feature_names_ = X_train.columns
    status_classes_ = df_foci_simple['status'].unique()
    class_names_ = np.char.mod('%d', status_classes_)

    model = RandomForestClassifier(n_estimators=10, random_state=30)
    model.fit(X_train_scaled, Y_train)

    rf_accrcy = []
    rf_accrcy_std = []
    rf_scores = cross_val_score(model, X_train_scaled, Y_train, cv=10)
    rf_accrcy.append(rf_scores.mean())
    rf_accrcy_std.append(rf_scores.std() * 2)

    '''
    # rf hyperparameter optimization 
 # estimators 200,500
    param_grid = {
        'n_estimators': [10, 50],
        'max_features': ['auto', 'sqrt', 'log2'],
        'max_depth': [4, 5, 6, 7, 8],
        'criterion': ['gini', 'entropy']
    }

    CV_rfc = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5)
    CV_rfc.fit(X_train_scaled, Y_train)
    best_params = CV_rfc.best_params_
    '''
    rfc1 = RandomForestClassifier(random_state=42, max_features='auto', n_estimators=50, max_depth=7, criterion='gini')
    rfc1.fit(X_train_scaled, Y_train)
    rf_scores = cross_val_score(rfc1, X_train_scaled, Y_train, cv=10)
    rf_accrcy.append(rf_scores.mean())
    rf_accrcy_std.append(rf_scores.std() * 2)

    # predict test set
    rf_pred = rfc1.predict(X_test_scaled)
    rf_accrcy.append(accuracy_score(Y_test, rf_pred))

    rf_pred = rfc1.predict(X_test_scaled)
    print(classification_report(Y_test, rf_pred))  # target_names= class_names

    # First aggregate all false positive rates
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

    metrics.plot_roc_curve(rfc1, X_test_scaled, Y_test)
    # plt.show()
    plt.figure(0).clf()
    pred_svm = clf_svm.predict(X_test_scaled)
    fpr, tpr, thresh = metrics.roc_curve(Y_test, pred_svm, pos_label=float(3))
    auc = metrics.roc_auc_score(Y_test, pred_svm)
    plt.plot(fpr, tpr, label="SVM, auc=" + str(auc))

    pred_rf = rfc1.predict(X_test_scaled)
    fpr, tpr, thresh = metrics.roc_curve(Y_test, pred_rf, pos_label=float(3))
    auc = metrics.roc_auc_score(Y_test, pred_rf)
    plt.plot(fpr, tpr, label="RF, auc=" + str(auc))
    plt.legend(loc=0)
    plt.show()

    # Extract single tree
    estimator = rfc2.estimators_[5]

    fig = plt.figure(figsize=(15, 10))
    plot_tree(rfc2.estimators_[0],
              feature_names=feature_names_,
              class_names=class_names_,
              filled=True, impurity=True,
              rounded=True)
    fig.savefig('rf_tree.png')
    plt.close()

    forest_scores = cross_val_score(rfc2, X_train, Y_train, cv=10)
    forest_scores.mean()

    # compare svm and rf
    plt.figure(figsize=(8, 4))
    plt.plot([1] * 10, svm_scores, ".")
    plt.plot([2] * 10, forest_scores, ".")
    plt.boxplot([svm_scores, forest_scores], labels=("SVM", "Random Forest"))
    plt.ylabel("Accuracy", fontsize=14)
    plt.show()

    plt.close()
    feature_list = list(X.columns)
    feature_imp = pd.Series(model.feature_importances_, index=feature_list).sort_values(ascending=False)
    print(feature_imp)

    # classify foci with svm
    classified_foci = focus_classifier(df_foci_props)
    df_foci_classified = df_foci_full
    df_foci_classified['focus_class'] = classified_foci
    df_foci = df_foci_classified[df_foci_classified['focus_class'] == 1]
    df_foci = df_foci.reset_index()

    # draw on classified detected foci with SVM, contours only
    for focus_SVM_i, focus_SVM_row in df_foci.iterrows():
        draw_on_img(file, file_root, b, focus_SVM_row['bbox'], focus_SVM_row['coords'], center_coordinates, box, 1, 1,
                    (255, 255, 255), draw_bbox=False,
                    draw_contours=True, draw_label=True, label=str(focus_SVM_row['focus_label']))  # str(i_SVM)
    io.imsave(roi_file[:-4] + '_focusDetectionCheck_SVM.tif', b.astype('uint8'))

    to_append = dir_[:5]
    to_append.append(roi_count + 1)
    summary_rows.append(to_append)

def multiclass_problems(norm_stand_train, norm_stand_test, df_foci_name, df_foci_idr_name, props, multiclass_metrics):
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    save_dir = base_dir + '/Results/' + time_dir + '/MulticlassProblems'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)
    #******************************************************************************
    # START : LOAD PROPERTIES
    datasets = load_properties()
    df_foci_v0 = norm_stand_train
    df_foci_idr_v0 = norm_stand_test
    df_foci_name = df_foci_name
    df_foci_idr_name = df_foci_idr_name

    df_foci = df_foci_v0.copy()
    df_foci_idr = df_foci_idr_v0
    df_foci_og = df_foci

    iloc_rows = []

    df_foci_simple = df_foci_og.copy()
    df_foci_simple_idr = df_foci_idr.copy()
    # END : LOAD PROPERTIES
    #******************************************************************************

    # ******************************************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST
    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA

    # organize data for classification
    # Y raw
    Y_raw_clsfrs = df_foci['status'].copy()
    Y_raw_clsfrs_idr = df_foci_idr['status'].copy()

    # X raw
    X_raw_clsfrs = df_foci.drop(columns = 'status', axis=1).copy()
    X_raw_clsfrs_idr = df_foci_idr.drop(columns = 'status', axis=1).copy()

    # weights
    from sklearn.utils import class_weight
    class_weight = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(Y_raw_clsfrs), y = Y_raw_clsfrs)

    # binarize the output
    classes = [1, 3, 4, 5]
    lb = preprocessing.LabelBinarizer()
    no_classes = len(classes)
    # train
    Y_raw_clsfrs = lb.fit_transform(Y_raw_clsfrs)
    n_classes_clsfrs = Y_raw_clsfrs.shape[1]
    # test
    Y_raw_clsfrs_idr = lb.fit_transform(Y_raw_clsfrs_idr)
    n_classes_clsfrs_idr = Y_raw_clsfrs_idr.shape[1]

    # split into train and test sets
    X_train_clsfrs, X_test_clsfrs, Y_train_clsfrs, Y_test_clsfrs = train_test_split(X_raw_clsfrs, Y_raw_clsfrs,
                                                                                    test_size=0.3,
                                                                                    random_state=42)  # random_state=0
    # inverse binarization
    Y_train_clsfrs_nobin = lb.inverse_transform(Y_train_clsfrs)
    Y_raw_clsfrs_idr_nobin = lb.inverse_transform(Y_raw_clsfrs_idr)
    # scale data to speed up classification
    X_train_scaled_clsfrs = X_train_clsfrs # scale(X_train_clsfrs)
    X_test_scaled_clsfrs = X_test_clsfrs # scale(X_test_clsfrs)
    X_scaled_clsfrs_idr = X_raw_clsfrs_idr # scale(X_raw_clsfrs_idr)

    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA
    # ******************************************************

    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- NO BINARY LABELS -- CROSS VALIDATED METRICS
    start_time = time.time()
    score_options = sorted(sklearn.metrics.SCORERS.keys())
    # Cross validated metrics
    model_svm = svm.SVC(kernel='rbf', probability=True, class_weight = 'balanced')
    model_knn = sklearn.neighbors.KNeighborsClassifier(metric= 'manhattan', n_neighbors= 15, weights= 'distance')
    model_rf = RandomForestClassifier(class_weight = 'balanced')

    models = [model_svm, model_knn, model_rf]
    X_Y_data = [[X_train_scaled_clsfrs, Y_train_clsfrs_nobin]]
    dataset_labels = ['train']
    dataset_names = [df_foci_name, df_foci_idr_name]
    model_labels = ['KNN','RF', 'SVM']
    all_scores = ('f1_weighted','precision_weighted','recall_weighted')
    cv_scores_full = []
    dataset_labels_ = []
    model_labels_ = []
    # compute cv scores, label lists
    for i_model in range(len(models)): #len(models)
        for i_data, label in enumerate(dataset_labels): #range(len(X_Y_data)): # len(X_Y_data)
            cv_scores = cross_validate(models[i_model], X_Y_data[i_data][0],  X_Y_data[i_data][1], cv=10, scoring = all_scores)
            cv_scores_full.append(cv_scores)
            # create label lists
            dataset_labels_.append([dataset_labels[i_data]]*10)
            model_labels_.append([model_labels[i_model]]*10)
            del cv_scores

    # organize labels into dataframe
    dataset_labels_l = list(itertools.chain.from_iterable(dataset_labels_))
    model_labels_l = list(itertools.chain.from_iterable(model_labels_))
    df_scores_full = pd.DataFrame(dataset_labels_l, columns= ['dataset'])
    df_scores_full['model'] = model_labels_l

    # organize scores into df columns
    for i_score in all_scores:
        score_label = 'test_' + i_score
        scores_full = []
        for i_cv_scores in cv_scores_full:
            cv_score = i_cv_scores[score_label]
            for i_cv_score in cv_score:
                scores_full.append(i_cv_score)
        df_scores_full[i_score] = scores_full # add scores to df

    multiclass_metrics_ = multiclass_metrics.sort_values(by = 'model', ascending = True).reset_index()

    # plot and save boxplots
    for i_plot in all_scores:
        plt.close()
        fig, ax = plt.subplots()
        bp = df_scores_full.boxplot(by = 'model', column = [i_plot] , grid=True, positions=[0,1,2],
            return_type='both', patch_artist = True, ax = ax,
            boxprops= dict(linewidth=3.0, color='black'),
            whiskerprops= dict(linestyle='-', linewidth= 3.0, color='black'),
            medianprops=dict(linestyle = '-',  linewidth = 3, color= 'black'),
            capprops = dict(linestyle = '-',  linewidth = 3, color= 'black'),
            flierprops = dict(marker = 'D', linewidth = 3, markerfacecolor= 'black'))
        for row_key, (ax,row) in bp.iteritems():
            for i,box in enumerate(row['boxes']):
                box.set_facecolor('steelblue')
        for count, i in enumerate(model_labels):
            y = multiclass_metrics_[i_plot][count]
            print(y)
            x = i
            print(x)
            ax.plot(x,y, 'r.', markersize=9, alpha= 0.5, color='olivedrab', marker='o') # plt.
        train_scores = df_scores_full[i_plot].to_list()
        test_scores = multiclass_metrics_[i_plot].to_list()
        full_scores = train_scores + test_scores
        full_scores = [round(elem, 2) for elem in full_scores]
        min_score = min(full_scores)
        max_score = max(full_scores)
        step = round(((max_score-min_score)/5)/2, 2)*2
        min_score = round(round((min_score - step)/2, 2)*2,1)
        max_score = round(round((max_score + 2*step)/2, 2)*2,1)
        ylabels = np.linspace(min_score, max_score, 5, endpoint = True).round(2)#np.arange(min_scores, max_scores, step)
        ax.yaxis.set_ticks(ylabels)
        plt.savefig(save_dir + '/' + i_plot + '_MulticlassProblems_cv-boxplot_' + time_dir + '.pdf')

    plt.close()
    end_time = time.time()
    total_time = end_time - start_time
    multiclass_metrics_.to_csv((save_dir + '/' + 'multiclass_metrics_' + time_dir + '.csv'))
    df_scores_full.to_csv((save_dir + '/' + 'df_scores_full_' + time_dir + '.csv'))

    print('Multiclass problems completed.')
    # END: CLASSIFICATION -- IMPLEMENTATION -- NO BINARY LABELS -- CROSS VALIDATED METRICS
    # ******************************************************

def metric_dependancy(norm_stand_train, norm_stand_test, df_foci_name, df_foci_idr_name, props, blur_props, sym_props):
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    save_dir = base_dir + '/Results/' + time_dir +'/MetricDependancies'
    if (not os.path.isdir(save_dir)):
        os.mkdir(save_dir)

    # ******************************************************************************
    # timer
    start_time = time.time()
    # ******************************************************************************
    # START : LOAD PROPERTIES

    datasets = load_properties()
    df_foci = norm_stand_train
    df_foci_idr = norm_stand_test
    df_foci_name = df_foci_name
    df_foci_idr_name = df_foci_idr_name
    df_foci_v0 = df_foci.copy()

    # END : LOAD PROPERTIES
    # ******************************************************************************

    # ******************************************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION
    # ******************************************************
    # START: CLASSIFICATION -- IMPLEMENTATION -- ORGANIZE DATA
    df_foci = df_foci.reset_index(drop=True)
    df_foci_idr = df_foci_idr.reset_index(drop=True)

    # select classes
    # sym
    df_foci_sym = df_foci.loc[(df_foci['status'] == 3) | (df_foci['status'] == 4)]
    df_foci_idr_sym = df_foci_idr.loc[(df_foci_idr['status'] == 3) | (df_foci_idr['status'] == 4)]
    # blur
    df_foci_blur = df_foci.loc[(df_foci['status'] == 1) | (df_foci['status'] == 3)]
    df_foci_idr_blur = df_foci_idr.loc[(df_foci_idr['status'] == 1) | (df_foci_idr['status'] == 3)]

    # Y data
    # sym
    Y_sym = df_foci_sym['status'].copy()
    Y_idr_sym = df_foci_idr_sym['status']
    # blur
    Y_blur = df_foci_blur['status'].copy()
    Y_idr_blur = df_foci_idr_blur['status'].copy()

    # X data
    # sym
    X_sym = df_foci_sym.drop(columns = 'status', axis=1).copy()
    X_idr_sym = df_foci_idr_sym.drop(columns = 'status', axis=1).copy()
    # blur
    X_blur = df_foci_blur.drop(columns = 'status', axis=1).copy()
    X_idr_blur = df_foci_idr_blur.drop(columns = 'status', axis=1).copy()

    # sym
    drop_props_sym = sym_props
    X_sym_drop = X_sym.drop(drop_props_sym, axis=1).copy()
    X_idr_sym_drop = X_idr_sym.drop(drop_props_sym, axis=1).copy()

    # blur
    drop_props_blur = blur_props
    X_blur_drop = X_blur.drop(drop_props_blur, axis=1).copy()
    X_idr_blur_drop = X_idr_blur.drop(drop_props_blur, axis=1).copy()

    # END: CLASSIFICATION -- IMPLEMENTATION -- ONE V. REST -- ORGANIZE DATA
    # ******************************************************

    # ******************************************************
    # --------------- START: TEST: EFFECTIVENESS OF SYMMETRY/BLUR METRIC
    # START: CLASSIFICATION -- IMPLEMENTATION -- NO BINARY LABELS -- CROSS VALIDATED METRICS

    # Cross validated metrics
    model_svm = svm.SVC(kernel='rbf', probability=True, class_weight='balanced')
    model_knn = sklearn.neighbors.KNeighborsClassifier(metric='manhattan', n_neighbors=15, weights='distance') # weights = 'uniform'
    model_rf = RandomForestClassifier(class_weight='balanced')

    models = [model_svm, model_knn, model_rf]
    metrics = ['+ sym', '- sym', '+ blur', '- blur']
    sym_data = [[X_sym, Y_sym],[X_idr_sym, Y_idr_sym]]
    sym_data_drop = [[X_sym_drop, Y_sym],[X_idr_sym_drop, Y_idr_sym]]
    blur_data = [[X_blur, Y_blur],[X_idr_blur, Y_idr_blur]]
    blur_data_drop = [[X_blur_drop, Y_blur],[X_idr_blur_drop, Y_idr_blur]]
    X_Y_data = [sym_data, sym_data_drop, blur_data, blur_data_drop]
    dataset_labels = [df_foci_name, df_foci_idr_name]
    model_labels = ['SVM', 'KNN', 'RF']
    all_scores = ('f1_weighted', 'precision_weighted', 'recall_weighted')
    cv_scores_full = []
    metric_labels_ = []
    dataset_labels_ = []
    model_labels_ = []
    cv_k = 10

    '''
    # optimize property drop
    droppedprops_labels_ = []
    for i_prop in props:
        print(i_prop)
        Y = df_foci_idr_blur_['status'].copy()
        df = df_foci_idr_blur_.drop(columns = df_foci_idr_blur_.columns.difference(props), axis=1).copy() #df_foci_sym, df_foci_idr_sym, df_foci_blur, df_foci_idr_blur
        # #df_foci_sym_, df_foci_idr_sym_, df_foci_blur_, df_foci_idr_blur_
        df_drop = df.drop(columns = i_prop, axis=1).copy()
        dataset_labels = ['+sym', '-sym']
        data = [df, df_drop]
        for idx_data, i_data in enumerate(data): # metrics
            print(idx_data)
            for i_model in range(len(models)): # either of the 3 classifiers
                cv_scores = cross_validate(models[i_model], i_data, Y, cv=cv_k, scoring=all_scores[0])
                cv_scores_full.append(np.mean(cv_scores['test_score']))
                print(dataset_labels[idx_data])
                print(model_labels[i_model])
                # create label lists
                droppedprops_labels_.append(i_prop)
                dataset_labels_.append(dataset_labels[idx_data])
                model_labels_.append(model_labels[i_model])
                del cv_scores
    df_optimization = pd.DataFrame(cv_scores_full, columns=[all_scores[0]])
    df_optimization['dropped_prop'] = droppedprops_labels_
    df_optimization['dataset'] = dataset_labels_
    df_optimization['model'] = model_labels_

    # + sym
    df_wsym = df_optimization[df_optimization.dataset == '+sym']
    df_wsym_svm = df_wsym[df_wsym.model == 'SVM'].reset_index()
    df_wsym_knn = df_wsym[df_wsym.model == 'KNN'].reset_index()
    df_wsym_rf = df_wsym[df_wsym.model == 'RF'].reset_index()
    # - sym
    df_wosym = df_optimization[df_optimization.dataset == '-sym']
    df_wosym_svm = df_wosym[df_wosym.model == 'SVM'].reset_index()
    df_wosym_knn = df_wosym[df_wosym.model == 'KNN'].reset_index()
    df_wosym_rf = df_wosym[df_wosym.model == 'RF'].reset_index()

    difrnc_sym_svm = abs(df_wsym_svm[all_scores[0]].sub(df_wosym_svm[all_scores[0]],fill_value=0))
    difrnc_sym_knn = abs(df_wsym_knn[all_scores[0]].sub(df_wosym_knn[all_scores[0]],fill_value=0))
    difrnc_sym_rf = abs(df_wsym_rf[all_scores[0]].sub(df_wosym_rf[all_scores[0]],fill_value=0))

    df_sym_svm_ = pd.DataFrame(difrnc_sym_svm)
    df_sym_knn_ = pd.DataFrame(difrnc_sym_knn)
    df_sym_rf_ = pd.DataFrame(difrnc_sym_rf)

    df_sym_svm_['props'] = props
    df_sym_knn_['props'] = props
    df_sym_rf_['props'] = props

    df_sym_svm_['Rank'] = df_sym_svm_[all_scores[0]].rank(ascending = 0)
    df_sym_knn_['Rank'] = df_sym_knn_[all_scores[0]].rank(ascending = 0)
    df_sym_rf_['Rank'] = df_sym_rf_[all_scores[0]].rank(ascending = 0)

    df_sym_svm_ = df_sym_svm_.set_index('Rank')
    df_sym_knn_ = df_sym_knn_.set_index('Rank')
    df_sym_rf_ = df_sym_rf_.set_index('Rank')

    df_sym_svm_ = df_sym_svm_.sort_index()
    df_sym_knn_ = df_sym_knn_.sort_index()
    df_sym_rf_ = df_sym_rf_.sort_index()

    df_sym_svm_.to_csv(save_dir + '/' + 'df_idr_blur_svm_' + time_dir + '.csv')
    df_sym_knn_.to_csv(save_dir + '/' + 'df_blur_knn_' + time_dir + '.csv')
    df_sym_rf_.to_csv(save_dir + '/' + 'df_blur_rf_' + time_dir + '.csv')

    print('end.')
    print('' + props)
    '''

    # compute cv scores, label lists
    for i_metric in range(len(metrics)): # metrics
        i_data_metric = X_Y_data[i_metric]
        for i_model in range(len(models)): # either of the 3 classifiers
            for i_dataset in range(len(i_data_metric)): # train or test dataset
                print(i_data_metric)
                cv_scores = cross_validate(models[i_model], i_data_metric[i_dataset][0], i_data_metric[i_dataset][1], cv=cv_k, scoring=all_scores)
                cv_scores_full.append(cv_scores)
                # create label lists
                metric_labels_.append([metrics[i_metric]] * cv_k)
                dataset_labels_.append([dataset_labels[i_dataset]] * cv_k)
                model_labels_.append([model_labels[i_model]] * cv_k)
                del cv_scores
        # organize labels into dataframe
        dataset_labels_l = list(itertools.chain.from_iterable(dataset_labels_))
        model_labels_l = list(itertools.chain.from_iterable(model_labels_))
        metric_labels_l = list(itertools.chain.from_iterable(metric_labels_))
        df_scores_full = pd.DataFrame(dataset_labels_l, columns=['dataset'])
        df_scores_full['model'] = model_labels_l
        df_scores_full['metric'] = metric_labels_l

    # organize scores into df columns
    score_steps = [] # spacing for boxplot y-axes
    score_min = [] # boxplot y-axes limits
    score_max = []
    yticks = []
    for i_score in all_scores:
        score_label = 'test_' + i_score
        scores_full = []
        for i_cv_scores in cv_scores_full:
            cv_score = i_cv_scores[score_label]
            for i_cv_score in cv_score:
                scores_full.append(i_cv_score)

        min_score = min(scores_full)
        max_score = max(scores_full)
        step = round(((max_score-min_score)/5)/2, 2)*2
        min_score = round(round((min_score - step)/2, 2)*2,1)
        max_score = round(round((max_score + step)/2, 2)*2,1)
        yticks_score= np.linspace(min_score, max_score, 5, endpoint = True).round(2)
        score_steps.append(step)
        score_min.append(min_score)
        score_max.append(max_score)
        yticks.append(yticks_score)
        df_scores_full[i_score] = scores_full #[round(elem, 2) for elem in scores_full]  # add scores to df
        del min_score, max_score, step

    # organize data
    # train
    df_scores_train = df_scores_full[df_scores_full.dataset == dataset_labels[0]]
    df_scores_train_sym = df_scores_train[(df_scores_train.metric == '+ sym') | (df_scores_train.metric == '- sym')]
    df_scores_train_blur = df_scores_train[(df_scores_train.metric == '+ blur') | (df_scores_train.metric == '- blur')]

    # test
    df_scores_test = df_scores_full[df_scores_full.dataset == dataset_labels[1]]
    df_scores_test_sym = df_scores_test[(df_scores_test.metric == '+ sym') | (df_scores_test.metric == '- sym')]
    df_scores_test_blur = df_scores_test[(df_scores_test.metric == '+ blur') | (df_scores_test.metric == '- blur')]

    df_scores_train_sym.to_csv(save_dir + '/' + 'df_scores_train_sym_' + time_dir + '.csv')
    df_scores_train_blur.to_csv(save_dir + '/' + 'df_scores_train_blur_' + time_dir + '.csv')
    df_scores_test_sym.to_csv(save_dir + '/' + 'df_scores_test_sym_' + time_dir + '.csv')
    df_scores_test_blur.to_csv(save_dir + '/' + 'df_scores_test_blur_' + time_dir + '.csv')

    # all datasets
    datasets = [df_scores_train_sym, df_scores_train_blur, df_scores_test_sym, df_scores_test_blur]
    # plot and save boxplots for all datasets
    for i_dataset in datasets:
        i_dataset = i_dataset.reset_index()
        dataset_plt = i_dataset['dataset'][0]
        metric_plt = i_dataset['metric'][0]
        # boxplots
        for i_idx, i_plot in enumerate(all_scores):
            plt.close()
            score_plt = sns.catplot(x="model", y=i_plot, hue="metric", kind="box", data= i_dataset)
            # format the labels with f-strings
            for ax in score_plt.axes.flat:
                ax.set_yticks(yticks[i_idx])
                ax.yaxis.set_major_formatter(tkr.FuncFormatter(lambda y, p: f'{y:.2f}'))
            score_plt.figure.savefig(save_dir + '/' + i_plot + '_' + dataset_plt + '_' + metric_plt + '_boxplot_MetricDependancies' + time_dir + '.pdf')
        plt.close()

    end_time = time.time()
    total_time = end_time - start_time
    print('Metric dependancies completed.')

def focus_classification_full(summary_dir):
    datasets = load_properties()
    df_foci = datasets[0]
    df_foci_idr = datasets[1]
    df_foci_name = datasets[2].split('.')[0]
    df_foci_idr_name = datasets[3].split('.')[0]
    props = datasets[4]
    blur_props = datasets[5]
    sym_props = datasets[6]
    df_foci_full = datasets[7]
    df_foci_idr_full = datasets[8]

    tables(df_foci_full, df_foci_idr_full)

    focus_norm = focus_normalization(datasets)

    props.remove('tot_pix_no') # after focus normalization wrt the focus pixel number, remove from properties

    heatmaps(df_foci, df_foci_idr, df_foci_name, df_foci_idr_name, props)

    prop_norm_stndrztn = property_normalization_and_standardization(focus_norm[0], focus_norm[1], focus_norm[2], focus_norm[3], props)

    histograms(prop_norm_stndrztn[0], prop_norm_stndrztn[1], prop_norm_stndrztn[2], prop_norm_stndrztn[3], props, blur_props, sym_props)

    binary_probs = binary_problems(prop_norm_stndrztn[0], prop_norm_stndrztn[1], prop_norm_stndrztn[2], prop_norm_stndrztn[3], props)

    multiclass_problems(prop_norm_stndrztn[0], prop_norm_stndrztn[1], prop_norm_stndrztn[2], prop_norm_stndrztn[3], props, binary_probs)

    metric_dependancy(prop_norm_stndrztn[0], prop_norm_stndrztn[1], prop_norm_stndrztn[2], prop_norm_stndrztn[3], props, blur_props, sym_props)

    print('Full experiment completed.')


    ########################################################################################################################

func = 'focus_classification_full'
#base_dir = 'H:/2021_FociData' # 'C:/' 'H:/2021_FociData' #'Z:/rothenberglab/archive/Maria/2021_FociData'
base_dir = 'data'

sample_to_protein = {'1': 'NoNCS', '2': 'NCS', '3': 'DNAPKi', '4': 'ATMi', '5': 'DMSO'}
gen_info = []
col_names_gen = ['protein', 'exp_folder', 'roi_id', 'NumROIs', 'num_foci']
col_gen_info = []

foci_info = []
col_foci_info = []

'''
col_foci_info = ['protein', 'exp_folder', 'roi_id', 'focus_label', 'status', 'centroid-0', 'centroid-1', 'orientation',
                 'area',
                 'perimeter', 'major_axis_length',
                 'minor_axis_length', 'eccentricity', 'coords', 'bbox',
                 'compare_val_correl',
                 'compare_val_chisq', 'compare_val_intrsct', 'compare_val_bc',
                 'compare_val_chisqalt', 'compare_val_hellngr', 'compare_val_kl',
                 'var_lap', 'var_lap_gb', 'var_lap_mag', 'var_lap_mag_gb']
'''

if (func == 'get_dir_list'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        print(summary_dir, ':')
        for subdir_list in dirs_dict[summary_dir]:
            print(",".join(subdir_list))

elif (func == 'pre_process_optimization'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        summary_dir = '2022_DF_data'
        pre_process_movies_optimization(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'pre_process'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        pre_process_movies(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'pre_process_idr'):
    dirs_dict = get_dir_list(base_dir)
    summary_dir = '2022_idr_data'
    pre_process_movies_idr(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'pre_process_df'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        summary_dir = '2022_DF_data'
        pre_process_movies_df(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'measure_foci'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        measure_foci(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'measure_rois_foci'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        measure_rois_foci(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'measure_rois'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        measure_rois(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'measure_props'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        #summary_dir = '2022_idr_data'
        measure_props(base_dir + '/' + summary_dir, dirs_dict[summary_dir])
    print('Done.')

elif (func == 'measure_props_idr'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        summary_dir = '2022_idr_data'
        measure_props_idr(base_dir + '/' + summary_dir, dirs_dict[summary_dir])
    print('Done.')


elif (func == 'measure_props_df'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        summary_dir = '2022_DF_data'
        measure_props_df(base_dir + '/' + summary_dir, dirs_dict[summary_dir])
    print('Done.')

elif (func == 'focus_libraries'):

    dirs_dict = get_dir_list(base_dir)
    foci_0 = []
    foci_1 = []
    foci_2 = []
    foci_3 = []
    foci_4 = []
    foci_5 = []
    foci_0_outlines = []
    foci_1_outlines = []
    foci_2_outlines = []
    foci_3_outlines = []
    foci_4_outlines = []
    foci_5_outlines = []
    foci_info_0 = []
    foci_info_1 = []
    foci_info_2 = []
    foci_info_3 = []
    foci_info_4 = []
    foci_info_5 = []

    for summary_dir in dirs_dict.keys():
        focus_libraries(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    focus_classes = [1, 2, 3, 4, 5]
    focus_classes = [str(i) for i in focus_classes]
    foci_full = [foci_1, foci_2, foci_3, foci_4, foci_5]
    foci_outlines_full = [foci_1_outlines, foci_2_outlines, foci_3_outlines, foci_4_outlines, foci_5_outlines]
    lower_boundary = 50
    for i_reshape in range(len(foci_full)):
        foci_i = foci_full[i_reshape]
        foci_outlines_i = foci_outlines_full[i_reshape]
        # reshape foci
        # initialize row to create column
        new_col_cols = 90
        arr1 = np.zeros((1, new_col_cols), dtype=np.int)
        i_app = 0 # row
        # initialize column to add completed columns to
        rows = np.shape(foci_i[0])[0] * 26
        cols = 1
        s = np.zeros((rows, cols))
        s_empty = np.zeros((rows, cols))
        for i_ar in foci_i:
            if (i_app < 25): # add to new column
                new_col = np.concatenate((arr1, i_ar), axis=0)
                arr1 = new_col
                i_app = i_app + 1
                new_col_rows = np.shape(new_col)[0]
                if (new_col_rows > (rows - lower_boundary)): # stop adding to new column when it's getting close to limit
                    i_app = 25

            if (i_app == 25): # fill large column with new column
                i_app = 0
                new_col_rows = np.shape(new_col)[0]
                if (new_col_rows < rows): # pad new column until it reaches
                    padding = rows - new_col_rows
                    new_col = np.pad(new_col, ((0, padding), (0, 0)), 'constant')
                s = np.concatenate((s, new_col), axis = 1)
                arr1 = np.zeros((1, new_col_cols), dtype=np.int) # reinitialize new column

        cv2.imwrite(base_dir + '/FocusLibrary_0/foci_full_'+ focus_classes[i_reshape] + '_rawmaxproj.tif', s.astype('uint16'))

        # reshape outlined foci
        # initialize row to create column
        arr1_outlines = np.zeros((1, new_col_cols), dtype=np.int)
        i_app_out = 0
        # initialize column to add completed columns to
        s_outlines = np.zeros((rows, cols))
        for i_ar_out in foci_outlines_i:
            if (i_app_out < 25): # add to new column
                new_col_outlines = np.concatenate((arr1_outlines, i_ar_out), axis=0)
                arr1_outlines = new_col_outlines
                i_app_out = i_app_out + 1
                if (np.shape(new_col_outlines)[0] > (rows - lower_boundary)): # stop adding to new column when it's getting close to limit
                    i_app_out = 25

            if (i_app_out == 25): # fill large column with new column
                i_app_out = 0
                new_col_rows_out = np.shape(new_col_outlines)[0]
                if (new_col_rows_out < rows): # pad new column until it reaches
                    padding = rows - new_col_rows_out
                    new_col_outlines = np.pad(new_col_outlines, ((0, padding), (0, 0)), 'constant')
                s_outlines = np.concatenate((s_outlines, new_col_outlines), axis = 1)
                arr1_outlines = np.zeros((1, new_col_cols), dtype=np.int) # reinitialize new column
        cv2.imwrite(base_dir + '/FocusLibrary_0/outlined_foci_full_' + focus_classes[i_reshape] + '_rawmaxproj.tif', s_outlines.astype('uint16'))

    # save csv files

    df_foci_info_0_ = pd.DataFrame(foci_info_0)
    df_foci_info_1_ = pd.DataFrame(foci_info_1)
    df_foci_info_2_ = pd.DataFrame(foci_info_2)
    df_foci_info_3_ = pd.DataFrame(foci_info_3)
    df_foci_info_4_ = pd.DataFrame(foci_info_4)
    df_foci_info_5_ = pd.DataFrame(foci_info_5)

    df_foci_info_0_.to_csv(base_dir + '/FocusLibrary_0/' + 'foci_0_library_info.csv')
    df_foci_info_1_.to_csv(base_dir + '/FocusLibrary_0/' + 'foci_1_library_info.csv')
    df_foci_info_2_.to_csv(base_dir + '/FocusLibrary_0/' + 'foci_2_library_info.csv')
    df_foci_info_3_.to_csv(base_dir + '/FocusLibrary_0/' + 'foci_3_library_info.csv')
    df_foci_info_4_.to_csv(base_dir + '/FocusLibrary_0/' + 'foci_4_library_info.csv')
    df_foci_info_5_.to_csv(base_dir + '/FocusLibrary_0/' + 'foci_5_library_info.csv')

    print('Done.')


elif (func == 'focus_libraries_train'):
    dirs_dict = get_dir_list(base_dir)
    foci_0 = []
    foci_1 = []
    foci_2 = []
    foci_3 = []
    foci_4 = []
    foci_5 = []
    foci_0_outlines = []
    foci_1_outlines = []
    foci_2_outlines = []
    foci_3_outlines = []
    foci_4_outlines = []
    foci_5_outlines = []
    foci_info_0 = []
    foci_info_1 = []
    foci_info_2 = []
    foci_info_3 = []
    foci_info_4 = []
    foci_info_5 = []

    summary_dir = '2022_idr_data'
    classifier = 'knn'

    # create saving directory
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) +'_'+ str(current_time.month) + '_'+ str(current_time.day) +'_'+ str(current_time.hour)
    if (not os.path.isdir(base_dir + '/FocusLibrary_1/'+ time_dir)):
        os.mkdir(base_dir + '/FocusLibrary_1/' + time_dir)

    saving_dir = base_dir + '/FocusLibrary_1/' + time_dir

    focus_libraries_train(base_dir)
    focus_classes = [0, 1, 2, 3, 4, 5]
    focus_classes = [str(i) for i in focus_classes]
    foci_full = [foci_0, foci_1, foci_2, foci_3, foci_4, foci_5]
    foci_outlines_full = [foci_0_outlines, foci_1_outlines, foci_2_outlines, foci_3_outlines, foci_4_outlines, foci_5_outlines]
    lower_boundary = 65
    for i_reshape in range(len(foci_full)): # iterate over focus class
        foci_i = foci_full[i_reshape] # pick focus class
        if (len(foci_i) < 1 ):
            continue
        foci_outlines_i = foci_outlines_full[i_reshape]
        print(i_reshape)
        # reshape foci
        # initialize row to create column
        new_col_cols = 90
        arr1 = np.zeros((1, new_col_cols), dtype=np.int)
        i_app = 0 # row
        # initialize column to add completed columns to
        rows = np.shape(foci_i[0])[0] * 26
        cols = 1
        s = np.zeros((rows, cols))
        s_empty = np.zeros((rows, cols))
        i_count = 0
        for i_ar in foci_i:
            i_count = i_count + 1
            print('Count: ' + str(i_count))
            if (i_count == 424):
                stop = 1
            arr1_r = np.shape(arr1)[0]
            i_ar_r = np.shape(i_ar)[0]
            row_check = arr1_r + i_ar_r
            if ((i_app < 25)): # add to new column
                new_col = np.concatenate((arr1, i_ar), axis=0)
                arr1 = new_col
                i_app = i_app + 1
                new_col_rows = np.shape(new_col)[0]
                if (new_col_rows > (rows - lower_boundary)): # stop adding to new column when it's getting close to upper row limit
                    i_app = 25
            if ((i_app == 25) or (i_count == len(foci_i))): #  fill large column with new column
                i_app = 0
                new_col_rows = np.shape(new_col)[0]
                if (new_col_rows < rows): # pad new column until it reaches
                    padding = rows - new_col_rows
                    old_col = new_col
                    new_col = np.pad(new_col, ((0, padding), (0, 0)), 'constant')

                if (len(new_col) != len(s)): # shorten new column that has too many rows
                    col_dif = abs(len(new_col)-len(s))
                    new_col = np.delete(new_col, col_dif, 0)
                print('Length of new column: ' + str(len(new_col)))

                s = np.concatenate((s, new_col), axis = 1)
                arr1 = np.zeros((1, new_col_cols), dtype=np.int) # reinitialize new column

        # H:\2021_FociData\2022_idr_data\Output\FocusLibrary_idr_v1
        cv2.imwrite(saving_dir +'/'+ focus_classes[i_reshape] + classifier +'_rawmaxproj.tif', s.astype('uint16')) # change

        # reshape outlined foci
        # initialize row to create column
        arr1_outlines = np.zeros((1, new_col_cols), dtype=np.int)
        i_app_out = 0
        i_outlines_count = 0
        # initialize column to add completed columns to
        s_outlines = np.zeros((rows, cols))
        for i_ar_out in foci_outlines_i:
            i_outlines_count = i_outlines_count + 1
            if (i_app_out < 25): # add to new column
                new_col_outlines = np.concatenate((arr1_outlines, i_ar_out), axis=0)
                arr1_outlines = new_col_outlines
                i_app_out = i_app_out + 1
                if (np.shape(new_col_outlines)[0] > (rows - lower_boundary)): # stop adding to new column when it's getting close to limit
                    i_app_out = 25

            if ((i_app_out == 25) or (i_outlines_count == len(foci_outlines_i))): # fill large column with new column
                i_app_out = 0
                new_col_rows_out = np.shape(new_col_outlines)[0]
                if (new_col_rows_out < rows): # pad new column until it reaches
                    padding = rows - new_col_rows_out
                    new_col_outlines = np.pad(new_col_outlines, ((0, padding), (0, 0)), 'constant')
                if (len(new_col_outlines) != len(s_outlines)): # shorten new column that has too many rows
                    col_dif_outlines = abs(len(new_col_outlines)-len(s))
                    new_col_outlines = np.delete(new_col_outlines, col_dif_outlines, 0)
                s_outlines = np.concatenate((s_outlines, new_col_outlines), axis = 1)
                arr1_outlines = np.zeros((1, new_col_cols), dtype=np.int) # reinitialize new column
        cv2.imwrite(saving_dir +'/'+ focus_classes[i_reshape] + classifier + '_rawmaxproj_outlined.tif' + focus_classes[i_reshape] + '_raw.tif', s_outlines.astype('uint16')) # change

    # save csv files
    df_foci_info_0_ = pd.DataFrame(foci_info_0)
    df_foci_info_1_ = pd.DataFrame(foci_info_1)
    df_foci_info_2_ = pd.DataFrame(foci_info_2)
    df_foci_info_3_ = pd.DataFrame(foci_info_3)
    df_foci_info_4_ = pd.DataFrame(foci_info_4)
    df_foci_info_5_ = pd.DataFrame(foci_info_5)

    # change path
    df_foci_info_0_.to_csv(saving_dir +'/foci_0_library_info_'+ classifier + '_train.csv')
    df_foci_info_1_.to_csv(saving_dir +'/foci_1_library_info_'+ classifier + '_train.csv')
    df_foci_info_2_.to_csv(saving_dir +'/foci_2_library_info_'+ classifier + '_train.csv')
    df_foci_info_3_.to_csv(saving_dir +'/foci_3_library_info_'+ classifier + '_train.csv')
    df_foci_info_4_.to_csv(saving_dir +'/foci_4_library_info_'+ classifier + '_train.csv')
    df_foci_info_5_.to_csv(saving_dir +'/foci_5_library_info_'+ classifier + '_train.csv')

    print('Done.')


elif (func == 'focus_libraries_idr'):

    dirs_dict = get_dir_list(base_dir)
    foci_0 = []
    foci_1 = []
    foci_2 = []
    foci_3 = []
    foci_4 = []
    foci_5 = []
    foci_0_outlines = []
    foci_1_outlines = []
    foci_2_outlines = []
    foci_3_outlines = []
    foci_4_outlines = []
    foci_5_outlines = []
    foci_info_0 = []
    foci_info_1 = []
    foci_info_2 = []
    foci_info_3 = []
    foci_info_4 = []
    foci_info_5 = []

    summary_dir = '2022_idr_data'
    classifier = 'knn'
    # create saving directory
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) +'_'+ str(current_time.month) + '_'+ str(current_time.day) +'_'+ str(current_time.hour)
    if (not os.path.isdir(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir)):
        os.mkdir(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir)

    focus_libraries_idr(base_dir, summary_dir)
    focus_classes = [0, 1, 2, 3, 4, 5]
    focus_classes = [str(i) for i in focus_classes]
    foci_full = [foci_0, foci_1, foci_2, foci_3, foci_4, foci_5]
    foci_outlines_full = [foci_0_outlines, foci_1_outlines, foci_2_outlines, foci_3_outlines, foci_4_outlines, foci_5_outlines]
    lower_boundary = 65
    for i_reshape in range(len(foci_full)): # iterate over focus class
        foci_i = foci_full[i_reshape] # pick focus class
        if (len(foci_i) < 1 ):
            continue
        foci_outlines_i = foci_outlines_full[i_reshape]
        print(i_reshape)
        # reshape foci
        # initialize row to create column
        new_col_cols = 90
        arr1 = np.zeros((1, new_col_cols), dtype=np.int)
        i_app = 0 # row
        # initialize column to add completed columns to
        rows = np.shape(foci_i[0])[0] * 26
        cols = 1
        s = np.zeros((rows, cols))
        s_empty = np.zeros((rows, cols))
        i_count = 0
        for i_ar in foci_i:
            i_count = i_count + 1
            print('Count: ' + str(i_count))
            if (i_app < 25): # add to new column
                new_col = np.concatenate((arr1, i_ar), axis=0)
                arr1 = new_col
                i_app = i_app + 1
                new_col_rows = np.shape(new_col)[0]
                if (new_col_rows > (rows - lower_boundary)): # stop adding to new column when it's getting close to upper row limit
                    i_app = 25
            if ((i_app == 25) or (i_count == len(foci_i))): #  fill large column with new column
                i_app = 0
                new_col_rows = np.shape(new_col)[0]
                if (new_col_rows < rows): # pad new column until it reaches
                    padding = rows - new_col_rows
                    old_col = new_col
                    new_col = np.pad(new_col, ((0, padding), (0, 0)), 'constant')

                if (len(new_col) != len(s)): # shorten new column that has too many rows
                    col_dif = abs(len(new_col)-len(s))
                    new_col = np.delete(new_col, col_dif, 0)
                print('Length of new column: ' + str(len(new_col)))
                s = np.concatenate((s, new_col), axis = 1)
                arr1 = np.zeros((1, new_col_cols), dtype=np.int) # reinitialize new column


        cv2.imwrite(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/' + time_dir +'/'+ focus_classes[i_reshape] + classifier +'_rawmaxproj.tif', s.astype('uint16')) # change

        # reshape outlined foci
        # initialize row to create column
        arr1_outlines = np.zeros((1, new_col_cols), dtype=np.int)
        i_app_out = 0
        i_outlines_count = 0
        # initialize column to add completed columns to
        s_outlines = np.zeros((rows, cols))
        for i_ar_out in foci_outlines_i:
            i_outlines_count = i_outlines_count + 1
            if (i_app_out < 25): # add to new column
                new_col_outlines = np.concatenate((arr1_outlines, i_ar_out), axis=0)
                arr1_outlines = new_col_outlines
                i_app_out = i_app_out + 1
                if (np.shape(new_col_outlines)[0] > (rows - lower_boundary)): # stop adding to new column when it's getting close to limit
                    i_app_out = 25

            if ((i_app_out == 25) or (i_outlines_count == len(foci_outlines_i))): # fill large column with new column
                i_app_out = 0
                new_col_rows_out = np.shape(new_col_outlines)[0]
                if (new_col_rows_out < rows): # pad new column until it reaches
                    padding = rows - new_col_rows_out
                    new_col_outlines = np.pad(new_col_outlines, ((0, padding), (0, 0)), 'constant')
                if (len(new_col_outlines) != len(s_outlines)): # shorten new column that has too many rows
                    col_dif_outlines = abs(len(new_col_outlines)-len(s))
                    new_col_outlines = np.delete(new_col_outlines, col_dif_outlines, 0)
                s_outlines = np.concatenate((s_outlines, new_col_outlines), axis = 1)
                arr1_outlines = np.zeros((1, new_col_cols), dtype=np.int) # reinitialize new column
        cv2.imwrite(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/'+ focus_classes[i_reshape] + classifier+ '_rawmaxproj_outlined.tif' + focus_classes[i_reshape] + '_raw.tif', s_outlines.astype('uint16')) # change

    # save csv files
    df_foci_info_0_ = pd.DataFrame(foci_info_0)
    df_foci_info_1_ = pd.DataFrame(foci_info_1)
    df_foci_info_2_ = pd.DataFrame(foci_info_2)
    df_foci_info_3_ = pd.DataFrame(foci_info_3)
    df_foci_info_4_ = pd.DataFrame(foci_info_4)
    df_foci_info_5_ = pd.DataFrame(foci_info_5)

    # change path
    df_foci_info_0_.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/foci_0_library_info_'+ classifier + '_idr.csv')
    df_foci_info_1_.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/foci_1_library_info_'+ classifier + '_idr.csv')
    df_foci_info_2_.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/foci_2_library_info_'+ classifier + '_idr.csv')
    df_foci_info_3_.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/foci_3_library_info_'+ classifier + '_idr.csv')
    df_foci_info_4_.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/foci_4_library_info_'+ classifier + '_idr.csv')
    df_foci_info_5_.to_csv(base_dir + '/' + summary_dir + '/Output/FocusLibrary_idr_v1/'+ time_dir +'/foci_5_library_info_'+ classifier + '_idr.csv')

    print('Done.')

elif (func == 'full_focus_info'):
    dirs_dict = get_dir_list(base_dir)
    # summary_dir = list(get_dir_list(base_dir))[3]
    # full_focus_info(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    for summary_dir in dirs_dict.keys():
        #summary_dir = '2022_idr_data'
        full_focus_info(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    # save data
    df_gen_info = pd.DataFrame(gen_info, columns=col_names_gen)
    df_foci_info = pd.DataFrame(foci_info, columns=col_foci_info[0]) #col_foci_info[0]

    # save to file
    df_gen_info.to_csv(base_dir + '/gen_info.csv')
    df_foci_info.to_csv(base_dir + '/foci_info.csv')
    print('Done.')

elif (func == 'del_prev_status_idr'):
    dirs_dict = get_dir_list(base_dir)
    # summary_dir = list(get_dir_list(base_dir))[3]
    # full_focus_info(base_dir + '/' + summary_dir, dirs_dict[summary_dir])
    summary_dir = '2022_idr_data'
    del_prev_status_idr(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'full_focus_info_idr'):
    # directory to save
    current_time = datetime.datetime.now()
    time_dir = str(current_time.year) + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(
        current_time.hour)
    if (not os.path.isdir(base_dir + '/CSV_files/Unclassified/' + time_dir)):
        os.mkdir(base_dir + '/CSV_files/Unclassified/' + time_dir)

    dirs_dict = get_dir_list(base_dir)
    # summary_dir = list(get_dir_list(base_dir))[3]
    # full_focus_info(base_dir + '/' + summary_dir, dirs_dict[summary_dir])
    summary_dir = '2022_idr_data'
    full_focus_info_idr(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    # save data
    df_gen_info = pd.DataFrame(gen_info, columns=col_gen_info)
    test1 = len(foci_info)
    test2 = len(col_foci_info)
    df_foci_info = pd.DataFrame(foci_info, columns=col_foci_info) #col_foci_info[0]

    # save to file
    df_gen_info.to_csv(base_dir + '/CSV_files/Unclassified/' + time_dir + '/gen_info_idr_' + time_dir + '.csv')
    df_foci_info.to_csv(base_dir + '/CSV_files/Unclassified/' + time_dir + '/foci_info_idr_' + time_dir + '.csv')
    print('Done.')

elif (func == 'full_focus_info_df_twochannel'):
    dirs_dict = get_dir_list(base_dir)
    summary_dir = '2022_DF_data'
    dfs_foci_R = []
    dfs_foci_G = []
    dfs_gen_R = []
    dfs_gen_G = []
    full_focus_info_df_twochannel(base_dir + '/' + summary_dir, dirs_dict[summary_dir])
    # save data
    df_gen_info_R = pd.DataFrame(dfs_gen_R, columns = col_gen_info)
    df_gen_info_G = pd.DataFrame(dfs_gen_G, columns = col_gen_info)
    df_foci_info_R = pd.DataFrame(dfs_foci_R, columns = col_foci_info)
    df_foci_info_G = pd.DataFrame(dfs_foci_G, columns = col_foci_info)
    # save to file
    df_gen_info_R.to_csv(base_dir + '/' + summary_dir + '/gen_info_df_R.csv')
    df_gen_info_G.to_csv(base_dir + '/' + summary_dir + '/gen_info_df_G.csv')
    df_foci_info_R.to_csv(base_dir + '/' + summary_dir + '/foci_info_df_R.csv')
    df_foci_info_G.to_csv(base_dir + '/' + summary_dir + '/foci_info_df_G.csv')
    print('Done.')

elif (func == 'full_focus_info_df'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        full_focus_info_df(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    # save data
    df_gen_info = pd.DataFrame(gen_info, columns=col_gen_info)
    df_foci_info = pd.DataFrame(foci_info, columns=col_foci_info)

    # save to file
    df_gen_info.to_csv(base_dir  + '/' + summary_dir + '/gen_info_df.csv')
    df_foci_info.to_csv(base_dir + '/' + summary_dir  + '/foci_info_df.csv')
    print('Done.')

elif (func == 'heatmaps'):
    heatmaps(base_dir)

elif (func == 'exploration_plots'):
    exploration_plots(base_dir)

elif (func == 'focus_normalization'):
    focus_normalization(base_dir)

elif (func == 'property_normalization_and_standardization'):
    property_normalization_and_standardization(base_dir)

elif (func == 'histogram_plots_train_test'):
    histogram_plots_train_test(base_dir)

elif (func == 'histograms'):
    histograms(base_dir)

elif (func == 'deepfoci_histograms'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        deepfoci_histograms(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'classifier_optimization'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        classifier_optimization(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    else:
        print("Error: Unknown argument.")

elif (func == 'focus_tables'):
    dirs_dict = get_dir_list(base_dir)
    for summary_dir in dirs_dict.keys():
        focus_tables(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'focus_tables_train_test'):
    focus_tables_train_test(base_dir)

elif (func == 'U_MAP_train_test'):
    dirs_dict = get_dir_list(base_dir)
    U_MAP_train_test(base_dir)

elif (func == 'binary_problems'):
    binary_problems(base_dir)


elif (func == 'classify_foci'):
    dirs_dict = get_dir_list(base_dir)
    # summary_dir = list(get_dir_list(base_dir))[0]
    # classify_foci(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

    for summary_dir in dirs_dict.keys():
        # summary_dir ='20200227_MRE1153BP1KO_DNAPKiATMi'
        classify_foci(base_dir + '/' + summary_dir, dirs_dict[summary_dir])

elif (func == 'multiclass_problems'):
    dirs_dict = get_dir_list(base_dir)
    multiclass_problems(base_dir)

elif (func == 'metric_dependancy'):
    metric_dependancy(base_dir)

elif (func == 'focus_classification_full'):
    focus_classification_full(base_dir)


else:
    print("Error: Unknown argument.")

